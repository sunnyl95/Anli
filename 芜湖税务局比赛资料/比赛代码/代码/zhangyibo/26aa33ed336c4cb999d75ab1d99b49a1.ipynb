{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# 输入数据的参数\n",
    "_INPUT1='{\"name\":\"input1\",\"type\":0,\"uri\":\"tmp_a3c38616db7f4a63a0e55ba1372e2864\"}'\n",
    "_INPUT2='{\"name\":\"input2\",\"type\":0,\"uri\":\"tmp_e0171dc04a9046cdb6702473b331dd93\"}'\n",
    "_INPUT3='{\"name\":\"input3\",\"type\":0,\"uri\":\"tmp_a11b8c56ba8140418bc1bf469e0be3b6\"}'\n",
    "_INPUT4='{\"name\":\"input4\",\"type\":0,\"uri\":\"tmp_81b3ceaaae7748a883ea10682f858aaa\"}'\n",
    "\n",
    "# 输出数据的参数\n",
    "_OUTPUT='[{\"name\":\"output1\",\"type\":0,\"uri\":\"tmp_e2b5cc1345e546b885d311d4110eba13\"},{\"name\":\"output2\",\"type\":0,\"uri\":\"tmp_ec7f11424ed74e39b2deac2e3a1fedfb\"},{\"name\":\"output3\",\"type\":0,\"uri\":\"tmp_4eb48aa0b455478d983f8ff0994fae1d\"},{\"name\":\"output4\",\"type\":0,\"uri\":\"tmp_77c50a0ea3244805931899086e3ed867\"}]'\n",
    "\n",
    "# 自定义参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 读取并返回dataframe对象\n",
    "# :param as_spark: 为True返回pyspark.sql.DataFrame对象，为False返回pandas.DataFrame对象，默认为False\n",
    "df_test = wfio.read_dataframe(_INPUT1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfio\n",
    "\n",
    "# 读取并返回dataframe对象\n",
    "# :param as_spark: 为True返回pyspark.sql.DataFrame对象，为False返回pandas.DataFrame对象，默认为False\n",
    "df_train1= wfio.read_dataframe(_INPUT2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfio\n",
    "\n",
    "# 读取并返回dataframe对象\n",
    "# :param as_spark: 为True返回pyspark.sql.DataFrame对象，为False返回pandas.DataFrame对象，默认为False\n",
    "df_train2= wfio.read_dataframe(_INPUT3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfio\n",
    "\n",
    "# 读取并返回dataframe对象\n",
    "# :param as_spark: 为True返回pyspark.sql.DataFrame对象，为False返回pandas.DataFrame对象，默认为False\n",
    "df_testB = wfio.read_dataframe(_INPUT4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2017q1fphdsl</th>\n",
       "      <th>2017q1jxje</th>\n",
       "      <th>2017q1jxsl</th>\n",
       "      <th>2017q1kpje</th>\n",
       "      <th>2017q1kps</th>\n",
       "      <th>2017q1kpse</th>\n",
       "      <th>2017q1kpsl</th>\n",
       "      <th>2017q1rkse</th>\n",
       "      <th>2017q2fphdsl</th>\n",
       "      <th>2017q2jxje</th>\n",
       "      <th>...</th>\n",
       "      <th>nsrmc</th>\n",
       "      <th>scjydz</th>\n",
       "      <th>xy</th>\n",
       "      <th>xydl</th>\n",
       "      <th>xyml</th>\n",
       "      <th>xyzl</th>\n",
       "      <th>yc</th>\n",
       "      <th>zcdz</th>\n",
       "      <th>zczby</th>\n",
       "      <th>zjnsrsbh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52</td>\n",
       "      <td>38751641.3</td>\n",
       "      <td>0.11</td>\n",
       "      <td>4262977.49</td>\n",
       "      <td>43</td>\n",
       "      <td>724706.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>32.66</td>\n",
       "      <td>155</td>\n",
       "      <td>144174175.76</td>\n",
       "      <td>...</td>\n",
       "      <td>芜湖市快讯务师事务所有限责任公司</td>\n",
       "      <td>芜湖市镜湖区文化路39-2号海螺商务楼北楼4-5层</td>\n",
       "      <td>会计、审计及税务服务</td>\n",
       "      <td>商务服务业</td>\n",
       "      <td>租赁和商务服务业</td>\n",
       "      <td>咨询与调查</td>\n",
       "      <td>1</td>\n",
       "      <td>芜湖市镜湖区文化路39-2号海螺商务楼北楼4-5层</td>\n",
       "      <td>300000</td>\n",
       "      <td>c7e1249ffc03eb9ded908c236bd1996d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>52629815.57</td>\n",
       "      <td>0.17</td>\n",
       "      <td>8947486.52</td>\n",
       "      <td>90</td>\n",
       "      <td>536849.19</td>\n",
       "      <td>0.06</td>\n",
       "      <td>71.04</td>\n",
       "      <td>90</td>\n",
       "      <td>78403672.86</td>\n",
       "      <td>...</td>\n",
       "      <td>芜湖市联软饰工程有限公司</td>\n",
       "      <td>芜湖市镜湖区镜湖万达广场3#楼1501</td>\n",
       "      <td>建筑装饰业</td>\n",
       "      <td>建筑装饰和其他建筑业</td>\n",
       "      <td>建筑业</td>\n",
       "      <td>建筑装饰业</td>\n",
       "      <td>1</td>\n",
       "      <td>芜湖市镜湖区镜湖万达广场3#楼1501</td>\n",
       "      <td>300000</td>\n",
       "      <td>2a38a4a9316c49e5a833517c45d31070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>芜湖市戴硕电子药包装材料有限公司</td>\n",
       "      <td>铁佛花园A栋12BA室</td>\n",
       "      <td>医疗用品及器材批发</td>\n",
       "      <td>批发业</td>\n",
       "      <td>批发和零售业</td>\n",
       "      <td>医药及医疗器材批发</td>\n",
       "      <td>1</td>\n",
       "      <td>铁佛花园A栋12BA室</td>\n",
       "      <td>500000</td>\n",
       "      <td>38b3eff8baf56627478ec76a704e9b52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>芜湖市诺依曼软件有限责任公司</td>\n",
       "      <td>镜湖区棠梅村</td>\n",
       "      <td>其他未列明金属制品制造</td>\n",
       "      <td>金属制品业</td>\n",
       "      <td>制造业</td>\n",
       "      <td>其他金属制品制造</td>\n",
       "      <td>1</td>\n",
       "      <td>镜湖区棠梅村</td>\n",
       "      <td>1000000</td>\n",
       "      <td>ec8956637a99787bd197eacd77acce5e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>芜湖市时空盒数字资担保有限公司</td>\n",
       "      <td>芜湖市吉和街吉祥新村2－16号</td>\n",
       "      <td>其他非货币银行服务</td>\n",
       "      <td>货币金融服务</td>\n",
       "      <td>金融业</td>\n",
       "      <td>非货币银行服务</td>\n",
       "      <td>1</td>\n",
       "      <td>芜湖市吉和街吉祥新村2－16号</td>\n",
       "      <td>12000000</td>\n",
       "      <td>6974ce5ac660610b44d9b9fed0ff9548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  2017q1fphdsl   2017q1jxje 2017q1jxsl  2017q1kpje 2017q1kps 2017q1kpse  \\\n",
       "0           52   38751641.3       0.11  4262977.49        43  724706.17   \n",
       "1          107  52629815.57       0.17  8947486.52        90  536849.19   \n",
       "2            0            0          0           0         0          0   \n",
       "3            0            0          0           0         0          0   \n",
       "4            0            0          0           0         0          0   \n",
       "\n",
       "  2017q1kpsl 2017q1rkse 2017q2fphdsl    2017q2jxje  ...             nsrmc  \\\n",
       "0       0.17      32.66          155  144174175.76  ...  芜湖市快讯务师事务所有限责任公司   \n",
       "1       0.06      71.04           90   78403672.86  ...      芜湖市联软饰工程有限公司   \n",
       "2          0          0            0             0  ...  芜湖市戴硕电子药包装材料有限公司   \n",
       "3          0          0            0             0  ...    芜湖市诺依曼软件有限责任公司   \n",
       "4          0          0            0             0  ...   芜湖市时空盒数字资担保有限公司   \n",
       "\n",
       "                      scjydz           xy        xydl      xyml       xyzl yc  \\\n",
       "0  芜湖市镜湖区文化路39-2号海螺商务楼北楼4-5层   会计、审计及税务服务       商务服务业  租赁和商务服务业      咨询与调查  1   \n",
       "1        芜湖市镜湖区镜湖万达广场3#楼1501        建筑装饰业  建筑装饰和其他建筑业       建筑业      建筑装饰业  1   \n",
       "2                铁佛花园A栋12BA室    医疗用品及器材批发         批发业    批发和零售业  医药及医疗器材批发  1   \n",
       "3                     镜湖区棠梅村  其他未列明金属制品制造       金属制品业       制造业   其他金属制品制造  1   \n",
       "4            芜湖市吉和街吉祥新村2－16号    其他非货币银行服务      货币金融服务       金融业    非货币银行服务  1   \n",
       "\n",
       "                        zcdz     zczby                          zjnsrsbh  \n",
       "0  芜湖市镜湖区文化路39-2号海螺商务楼北楼4-5层    300000  c7e1249ffc03eb9ded908c236bd1996d  \n",
       "1        芜湖市镜湖区镜湖万达广场3#楼1501    300000  2a38a4a9316c49e5a833517c45d31070  \n",
       "2                铁佛花园A栋12BA室    500000  38b3eff8baf56627478ec76a704e9b52  \n",
       "3                     镜湖区棠梅村   1000000  ec8956637a99787bd197eacd77acce5e  \n",
       "4            芜湖市吉和街吉祥新村2－16号  12000000  6974ce5ac660610b44d9b9fed0ff9548  \n",
       "\n",
       "[5 rows x 114 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show data\n",
    "df_train1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017q1fphdsl\t2017q1jxje\t2017q1jxsl\t2017q1kpje\t2017q1kps\t2017q1kpse\t2017q1kpsl\t2017q1rkse\t2017q2fphdsl\t2017q2jxje\t2017q2jxsl\t2017q2kpje\t2017q2kps\t2017q2kpse\t2017q2kpsl\t2017q2rkse\t2017q3fphdsl\t2017q3jxje\t2017q3jxsl\t2017q3kpje\t2017q3kps\t2017q3kpse\t2017q3kpsl\t2017q3rkse\t2017q4fphdsl\t2017q4jxje\t2017q4jxsl\t2017q4kpje\t2017q4kps\t2017q4kpse\t2017q4kpsl\t2017q4rkse\t2018q1fphdsl\t2018q1jxje\t2018q1jxsl\t2018q1kpje\t2018q1kps\t2018q1kpse\t2018q1kpsl\t2018q1rkse\t2018q2fphdsl\t2018q2jxje\t2018q2jxsl\t2018q2kpje\t2018q2kps\t2018q2kpse\t2018q2kpsl\t2018q2rkse\t2018q3fphdsl\t2018q3jxje\t2018q3jxsl\t2018q3kpje\t2018q3kps\t2018q3kpse\t2018q3kpsl\t2018q3rkse\t2018q4fphdsl\t2018q4jxje\t2018q4jxsl\t2018q4kpje\t2018q4kps\t2018q4kpse\t2018q4kpsl\t2018q4rkse\t2019q1fphdsl\t2019q1jxje\t2019q1jxsl\t2019q1kpje\t2019q1kps\t2019q1kpse\t2019q1kpsl\t2019q1rkse\t2019q2fphdsl\t2019q2jxje\t2019q2jxsl\t2019q2kpje\t2019q2kps\t2019q2kpse\t2019q2kpsl\t2019q2rkse\t2019q3fphdsl\t2019q3jxje\t2019q3jxsl\t2019q3kpje\t2019q3kps\t2019q3kpse\t2019q3kpsl\t2019q3rkse\t2019q4fphdsl\t2019q4jxje\t2019q4jxsl\t2019q4kpje\t2019q4kps\t2019q4kpse\t2019q4kpsl\t2019q4rkse\tbsrxm\tbsrxmmp\tcyrs\tdjrq\tfdbrxmp\tfddbrxm\thy\thydl\thyml\thyzl\tjyfw\tnsrmc\tscjydz\txzjd\tzcdz\tzczb\tzjnsrsbh\n"
     ]
    }
   ],
   "source": [
    "print('\\t'.join(df_testB.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017q1fphdsl\t2017q1jxje\t2017q1jxsl\t2017q1kpje\t2017q1kps\t2017q1kpse\t2017q1kpsl\t2017q1rkse\t2017q2fphdsl\t2017q2jxje\t2017q2jxsl\t2017q2kpje\t2017q2kps\t2017q2kpse\t2017q2kpsl\t2017q2rkse\t2017q3fphdsl\t2017q3jxje\t2017q3jxsl\t2017q3kpje\t2017q3kps\t2017q3kpse\t2017q3kpsl\t2017q3rkse\t2017q4fphdsl\t2017q4jxje\t2017q4jxsl\t2017q4kpje\t2017q4kps\t2017q4kpse\t2017q4kpsl\t2017q4rkse\t2018q1fphdsl\t2018q1jxje\t2018q1jxsl\t2018q1kpje\t2018q1kps\t2018q1kpse\t2018q1kpsl\t2018q1rkse\t2018q2fphdsl\t2018q2jxje\t2018q2jxsl\t2018q2kpje\t2018q2kps\t2018q2kpse\t2018q2kpsl\t2018q2rkse\t2018q3fphdsl\t2018q3jxje\t2018q3jxsl\t2018q3kpje\t2018q3kps\t2018q3kpse\t2018q3kpsl\t2018q3rkse\t2018q4fphdsl\t2018q4jxje\t2018q4jxsl\t2018q4kpje\t2018q4kps\t2018q4kpse\t2018q4kpsl\t2018q4rkse\t2019q1fphdsl\t2019q1jxje\t2019q1jxsl\t2019q1kpje\t2019q1kps\t2019q1kpse\t2019q1kpsl\t2019q1rkse\t2019q2fphdsl\t2019q2jxje\t2019q2jxsl\t2019q2kpje\t2019q2kps\t2019q2kpse\t2019q2kpsl\t2019q2rkse\t2019q3fphdsl\t2019q3jxje\t2019q3jxsl\t2019q3kpje\t2019q3kps\t2019q3kpse\t2019q3kpsl\t2019q3rkse\t2019q4fphdsl\t2019q4jxje\t2019q4jxsl\t2019q4kpje\t2019q4kps\t2019q4kpse\t2019q4kpsl\t2019q4rkse\tcwrysjh\tcwryxm\tcyrs\tdjkyrq\tfddbrxm\tfrsjh\tjdxz\tjyfw\tnsrmc\tscjydz\txy\txydl\txyml\txyzl\tyc\tzcdz\tzczby\tzjnsrsbh\n",
      "2017q1fphdsl\t2017q1jxje\t2017q1jxsl\t2017q1kpje\t2017q1kps\t2017q1kpse\t2017q1kpsl\t2017q1rkse\t2017q2fphdsl\t2017q2jxje\t2017q2jxsl\t2017q2kpje\t2017q2kps\t2017q2kpse\t2017q2kpsl\t2017q2rkse\t2017q3fphdsl\t2017q3jxje\t2017q3jxsl\t2017q3kpje\t2017q3kps\t2017q3kpse\t2017q3kpsl\t2017q3rkse\t2017q4fphdsl\t2017q4jxje\t2017q4jxsl\t2017q4kpje\t2017q4kps\t2017q4kpse\t2017q4kpsl\t2017q4rkse\t2018q1fphdsl\t2018q1jxje\t2018q1jxsl\t2018q1kpje\t2018q1kps\t2018q1kpse\t2018q1kpsl\t2018q1rkse\t2018q2fphdsl\t2018q2jxje\t2018q2jxsl\t2018q2kpje\t2018q2kps\t2018q2kpse\t2018q2kpsl\t2018q2rkse\t2018q3fphdsl\t2018q3jxje\t2018q3jxsl\t2018q3kpje\t2018q3kps\t2018q3kpse\t2018q3kpsl\t2018q3rkse\t2018q4fphdsl\t2018q4jxje\t2018q4jxsl\t2018q4kpje\t2018q4kps\t2018q4kpse\t2018q4kpsl\t2018q4rkse\t2019q1fphdsl\t2019q1jxje\t2019q1jxsl\t2019q1kpje\t2019q1kps\t2019q1kpse\t2019q1kpsl\t2019q1rkse\t2019q2fphdsl\t2019q2jxje\t2019q2jxsl\t2019q2kpje\t2019q2kps\t2019q2kpse\t2019q2kpsl\t2019q2rkse\t2019q3fphdsl\t2019q3jxje\t2019q3jxsl\t2019q3kpje\t2019q3kps\t2019q3kpse\t2019q3kpsl\t2019q3rkse\t2019q4fphdsl\t2019q4jxje\t2019q4jxsl\t2019q4kpje\t2019q4kps\t2019q4kpse\t2019q4kpsl\t2019q4rkse\tbsrxm\tbsrxmmp\tcyrs\tdjrq\tfdbrxmp\tfddbrxm\thy\thydl\thyml\thyzl\tjyfw\tnsrmc\tscjydz\txzjd\tyc\tzcdz\tzczb\tzjnsrsbh\n",
      "2017q1fphdsl\t2017q1jxje\t2017q1jxsl\t2017q1kpje\t2017q1kps\t2017q1kpse\t2017q1kpsl\t2017q1rkse\t2017q2fphdsl\t2017q2jxje\t2017q2jxsl\t2017q2kpje\t2017q2kps\t2017q2kpse\t2017q2kpsl\t2017q2rkse\t2017q3fphdsl\t2017q3jxje\t2017q3jxsl\t2017q3kpje\t2017q3kps\t2017q3kpse\t2017q3kpsl\t2017q3rkse\t2017q4fphdsl\t2017q4jxje\t2017q4jxsl\t2017q4kpje\t2017q4kps\t2017q4kpse\t2017q4kpsl\t2017q4rkse\t2018q1fphdsl\t2018q1jxje\t2018q1jxsl\t2018q1kpje\t2018q1kps\t2018q1kpse\t2018q1kpsl\t2018q1rkse\t2018q2fphdsl\t2018q2jxje\t2018q2jxsl\t2018q2kpje\t2018q2kps\t2018q2kpse\t2018q2kpsl\t2018q2rkse\t2018q3fphdsl\t2018q3jxje\t2018q3jxsl\t2018q3kpje\t2018q3kps\t2018q3kpse\t2018q3kpsl\t2018q3rkse\t2018q4fphdsl\t2018q4jxje\t2018q4jxsl\t2018q4kpje\t2018q4kps\t2018q4kpse\t2018q4kpsl\t2018q4rkse\t2019q1fphdsl\t2019q1jxje\t2019q1jxsl\t2019q1kpje\t2019q1kps\t2019q1kpse\t2019q1kpsl\t2019q1rkse\t2019q2fphdsl\t2019q2jxje\t2019q2jxsl\t2019q2kpje\t2019q2kps\t2019q2kpse\t2019q2kpsl\t2019q2rkse\t2019q3fphdsl\t2019q3jxje\t2019q3jxsl\t2019q3kpje\t2019q3kps\t2019q3kpse\t2019q3kpsl\t2019q3rkse\t2019q4fphdsl\t2019q4jxje\t2019q4jxsl\t2019q4kpje\t2019q4kps\t2019q4kpse\t2019q4kpsl\t2019q4rkse\tbsrxm\tbsrxmmp\tcyrs\tdjrq\tfdbrxmp\tfddbrxm\thy\thydl\thyml\thyzl\tjyfw\tnsrmc\tscjydz\txzjd\tzcdz\tzczb\tzjnsrsbh\n"
     ]
    }
   ],
   "source": [
    "print('\\t'.join(df_train1.columns))\n",
    "print('\\t'.join(df_train2.columns))\n",
    "print('\\t'.join(df_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显然训练集1和训练集2属性排列顺序不一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理时间特征\n",
    "def process_date(df_date,col='djkyrq'):\n",
    "    date=[]\n",
    "    for i in range(len(df_date)):\n",
    "        str1=(df_date.loc[i,col])\n",
    "        gap=2020-int(str1[0:4])\n",
    "        date.append(gap)\n",
    "    df=df_date.drop(col,axis=1)\n",
    "    df['rq']=pd.Series(date)\n",
    "    return df\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train1=process_date(df_train1)\n",
    "df_train2=process_date(df_train2,'djrq')\n",
    "df_test=process_date(df_test,'djrq')\n",
    "df_testB=process_date(df_testB,'djrq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017q1fphdsl\t2017q1jxje\t2017q1jxsl\t2017q1kpje\t2017q1kps\t2017q1kpse\t2017q1kpsl\t2017q1rkse\t2017q2fphdsl\t2017q2jxje\t2017q2jxsl\t2017q2kpje\t2017q2kps\t2017q2kpse\t2017q2kpsl\t2017q2rkse\t2017q3fphdsl\t2017q3jxje\t2017q3jxsl\t2017q3kpje\t2017q3kps\t2017q3kpse\t2017q3kpsl\t2017q3rkse\t2017q4fphdsl\t2017q4jxje\t2017q4jxsl\t2017q4kpje\t2017q4kps\t2017q4kpse\t2017q4kpsl\t2017q4rkse\t2018q1fphdsl\t2018q1jxje\t2018q1jxsl\t2018q1kpje\t2018q1kps\t2018q1kpse\t2018q1kpsl\t2018q1rkse\t2018q2fphdsl\t2018q2jxje\t2018q2jxsl\t2018q2kpje\t2018q2kps\t2018q2kpse\t2018q2kpsl\t2018q2rkse\t2018q3fphdsl\t2018q3jxje\t2018q3jxsl\t2018q3kpje\t2018q3kps\t2018q3kpse\t2018q3kpsl\t2018q3rkse\t2018q4fphdsl\t2018q4jxje\t2018q4jxsl\t2018q4kpje\t2018q4kps\t2018q4kpse\t2018q4kpsl\t2018q4rkse\t2019q1fphdsl\t2019q1jxje\t2019q1jxsl\t2019q1kpje\t2019q1kps\t2019q1kpse\t2019q1kpsl\t2019q1rkse\t2019q2fphdsl\t2019q2jxje\t2019q2jxsl\t2019q2kpje\t2019q2kps\t2019q2kpse\t2019q2kpsl\t2019q2rkse\t2019q3fphdsl\t2019q3jxje\t2019q3jxsl\t2019q3kpje\t2019q3kps\t2019q3kpse\t2019q3kpsl\t2019q3rkse\t2019q4fphdsl\t2019q4jxje\t2019q4jxsl\t2019q4kpje\t2019q4kps\t2019q4kpse\t2019q4kpsl\t2019q4rkse\tcwrysjh\tcwryxm\tcyrs\tfddbrxm\tfrsjh\tjdxz\tjyfw\tnsrmc\tscjydz\txy\txydl\txyml\txyzl\tyc\tzcdz\tzczby\tzjnsrsbh\trq\n"
     ]
    }
   ],
   "source": [
    "print('\\t'.join(df_train1.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##代号x1表示特征中的数字特征\n",
    "#rq 代表等级日期\n",
    "#rs 代表从业人数\n",
    "#zb 代表注册资本\n",
    "x1_train1=df_train1.iloc[:,:96]\n",
    "x1_train1['rq']=df_train1['rq']\n",
    "x1_train2=df_train2.iloc[:,:96]\n",
    "x1_train2['rq']=df_train2['rq']\n",
    "x1_test=df_test.iloc[:,:96]\n",
    "x1_test['rq']=df_test['rq']\n",
    "x1_testB=df_testB.iloc[:,:96]\n",
    "x1_testB['rq']=df_testB['rq']\n",
    "x1_train1['rs']=df_train1['cyrs']\n",
    "x1_train2['rs']=df_train2['cyrs']\n",
    "x1_test['rs']=df_test['cyrs']\n",
    "x1_testB['rs']=df_testB['cyrs']\n",
    "x1_train1['zb']=df_train1['zczby']\n",
    "x1_train2['rb']=df_train2['zczb']\n",
    "x1_test['zb']=df_test['zczb']\n",
    "x1_testB['zb']=df_testB['zczb']\n",
    "#代号x2代表特征中的非数字特征\n",
    "x2_train1=df_train1.iloc[:,96:]\n",
    "x2_train2=df_train2.iloc[:,96:]\n",
    "x2_test=df_test.iloc[:,96:]\n",
    "x2_testB=df_testB.iloc[:,96:]\n",
    "x2_train1=x2_train1.drop(['rq','cyrs','zczby','yc','zjnsrsbh'],axis=1)\n",
    "x2_train2=x2_train2.drop(['rq','cyrs','zczb','yc','zjnsrsbh'],axis=1)\n",
    "x2_test=x2_test.drop(['rq','cyrs','zczb','zjnsrsbh'],axis=1)\n",
    "x2_testB=x2_testB.drop(['rq','cyrs','zczb','zjnsrsbh'],axis=1)\n",
    "#标记\n",
    "y_train1=df_train1.loc[:,'yc']\n",
    "y_train2=df_train2.loc[:,'yc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将识别异常值，并将其设置为0\n",
    "def setnull2zero(df):\n",
    "    df_new=pd.DataFrame()\n",
    "    for i in range(len(df.columns)):\n",
    "        df_new[str(i)]=pd.to_numeric(df.iloc[:,i], errors='coerce')\n",
    "    missing_values_count= df_new.isnull().sum()\n",
    "    print('不是数字的有%s个'%missing_values_count.sum())\n",
    "    df=df_new.fillna(0)\n",
    "    df=df.astype('float')\n",
    "    missing_values_count= df.isnull().sum()\n",
    "    print('已处理，不是数字的有%s个'%missing_values_count.sum())\n",
    "    \n",
    "     #无量纲化\n",
    "    #from sklearn.preprocessing import MinMaxScaler\n",
    "    #df=pd.DataFrame(MinMaxScaler().fit_transform(df))\n",
    "    #print('无量纲化完成')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#识别异常值，并将他丢弃\n",
    "def setnull_drop(df):\n",
    "    df_new=pd.DataFrame()\n",
    "    for i in range(len(df.columns)):\n",
    "        df_new[str(i)]=pd.to_numeric(df.iloc[:,i], errors='coerce')\n",
    "    df = df_new.dropna(axis=0)\n",
    "    print('已处理')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不是数字的有0个\n",
      "已处理，不是数字的有0个\n",
      "不是数字的有235137个\n",
      "已处理，不是数字的有0个\n",
      "不是数字的有137694个\n",
      "已处理，不是数字的有0个\n",
      "不是数字的有140948个\n",
      "已处理，不是数字的有0个\n"
     ]
    }
   ],
   "source": [
    "x1_train1=setnull2zero(x1_train1)\n",
    "x1_train2=setnull2zero(x1_train2)\n",
    "x1_test=setnull2zero(x1_test)\n",
    "x1_testB=setnull2zero(x1_testB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理非数值特征\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "def encode(df):\n",
    "    for col in df.columns:\n",
    "        df[col]=le.fit_transform(df[col])\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode(x2_train1)\n",
    "encode(x2_train2)\n",
    "encode(x2_test)\n",
    "encode(x2_testB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#合并数据\n",
    "x_train1=pd.concat([x1_train1,x2_train1],axis=1,sort=False)\n",
    "x_train2=pd.concat([x1_train2,x2_train2],axis=1,sort=False)\n",
    "x_train=pd.concat([x_train1,x_train2],axis=0,ignore_index=True,sort=False)\n",
    "y_train=pd.concat([y_train1,y_train2],axis=0,ignore_index=True,sort=False)\n",
    "x_test=pd.concat([x1_test,x2_test],axis=1,sort=False) \n",
    "x_testB=pd.concat([x1_testB,x2_testB],axis=1,sort=False)\n",
    "df_train=pd.concat([df_train1,df_train2],axis=0,ignore_index=True,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t11\t12\t13\t14\t15\t16\t17\t18\t19\t20\t21\t22\t23\t24\t25\t26\t27\t28\t29\t30\t31\t32\t33\t34\t35\t36\t37\t38\t39\t40\t41\t42\t43\t44\t45\t46\t47\t48\t49\t50\t51\t52\t53\t54\t55\t56\t57\t58\t59\t60\t61\t62\t63\t64\t65\t66\t67\t68\t69\t70\t71\t72\t73\t74\t75\t76\t77\t78\t79\t80\t81\t82\t83\t84\t85\t86\t87\t88\t89\t90\t91\t92\t93\t94\t95\t96\t97\t98\tcwrysjh\tcwryxm\tfddbrxm\tfrsjh\tjdxz\tjyfw\tnsrmc\tscjydz\txy\txydl\txyml\txyzl\tzcdz\tbsrxm\tbsrxmmp\tfdbrxmp\thy\thydl\thyml\thyzl\txzjd\n",
      "0\t1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t11\t12\t13\t14\t15\t16\t17\t18\t19\t20\t21\t22\t23\t24\t25\t26\t27\t28\t29\t30\t31\t32\t33\t34\t35\t36\t37\t38\t39\t40\t41\t42\t43\t44\t45\t46\t47\t48\t49\t50\t51\t52\t53\t54\t55\t56\t57\t58\t59\t60\t61\t62\t63\t64\t65\t66\t67\t68\t69\t70\t71\t72\t73\t74\t75\t76\t77\t78\t79\t80\t81\t82\t83\t84\t85\t86\t87\t88\t89\t90\t91\t92\t93\t94\t95\t96\t97\t98\tcwrysjh\tcwryxm\tfddbrxm\tfrsjh\tjdxz\tjyfw\tnsrmc\tscjydz\txy\txydl\txyml\txyzl\tzcdz\n"
     ]
    }
   ],
   "source": [
    "x_train1.head()\n",
    "print('\\t'.join(x_train.columns))\n",
    "print('\\t'.join(x_train1.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t11\t12\t13\t14\t15\t16\t17\t18\t19\t20\t21\t22\t23\t24\t25\t26\t27\t28\t29\t30\t31\t32\t33\t34\t35\t36\t37\t38\t39\t40\t41\t42\t43\t44\t45\t46\t47\t48\t49\t50\t51\t52\t53\t54\t55\t56\t57\t58\t59\t60\t61\t62\t63\t64\t65\t66\t67\t68\t69\t70\t71\t72\t73\t74\t75\t76\t77\t78\t79\t80\t81\t82\t83\t84\t85\t86\t87\t88\t89\t90\t91\t92\t93\t94\t95\t96\t97\t98\tbsrxm\tbsrxmmp\tfdbrxmp\tfddbrxm\thy\thydl\thyml\thyzl\tjyfw\tnsrmc\tscjydz\txzjd\tzcdz\n"
     ]
    }
   ],
   "source": [
    "print('\\t'.join(x_train2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ustciscrLab_A import get_score\n",
    "#from ustciscrBDL_B import get_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用随机森林试一下\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr=RandomForestRegressor(n_estimators=150)\n",
    "we_rfr=rfr.fit(x_train1,np.array(y_train1).ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "model_AdaBoostRegressor = AdaBoostRegressor(base_estimator=None, n_estimators=150, learning_rate=1.0, \n",
    "                                            loss='linear', random_state=None)\n",
    "we_AdaBoostRegressor=model_AdaBoostRegressor.fit(x_train2,y_train2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测\n",
    "\n",
    "y_pred_rfr=we_rfr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测\n",
    "y_pred_ad=we_rfr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=(y_pred_ad*0.3+y_pred_rfr*0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-id:16\n",
      "程序开始运行时间为：2019-12-22 14:02:56.348630\n",
      "程序结束运行时间为：2019-12-22 14:02:56.896477\n",
      "程序运行时间（去除打分耗时）为：0.547847\n",
      "程序的准确率为：93.56997849\n",
      "最终得分为：94.21298064\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#打分\n",
    "get_score.post_user_id('16')\n",
    "user_test_data=pd.DataFrame()\n",
    "\n",
    "user_test_data['zjnsrsbh']=df_train1['zjnsrsbh']\n",
    "user_test_data['Probability']=pd.Series(we_rfr.predict(x_train1)*0.7+we_AdaBoostRegressor.predict(x_train2)*0.3)\n",
    "get_score.post_test_data(user_test_data,df_train1)\n",
    "user_verify_data=pd.DataFrame()\n",
    "user_verify_data['zjnsrsbh']=df_test['zjnsrsbh']\n",
    "user_verify_data['Probability']=pd.Series(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-id:16\n",
      "程序开始运行时间为：2019-12-22 14:02:57.106407\n",
      "程序结束运行时间为：2019-12-22 14:02:57.124319\n",
      "程序运行时间（去除打分耗时）为：0.017912\n",
      "程序的准确率为：90.43140267\n",
      "最终得分为：91.3882624\n"
     ]
    }
   ],
   "source": [
    "get_score.post_user_id('16')\n",
    "get_score.post_verify_data(user_verify_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlxtend'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-f247e6baa91a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmlxtend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregressor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStackingRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGradientBoostingRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mlxtend'"
     ]
    }
   ],
   "source": [
    "\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "lr = LinearRegression()\n",
    "svr_lin = SVR(kernel='linear')\n",
    "gbr=GradientBoostingRegressor(n_estimators=150,learning_rate=0.1,max_depth=10, random_state=0, loss='ls')\n",
    "stregr = StackingRegressor(regressors=[svr_lin,model_AdaBoostRegressor], \n",
    "                           meta_regressor=lr)\n",
    "model1=stregr.fit(x_train1,y_train1)\n",
    "model2=stregr.fit(x_train2,y_train2)\n",
    "pre1=model1.predict(x_testB)\n",
    "pre2=model2.predict(x_testB)\n",
    "pre=pre1*0.7+pre2*0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_score.post_user_id('16')\n",
    "user_test_data=pd.DataFrame()\n",
    "\n",
    "user_test_data['zjnsrsbh']=df_train1['zjnsrsbh']\n",
    "user_test_data['Probability']=pd.Series(model1.predict(x_train1)+model2.predict(x_train2)*0.3)\n",
    "get_score.post_test_data(user_test_data,df_train1)\n",
    "user_verify_data=pd.DataFrame()\n",
    "user_verify_data['zjnsrsbh']=df_test['zjnsrsbh']\n",
    "user_verify_data['Probability']=pd.Series(pre)\n",
    "\n",
    "get_score.post_verify_data(user_verify_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#采用 stacking的策略，集成学习一下\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class BasicModel(object):\n",
    "    \"\"\"Parent class of basic models\"\"\"\n",
    "    def train(self, x_train, y_train, x_val, y_val):\n",
    "        \"\"\"return a trained model and eval metric of validation data\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict(self, model, x_test):\n",
    "        \"\"\"return the predicted result of test data\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_oof(self, x_train, y_train, x_test, n_folds = 5):\n",
    "        \"\"\"K-fold stacking\"\"\"\n",
    "        num_train, num_test = x_train.shape[0], x_test.shape[0]\n",
    "        oof_train = np.zeros((num_train,)) \n",
    "        oof_test = np.zeros((num_test,))\n",
    "        oof_test_all_fold = np.zeros((num_test, n_folds))\n",
    "        aucs = []\n",
    "        KF = KFold(n_splits = n_folds, random_state=2017)\n",
    "        for i, (train_index, val_index) in enumerate(KF.split(x_train)):\n",
    "            print('{0} fold, train {1}, val {2}'.format(i, \n",
    "                                                        len(train_index),\n",
    "                                                        len(val_index)))\n",
    "            x_tra, y_tra = x_train[train_index], y_train[train_index]\n",
    "            x_val, y_val = x_train[val_index], y_train[val_index]\n",
    "            model, auc = self.train(x_tra, y_tra, x_val, y_val)\n",
    "            aucs.append(auc)\n",
    "            oof_train[val_index] = self.predict(model, x_val)\n",
    "            oof_test_all_fold[:, i] = self.predict(model, x_test)\n",
    "        oof_test = np.mean(oof_test_all_fold, axis=1)\n",
    "        print('all aucs {0}, average {1}'.format(aucs, np.mean(aucs)))\n",
    "        return oof_train, oof_test\n",
    "#上面最重要的就是进行 K-fold 训练的 get_oof 方法，该方法最终返回训练集和测试集在基模型上的预测结果，也就是两个一维向量，长度分别是训练集和测试集的样本数。\n",
    "\n",
    "#下面以基模型都只需要实现 BasicModel 中的 train 和 predict 方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#采用boosting策略的集成学习，作为基模型之一\n",
    "import xgboost as xgb\n",
    "class XGBRegress(BasicModel):\n",
    "    def __init__(self):\n",
    "        \"\"\"set parameters\"\"\"\n",
    "        self.num_rounds=1000\n",
    "        self.early_stopping_rounds = 15\n",
    "        self.params = {\n",
    "        'booster': 'gbtree',\n",
    "        'objective': 'reg:linear',\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.85,\n",
    "        'eta': 0.05,\n",
    "        'max_depth': 7,\n",
    "        'seed': 2016,\n",
    "        'silent': 0,\n",
    "        'eval_metric': 'rmse'\n",
    "         }\n",
    "        \n",
    "    def train(self, x_train, y_train, x_val, y_val):\n",
    "        print('train with xgb model')\n",
    "        xgbtrain = xgb.DMatrix(x_train, y_train)\n",
    "        xgbval = xgb.DMatrix(x_val, y_val)\n",
    "        watchlist = [(xgbtrain,'train'), (xgbval, 'val')]\n",
    "        model = xgb.train(self.params, \n",
    "                          xgbtrain, \n",
    "                          self.num_rounds\n",
    "                          watchlist,\n",
    "                          early_stopping_rounds = self.early_stopping_rounds)\n",
    "        return model, float(model.eval(xgbval).split()[1].split(':')[1])\n",
    "\n",
    "    def predict(self, model, x_test):\n",
    "        print('test with xgb model')\n",
    "        xgbtest = xgb.DMatrix(x_test)\n",
    "        return model.predict(xgbtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#采用bagging策略的集成学习(随机森林），作为基础模型之一\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "class RFRegress(BasicModel):\n",
    "    def __init__(self):\n",
    "        \"\"\"set parameters\"\"\"\n",
    "        self.params = {\n",
    "        'n_estimators'=100\n",
    "         }\n",
    "    def train(self,x_train,y_train):\n",
    "        print('train with RandomForest')\n",
    "        rfr=RandomForestRegressor(self.params)\n",
    "        we_rfr=rfr.fit(x_train,np.array(y_train).ravel())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "from sklearn.model_selection import KFold\n",
    "def get_stacking(clf, x_train, y_train, x_test, n_folds=10):\n",
    "    \"\"\"\n",
    "    这个函数是stacking的核心，使用交叉验证的方法得到次级训练集\n",
    "    x_train, y_train, x_test 的值应该为numpy里面的数组类型 numpy.ndarray .\n",
    "    如果输入为pandas的DataFrame类型则会把报错\"\"\"\n",
    "    train_num, test_num = x_train.shape[0], x_test.shape[0]\n",
    "    second_level_train_set = np.zeros((train_num,))\n",
    "    second_level_test_set = np.zeros((test_num,))\n",
    "    test_nfolds_sets = np.zeros((test_num, n_folds))\n",
    "    kf = KFold(n_splits=n_folds)\n",
    "\n",
    "    for i,(train_index, test_index) in enumerate(kf.split(x_train)):\n",
    "        x_tra, y_tra = x_train[train_index], y_train[train_index]\n",
    "        x_tst, y_tst =  x_train[test_index], y_train[test_index]\n",
    "\n",
    "        clf.fit(x_tra, y_tra)\n",
    "\n",
    "        second_level_train_set[test_index] = clf.predict(x_tst)\n",
    "        test_nfolds_sets[:,i] = clf.predict(x_test)\n",
    "\n",
    "    second_level_test_set[:] = test_nfolds_sets.mean(axis=1)\n",
    "    return second_level_train_set, second_level_test_set\n",
    "\n",
    "\n",
    "\n",
    "#不同的基分类器，需要调参\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model_rfr=RandomForestRegressor(n_estimators=100, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
    "                          min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                          min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, \n",
    "                          warm_start=False, ccp_alpha=0.0, max_samples=None)\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "model_AdaBoostRegressor = AdaBoostRegressor(base_estimator=None, n_estimators=100, learning_rate=1.0, \n",
    "                                            loss='linear', random_state=None)\n",
    "from sklearn import svm\n",
    "model_SVR = svm.SVR(kernel='rbf', degree=3, gamma='scale', \n",
    "                    coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)\n",
    "\n",
    "#加入数据\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(x_train, y_train, test_size=0.2)\n",
    "\n",
    "train_sets = []\n",
    "test_sets = []\n",
    "for clf in [model_rfr,model_AdaBoostRegressor,model_SVR]:\n",
    "    train_set, test_set = get_stacking(clf, train_x, train_y, test_x)\n",
    "    train_sets.append(train_set)\n",
    "    test_sets.append(test_set)\n",
    "\n",
    "meta_train = np.concatenate([result_set.reshape(-1,1) for result_set in train_sets], axis=1)\n",
    "meta_test = np.concatenate([y_test_set.reshape(-1,1) for y_test_set in test_sets], axis=1)\n",
    "\n",
    "#使用logisti回归作为我们的次级分类器\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(meta_train, train_y)\n",
    "lr_predict = dt_model.predict(meta_test)\n",
    "\n",
    "print(lr_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
