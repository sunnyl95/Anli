{"nbformat_minor":2,"metadata":{"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"name":"python","file_extension":".py","mimetype":"text/x-python","version":"3.7.3"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"cells":[{"outputs":[],"metadata":{"tags":["parameters"]},"execution_count":1,"source":["# 输入数据的参数\n","_INPUT1='{\"name\":\"input1\",\"type\":0,\"uri\":\"tmp_33efb2caf64f4636aedf2ac8b0918c8d\"}'\n","_INPUT3='{\"name\":\"input3\",\"type\":0,\"uri\":\"tmp_9fd5b54c7ca24e46b198257ef2db93a2\"}'\n","_INPUT2='{\"name\":\"input2\",\"type\":0,\"uri\":\"tmp_0121879d12a64bd18af72c6992ca330e\"}'\n","_INPUT4='{\"name\":\"input4\",\"type\":0,\"uri\":\"tmp_b3afb39beb1c4e3caef20035420113ea\"}'\n","\n","# 输出数据的参数\n","_OUTPUT='[{\"name\":\"output1\",\"type\":0,\"uri\":\"tmp_dafc1a848f9642b7bdf482cde3c04c0d\"},{\"name\":\"output2\",\"type\":0,\"uri\":\"tmp_bbf9885902314f12a2cee82588a90e4f\"},{\"name\":\"output3\",\"type\":0,\"uri\":\"tmp_9fbb9ca946a04f87aad25ffcc1f93c2f\"},{\"name\":\"output4\",\"type\":0,\"uri\":\"tmp_85e9ee649ef7493cb1381d4db310099a\"}]'\n","\n","# 自定义参数\n"],"cell_type":"code"},{"outputs":[],"metadata":{},"execution_count":2,"source":["import numpy as np\n","import pandas as pd\n","\n","from sklearn.linear_model import LogisticRegression\n","\n","import wfio\n","# from ustciscrLab_A import get_score\n","from ustciscrBDL_B import get_score"],"cell_type":"code"},{"metadata":{},"source":["# load data"],"cell_type":"markdown"},{"outputs":[{"output_type":"stream","name":"stdout","text":["(10000, 114) (10000, 114) (6000, 113) (6000, 113)\n"]}],"metadata":{},"execution_count":3,"source":["train_data = wfio.read_dataframe(_INPUT1)\n","test_data = wfio.read_dataframe(_INPUT2)\n","train_data_add = wfio.read_dataframe(_INPUT3)\n","test_data_add = wfio.read_dataframe(_INPUT4)\n","print(train_data.shape, train_data_add.shape, test_data.shape, test_data_add.shape)\n","# train_data = pd.concat([train_data, train_data_add], ignore_index=True)\n","# print(train_data.shape)\n","\n","test_data = test_data_add"],"cell_type":"code"},{"metadata":{},"source":["# preprocess"],"cell_type":"markdown"},{"outputs":[{"output_type":"stream","name":"stdout","text":["{'xydl', 'xyzl', 'xy', 'cwryxm', 'frsjh', 'jdxz', 'cwrysjh', 'xyml', 'zczby', 'djkyrq'}\n","{'hyzl', 'xzjd', 'fdbrxmp', 'zczb', 'bsrxmmp', 'hydl', 'hyml', 'djrq', 'hy', 'bsrxm'}\n","(20000, 114)\n"]},{"output_type":"stream","name":"stderr","text":["/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:11: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n","of pandas will change to not sort by default.\n","\n","To accept the future behavior, pass 'sort=False'.\n","\n","To retain the current behavior and silence the warning, pass 'sort=True'.\n","\n","  # This is added back by InteractiveShellApp.init_path()\n"]}],"metadata":{},"execution_count":4,"source":["cols1 = train_data.columns.tolist()\n","cols2 = train_data_add.columns.tolist()\n","cols3 = test_data.columns.tolist()\n","print(set(cols1)-set(cols2))\n","print(set(cols2)-set(cols1))\n","col_dict = {'xyzl': 'hyzl', 'frsjh': 'fdbrxmp', 'cwrysjh': 'bsrxmmp', 'cwryxm': 'bsrxm', 'jdxz': 'xzjd', 'xy': 'hy', 'xydl': 'hydl', 'xyml': 'hyml', 'zczby': 'zczb', 'djkyrq': 'djrq'}\n","\n","for col1, col2 in col_dict.items():\n","    train_data.rename(columns={col1: col2},inplace=True)\n","    \n","train_data = pd.concat([train_data, train_data_add], ignore_index=True)\n","print(train_data.shape)"],"cell_type":"code"},{"outputs":[{"output_type":"stream","name":"stdout","text":["(20000, 114)\n"]}],"metadata":{},"execution_count":5,"source":["# desc(train_data)\n","train_data.isna().any()\n","train_data.dropna(axis=0,how='all') \n","print(train_data.shape)\n","\n","test_data[test_data=='']=0\n","test_data[test_data=='\"\"\"\"\"\"\"\"\"\"\"\"\"\"']=0\n","\n","train_data[train_data=='']=0\n","train_data[train_data=='\"\"\"\"\"\"\"\"\"\"\"\"\"\"']=0"],"cell_type":"code"},{"metadata":{},"source":["# feature extraction"],"cell_type":"markdown"},{"outputs":[],"metadata":{},"execution_count":6,"source":["id_title = 'zjnsrsbh'\n","label_title = 'yc'\n","result_title = 'Probability'\n","\n","def desc(df):\n","    return list(zip([item for item in df.dtypes.index], [item.name for item in df.dtypes]))\n","\n","def get_type2index(column):\n","    type2index = dict()\n","    for item in column:\n","        if item not in type2index.keys():\n","            type2index[item] = len(type2index)\n","    return type2index\n","\n","def get_onehot(df, title):\n","    column = df[title]\n","    type2index = get_type2index(column)\n","    onehot_columns = [f\"{title}_{index}\" for index in type2index.values()]\n","    dim = len(type2index)\n","    size = len(column)\n","    data = np.zeros([size, dim]).astype(np.int32)\n","    for i, item in enumerate(column):\n","        data[i][type2index[item]] = 1\n","    onehot_df = pd.DataFrame(data, columns=onehot_columns, index=column.index)\n","    return onehot_df\n","\n","def all_one_or_zero(preds):\n","    # linear: 1, (0.5, 1 - 1e-5), (0.29, 4e-4), \n","    # linear: 2, (0.4 1 - 1e-5), (0.29, 4e-4), 95.43225, no session feature\n","    # linear: 3, (0.4 1 - 1e-5), (0.3, 4e-4), 95.4124, with session feature\n","    preds[preds >= 0.4] = 1 - 1e-5\n","    preds[preds < 0.29] = 4e-4\n","    #preds[preds >= 0.4] = 1 - 1e-5\n","    #preds[preds < 0.3] = 4e-4\n","    return preds\n","\n","# 最后一次开票季度-注册季度（减2年）\n","# 离线/在线不同版本（数据日期格式不同）\n","def get_interval_session(row):\n","    kps_columns = ['2017q1kps', '2017q2kps', '2017q3kps', '2017q4kps', '2018q1kps', '2018q2kps', '2018q3kps', '2018q4kps', '2019q1kps', '2019q2kps', '2019q3kps', '2019q4kps']\n","    # 注册季度\n","    # 离线\n","    # (datetime.datetime) yyyy/(M)M/(d)d\n","    # date = row['djkyrq']\n","    # zc_year, zc_month = date.year, date.month\n","    # 在线\n","    # yyyy-MM-dd hh:mm:ss\n","    zc_year, zc_month, _ = row['djrq'].strip().split(' ')[0].split('-')\n","    zc_year, zc_month = int(zc_year), int(zc_month)\n","    zc_year -= 2    # 注册年份减2年\n","    # 1(3, 4, 5), 2(6, 7, 8), 3(9, 10, 11), 4(12, 1, 2)\n","    if zc_month <= 2:\n","        zc_session = 4\n","        zc_year -= 1\n","    elif zc_month <= 5:\n","        zc_session = 1\n","    elif zc_month <= 8:\n","        zc_session = 2\n","    elif zc_month <= 11:\n","        zc_session = 3\n","    else:\n","        zc_session = 4\n","    # 最后一次开票季度\n","    for title in reversed(kps_columns):\n","        if row[title] > 0:\n","            final_date = title.strip('kps')\n","            final_year, final_session = final_date.split('q')\n","            final_year, final_session = int(final_year), int(final_session)\n","            break\n","    else:\n","        # 没有开票记录\n","        final_year = zc_year\n","        final_session = zc_session\n","    interval_session = (final_year - zc_year) * 4 + final_session - zc_session\n","    return interval_session\n","\n","# 同一列重复出现的内容\n","def get_repeat_content(column):\n","    all_content = set()\n","    repeat_content = set()\n","    for item in column:\n","        if item not in all_content:\n","            all_content.add(item)\n","        elif item not in repeat_content:\n","            repeat_content.add(item)\n","    return repeat_content\n","\n","# 地址/手机号码/名称是否重复\n","def get_repeat_value(cell, repeat_content):\n","    if cell in repeat_content:\n","        return 1\n","    else:\n","        return 0\n","    \n","# 挑选带有标签的特征\n","def preprocess_label(feature_df, label_df):\n","    label_df[label_title] = label_df[label_title].astype(np.int32)\n","    feature_df_with_label = pd.merge(label_df, feature_df, how='inner', on=id_title)\n","    return feature_df_with_label\n","\n","def split(feature_df_with_label, train_rate=0.8):\n","    all_size = len(feature_df_with_label)\n","    train_size = int(all_size * 0.8)\n","    test_size = all_size - train_size\n","    train_selected = np.array([True] * train_size + [False] * test_size)\n","    np.random.shuffle(train_selected)\n","    test_selected = ~train_selected\n","    train_df = feature_df_with_label[train_selected]\n","    test_df = feature_df_with_label[test_selected]\n","    return train_df, test_df"],"cell_type":"code"},{"outputs":[],"metadata":{},"execution_count":7,"source":["def preprocess_feature(feature_df, with_norm=True):\n","    title_float64 = [\n","        '2017q1jxje', '2017q1jxsl', '2017q1kpje', '2017q1kpse', '2017q1kpsl', '2017q1rkse',\n","        '2017q2jxje', '2017q2jxsl', '2017q2kpje', '2017q2kpse', '2017q2kpsl', '2017q2rkse',\n","        '2017q3jxje', '2017q3jxsl', '2017q3kpje', '2017q3kpse', '2017q3kpsl', '2017q3rkse',\n","        '2017q4jxje', '2017q4jxsl', '2017q4kpje', '2017q4kpse', '2017q4kpsl', '2017q4rkse',\n","        '2018q1jxje', '2018q1jxsl', '2018q1kpje', '2018q1kpse', '2018q1kpsl', '2018q1rkse',\n","        '2018q2jxje', '2018q2jxsl', '2018q2kpje', '2018q2kpse', '2018q2kpsl', '2018q2rkse',\n","        '2018q3jxje', '2018q3jxsl', '2018q3kpje', '2018q3kpse', '2018q3kpsl', '2018q3rkse',\n","        '2018q4jxje', '2018q4jxsl', '2018q4kpje', '2018q4kpse', '2018q4kpsl', '2018q4rkse',\n","        '2019q1jxje', '2019q1jxsl', '2019q1kpje', '2019q1kpse', '2019q1kpsl', '2019q1rkse',\n","        '2019q2jxje', '2019q2jxsl', '2019q2kpje', '2019q2kpse', '2019q2kpsl', '2019q2rkse',\n","        '2019q3jxje', '2019q3jxsl', '2019q3kpje', '2019q3kpse', '2019q3kpsl', '2019q3rkse',\n","        '2019q4jxje', '2019q4jxsl', '2019q4kpje', '2019q4kpse', '2019q4kpsl', '2019q4rkse'\n","    ]\n","    title_int64 = [\n","        '2017q1fphdsl', '2017q1kps', \n","        '2017q2fphdsl', '2017q2kps', \n","        '2017q3fphdsl', '2017q3kps', \n","        '2017q4fphdsl', '2017q4kps', \n","        '2018q1fphdsl', '2018q1kps', \n","        '2018q2fphdsl', '2018q2kps', \n","        '2018q3fphdsl', '2018q3kps', \n","        '2018q4fphdsl', '2018q4kps', \n","        '2019q1fphdsl', '2019q1kps', \n","        '2019q2fphdsl', '2019q2kps', \n","        '2019q3fphdsl', '2019q3kps', \n","        '2019q4fphdsl', '2019q4kps', \n","        'cyrs'\n","    ]\n","\n","    title_category = ['hy', 'hydl', 'hyml', 'hyzl'] # 行业，行业大类，行业门类，行业中类\n","    title_addr = ['scjydz', 'zcdz'] # 生产经营地址地址，注册地址\n","    title_number = ['bsrxmmp', 'fdbrxmp'] # 财务人员手机号，法人手机号\n","    title_name = ['bsrxm', 'fddbrxm', 'nsrmc'] # 财务人员姓名，法定代表人姓名，纳税人姓名\n","    date_col = 'djrq' # 登记开业日期\n","\n","    feature_titles = list()\n","    # 无预处理\n","    # id\n","    # title_id = ['zjnsrsbh']\n","    # print(len(get_repeat_content(feature_df['zjnsrsbh'])))\n","    # 类别基本不同：经营范围 8999/10000=0.8999\n","    # title_category = ['jyfw']\n","    # 地址基本相同：街道乡镇 9996/10000=0.9996\n","    # title_addr = ['jdxz']\n","    \n","    # 数值预处理：object转float64/int64,正则化\n","    feature_df[title_float64+title_int64] = feature_df[title_float64+title_int64].astype(np.float64)\n","    # feature_df[title_int64] = feature_df[title_int64].astype(np.int64)\n","    if with_norm:\n","        for title in title_float64 + title_int64:\n","            value = feature_df[title]\n","            value = (value - value.mean()) / value.std()\n","            feature_df.loc[:, title] = value\n","    feature_titles.extend(title_float64)\n","    feature_titles.extend(title_int64)\n","\n","    # 类别预处理：object转onehot\n","    onehot_dfs = list()\n","    for title in title_category:\n","        # all_len = len(feature_df[title])\n","        # type_len = len(get_type2index(feature_df[title]))\n","        # print(f\"{title}\\t{type_len}/{all_len}={type_len/all_len}\")\n","        onehot_df = get_onehot(feature_df, title)\n","        onehot_dfs.append(onehot_df)\n","        feature_titles.extend(onehot_df.columns)\n","    feature_df = pd.concat([feature_df] + onehot_dfs, axis=1)\n","    \n","    # 日期预处理：注册日期减2年，最后开票日期减注册日期\n","    # title_date = ['djkyrq']\n","    interval_title = 'interval_session'\n","    feature_df[interval_title] = feature_df.apply(get_interval_session, axis=1)\n","    value = feature_df[interval_title]\n","    value = (value - value.mean()) / value.std()\n","    feature_df.loc[:, interval_title] = value\n","    feature_titles.append(interval_title)\n","\n","    # 地址/电话号码/名称预处理：是否重复出现\n","    to_check_titles = title_addr + title_number + title_name\n","    for title in to_check_titles:\n","        repeat_title = title + \"_is_repeat\"\n","        # print(f\"{title}\\t{len(get_repeat_content(feature_df[title]))}\")\n","        column = feature_df[title]\n","        repeat_content = get_repeat_content(column)\n","        feature_df[repeat_title] = column.apply(get_repeat_value, repeat_content = repeat_content)\n","        feature_titles.append(repeat_title)\n","\n","    return feature_df, feature_titles\n","\n","def create_submit_data(test_data, prob_list):\n","    prob = {\n","        \"zjnsrsbh\": test_data['zjnsrsbh'],\n","    }\n","    submit_data = pd.DataFrame(prob)\n","    #all_one_or_zero(prob_list)\n","    submit_data['Probability'] = prob_list\n","    return submit_data"],"cell_type":"code"},{"outputs":[{"output_type":"stream","name":"stdout","text":["(26000, 97) (26000, 1043) (20000,)\n","(20000, 97) (6000, 97)\n","(20000, 1043) (6000, 1043)\n"]}],"metadata":{},"execution_count":8,"source":["import copy\n","train_data = train_data.sample(frac=1)\n","train_x, train_y = train_data.drop('yc', axis=1), train_data['yc'].values\n","feature_df = copy.deepcopy(pd.concat([train_x, test_data], ignore_index=True))\n","# feature_df = fillna_with_mean(feature_df)\n","feature_df.head(1)\n","features, fea_columns = preprocess_feature(feature_df)\n","# backup: \n","invoice_columns = [x for x in fea_columns if 'hy' not in x and 'repeat' not in x and 'session' not in x]\n","#fea_columns = [x for x in fea_columns if 'hy' not in x and 'repeat' not in x]\n","invoice_features = features[invoice_columns]\n","hy_columns = [x for x in fea_columns if 'hy' in x]\n","hy_features = features[hy_columns]\n","\n","print(invoice_features.shape, hy_features.shape, train_y.shape)\n","invoice_features = invoice_features.values.astype(float)\n","hy_features = hy_features.values.astype(float)\n","\n","train_y = train_y.astype(float)\n","train_invoice, test_invoice = invoice_features[:20000,],invoice_features[20000:,]\n","train_hy, test_hy = hy_features[:20000,],hy_features[20000:,]\n","print(train_invoice.shape, test_invoice.shape)\n","print(train_hy.shape, test_hy.shape)"],"cell_type":"code"},{"metadata":{},"source":[],"cell_type":"markdown"},{"outputs":[],"metadata":{},"source":[],"cell_type":"code"},{"metadata":{},"source":["# package install"],"cell_type":"markdown"},{"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n","Requirement already satisfied: tensorflow==1.14 in /opt/conda/lib/python3.7/site-packages (1.14.0)\n","Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (0.33.4)\n","Requirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (0.1.8)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (1.1.0)\n","Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (1.14.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (3.7.1)\n","Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (0.8.1)\n","Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (1.0.8)\n","Requirement already satisfied: numpy<2.0,>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (1.16.4)\n","Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (1.1.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (1.11.2)\n","Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (1.12.0)\n","Requirement already satisfied: gast>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (0.3.2)\n","Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (1.14.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (1.20.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14) (0.9.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (41.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.1.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (0.16.0)\n","Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow==1.14) (2.10.0)\n","Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n","Requirement already satisfied: xgboost in /opt/conda/lib/python3.7/site-packages (0.90)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.3.0)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.16.4)\n"]}],"metadata":{},"execution_count":9,"source":["!pip install tensorflow==1.14\n","!pip install xgboost"],"cell_type":"code"},{"metadata":{},"source":["# DNN"],"cell_type":"markdown"},{"outputs":[],"metadata":{},"execution_count":10,"source":["class NNClassfier():\n","    def __init__(self, placeholders, learning_rate, weight_decay, use_hy=False):\n","        self.loss = 0\n","        self.accuracy = 0\n","        self.placeholders = placeholders\n","        self.weight_decay = weight_decay\n","        self.hidden_size = 100\n","        self.use_hy = use_hy\n","        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","        self.build()\n","        \n","    # def loss_func():\n","        \n","    def build(self):\n","        invoice_input = self.placeholders['invoice_input']\n","        invoice_out = tf.layers.dense(inputs=invoice_input, units=100, activation=tf.nn.relu)\n","        invoice_out = tf.layers.dense(inputs=invoice_out, units=100, activation=tf.nn.relu)\n","        \n","        hy_input = self.placeholders['hy_input']\n","        hy_out = tf.layers.dense(inputs=hy_input, units=100, activation=tf.nn.relu)\n","        hy_out = tf.layers.dense(inputs=hy_out, units=100, activation=tf.nn.relu)\n","        \n","        hidden_out = tf.concat([invoice_out, hy_out], axis=-1)\n","        hidden_out = tf.layers.dense(inputs=hidden_out, units=100, activation=tf.nn.relu)\n","        \n","        if not self.use_hy:\n","            hidden_out = invoice_out\n","        logits = tf.layers.dense(inputs=hidden_out, units=1, activation=None)\n","        self.scores = tf.nn.sigmoid(logits)\n","        \n","        varss = tf.trainable_variables()\n","        labels = self.placeholders['labels']\n","        self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n","        #self.loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(labels, logits, 0.1, name=None))\n","        self.loss += tf.add_n([tf.nn.l2_loss(v) for v in varss]) * self.weight_decay\n","        \n","        self.opt_op = self.optimizer.minimize(self.loss)\n","        \n","\n","def linear_model(X, y, class_weight={0: 0.1, 1: 0.9}):\n","    model = LogisticRegression(class_weight=class_weight, solver='lbfgs')\n","    model.fit(X, y)\n","    return model\n","\n","model = linear_model(train_invoice, train_y)"],"cell_type":"code"},{"outputs":[],"metadata":{},"execution_count":15,"source":["class InvoiceDetection():\n","    \n","    def __init__(self, invoice_dim, hy_dim=1, company_dim=1, repeat_dim=1, batch_size=512, lr=0.001, lambda_=1e-4, epoch=40, dropout=0.5):\n","        \n","        self.lr = lr\n","        self.lambda_ = lambda_\n","        self.dropout = dropout\n","        self.epoch = epoch\n","        self.batch_size = batch_size\n","        \n","        self.train_num = 0\n","        self.test_num = 0\n","        self.invoice_data = None\n","        self.hy_data = None\n","        self.label_data = None\n","        self.company_data = None\n","        self.repeat_data = None\n","        \n","        self.invoice_data_test = None\n","        self.hy_data_test = None\n","        self.label_data_test = None\n","        self.company_data_test = None\n","        self.repeat_data_test = None\n","        self.test_id = None\n","        \n","        self.placeholders = {\n","            \"invoice_input\": tf.placeholder(tf.float64, shape=(None, invoice_dim), name='invoice_input'),\n","            \"company_input\": tf.placeholder(tf.float64, shape=(None, company_dim), name='company_input'),\n","            \"hy_input\": tf.placeholder(tf.float64, shape=(None, hy_dim), name='hy_input'),\n","            \"repeat_input\": tf.placeholder(tf.float64, shape=(None, repeat_dim), name='repeat_input'),\n","            \"labels\": tf.placeholder(tf.float64, shape=(None, 1), name='labels')\n","        }\n","        \n","        self.model = NNClassfier(self.placeholders, self.lr, self.lambda_)\n","        \n","        # Initialize session\n","        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1.0)\n","        self.config = tf.ConfigProto(gpu_options=gpu_options)\n","        self.config.gpu_options.allow_growth = True\n","        self.sess = tf.Session(config=self.config)\n","        \n","        self.sess.run(tf.global_variables_initializer())\n","        self.sess.run(tf.local_variables_initializer())\n","        \n","        \n","    def construct_feed_dict(self, invoice_batch, label_batch, test_mode=False, hy_batch=[], company_batch=[], repeat_batch=[]):\n","        feed_dict = dict()\n","        if not test_mode:\n","            feed_dict.update({self.placeholders['labels']: label_batch})\n","            \n","        feed_dict.update({self.placeholders['invoice_input']: invoice_batch})\n","        \n","        if len(hy_batch) != 0:\n","            feed_dict.update({self.placeholders['hy_input']: hy_batch})\n","        if len(company_batch) != 0:\n","            feed_dict.update({self.placeholders['company_input']: company_batch})\n","        if len(repeat_batch) != 0:\n","            feed_dict.update({self.placeholders['repeat_input']: repeat_batch})\n","            \n","        return feed_dict\n","        \n","    def load_data(self, train_, test_):\n","        self.invoice_data, self.hy_data, self.label_data = train_\n","        self.test_id, self.invoice_data_test, self.hy_data_test = test_\n","        #self.invoice_data, self.label_data = train_\n","        #self.test_id, self.invoice_data_test = test_\n","        self.train_num = self.label_data.shape[0]\n","        self.test_num = self.test_id.shape[0]\n","\n","    # invoice_data, hy_data, label_data, company_data=None, repeat_data=None\n","    def train(self):\n","        # start...\n","        for epo in range(self.epoch):\n","            losses, count = 0, 0\n","            for i in range(0, self.train_num, self.batch_size):\n","                feed_dict = self.construct_feed_dict(self.invoice_data[i:i+self.batch_size, :],\n","                                                     self.label_data[i:i+self.batch_size, :],\n","                                                     hy_batch=self.hy_data[i:i+self.batch_size, :])\n","                #print(feed_dict)\n","                _, loss = self.sess.run([self.model.opt_op, self.model.loss], feed_dict=feed_dict)\n","                losses += loss\n","                count += 1\n","            # print(\"EPOCH\", epo, \" | LOSS\", loss, \" | ACC\", accuracy)\n","            print(\"EPOCH\", epo, \"| LOSS\", loss)\n","            if epo % 5 == 0:\n","                self.test()\n","            # print(\"EPOCH\", epo, \" | Train Loss\", losses / count, \" | Test Loss\", loss_test, \" | ACC\", acc)\n","            \n","    def test(self):\n","        losses, count, predict_proba = 0, 0, []\n","        for i in range(0, self.test_num, self.batch_size):\n","            feed_dict = self.construct_feed_dict(self.invoice_data_test[i:i+self.batch_size, :], None,\n","                                                 test_mode=True,\n","                                                 hy_batch=self.hy_data_test[i:i+self.batch_size, :]\n","                                                 )\n","            # print(feed_dict)\n","            probs = self.sess.run(self.model.scores, feed_dict=feed_dict)\n","            #losses += loss\n","            count += 1\n","            predict_proba += probs.T[0].tolist()\n","        \n","#         get_score.post_user_id('10')\n","#         predict_proba = np.array(predict_proba)\n","#         print(predict_proba[:10])\n","#         submit_data = create_submit_data(self.test_id, predict_proba)\n","#         get_score.post_verify_data(submit_data)"],"cell_type":"code"},{"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd78c24a828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd78c24a828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd78c24a828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd78c24a828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd78c24a828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd78c24a828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd78c24a828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd78c24a828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd784481d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd784481d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd784481d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd784481d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd784481d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd784481d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd784481d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd784481d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd7845efda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd7845efda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd7845efda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd7845efda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd7845efda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd7845efda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd7845efda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd7845efda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n","EPOCH 0 | LOSS 0.3059950571971946\n","EPOCH 1 | LOSS 0.21609742908266388\n","EPOCH 2 | LOSS 0.15229456156429227\n","EPOCH 3 | LOSS 0.12459314944224506\n","EPOCH 4 | LOSS 0.1170598972877252\n","EPOCH 5 | LOSS 0.11382046177682283\n","EPOCH 6 | LOSS 0.11219009040280115\n","EPOCH 7 | LOSS 0.11190093850523282\n","EPOCH 8 | LOSS 0.11329653273867446\n","EPOCH 9 | LOSS 0.11159888063521618\n","EPOCH 10 | LOSS 0.11270272668890532\n","EPOCH 11 | LOSS 0.1116691626684612\n","EPOCH 12 | LOSS 0.1100719408133644\n","EPOCH 13 | LOSS 0.1131694988882202\n","EPOCH 14 | LOSS 0.11204958270412763\n","EPOCH 15 | LOSS 0.11236799938345396\n","EPOCH 16 | LOSS 0.11140600802066145\n","EPOCH 17 | LOSS 0.11205063774511567\n","EPOCH 18 | LOSS 0.10915200079603622\n","EPOCH 19 | LOSS 0.11013590720935057\n","EPOCH 20 | LOSS 0.11274917205594953\n","EPOCH 21 | LOSS 0.10994386482809196\n","EPOCH 22 | LOSS 0.1126278281582967\n","EPOCH 23 | LOSS 0.10938084441750588\n","EPOCH 24 | LOSS 0.11179047847110904\n","EPOCH 25 | LOSS 0.10784722428880836\n","EPOCH 26 | LOSS 0.11252277576544978\n","EPOCH 27 | LOSS 0.11048199931041007\n","EPOCH 28 | LOSS 0.11158331411442707\n","EPOCH 29 | LOSS 0.11106570449055667\n","EPOCH 30 | LOSS 0.10693527112963182\n","EPOCH 31 | LOSS 0.1116922774303141\n","EPOCH 32 | LOSS 0.11043117210749598\n","EPOCH 33 | LOSS 0.1104491952863995\n","EPOCH 34 | LOSS 0.11000703001345143\n","EPOCH 35 | LOSS 0.10838668943253928\n","EPOCH 36 | LOSS 0.11242870875383906\n","EPOCH 37 | LOSS 0.11018414360675398\n","EPOCH 38 | LOSS 0.10961019563843277\n","EPOCH 39 | LOSS 0.11016637157248334\n"]}],"metadata":{},"execution_count":16,"source":["import tensorflow as tf\n","tf.__version__\n","\n","processor = InvoiceDetection(train_invoice.shape[1], train_hy.shape[1], lambda_=0.0001)\n","\n","train_ = (train_invoice, train_hy, train_y.reshape(-1,1))\n","test_ = (test_data[['zjnsrsbh']], test_invoice, test_hy)\n","processor.load_data(train_, test_)\n","\n","processor.train()"],"cell_type":"code"},{"outputs":[],"metadata":{},"source":[],"cell_type":"code"},{"metadata":{},"source":["# Xgboost"],"cell_type":"markdown"},{"outputs":[],"metadata":{},"execution_count":18,"source":["import xgboost\n","xgb = xgboost.XGBClassifier(nthread=4,\n","                          learning_rate=0.08,\n","                          n_estimators=250,\n","                          max_depth=5,\n","                          gamma=0,\n","                          subsample=0.9,\n","                          colsample_bytree=0.5)\n","\n","xgb.fit(train_invoice, train_y)\n","\n","get_score.post_user_id('10')\n","predict_proba = xgb.predict_proba(test_invoice)[:, 1].astype(np.float64)\n","submit_data = create_submit_data(test_data, predict_proba)\n","# get_score.post_verify_data(submit_data)"],"cell_type":"code"},{"metadata":{},"source":["# MLP"],"cell_type":"markdown"},{"outputs":[],"metadata":{},"source":["from sklearn.neural_network import MLPClassifier\n","# (0.001,0.001)\n","mlp = MLPClassifier(activation='relu',\n","                    solver='adam',\n","                    alpha=0.001, # 0.001, 0.01\n","                    learning_rate_init=0.001, # 0.05\n","                    max_iter=500,\n","                    learning_rate='invscaling') # constant, adaptive, invscaling\n","mlp.fit(train_invoice, train_y)\n","\n","get_score.post_user_id('10')\n","predict_proba = mlp.predict_proba(test_invoice)[:, 1].astype(np.float64)\n","submit_data = create_submit_data(test_data, predict_proba)\n","# get_score.post_verify_data(submit_data)"],"cell_type":"code"},{"metadata":{},"source":["# SVM"],"cell_type":"markdown"},{"outputs":[],"metadata":{},"source":["from sklearn.svm import SVC\n","svm = SVC(C=1.0, kernel='rbf', gamma='auto', probability=True)\n","svm.fit(train_invoice, train_y)\n","\n","get_score.post_user_id('10')\n","predict_proba = svm.predict_proba(test_invoice)[:, 1].astype(np.float64)\n","submit_data = create_submit_data(test_data, predict_proba)\n","# get_score.post_verify_data(submit_data)"],"cell_type":"code"},{"metadata":{},"source":["# bayes"],"cell_type":"markdown"},{"outputs":[],"metadata":{},"execution_count":17,"source":["from sklearn import naive_bayes\n","bayes = naive_bayes.BernoulliNB(alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None)\n","bayes.fit(train_invoice, train_y)\n","\n","get_score.post_user_id('10')\n","predict_proba = bayes.predict_proba(test_invoice, )[:, 1].astype(np.float64)\n","submit_data = create_submit_data(test_data, predict_proba)\n","# get_score.post_verify_data(submit_data)"],"cell_type":"code"}],"nbformat":4}