{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# 输入数据的参数\n",
    "_INPUT1='{\"name\":\"input1\",\"type\":0,\"uri\":\"tmp_ea2f608964aa41c59b340462308fec2c\"}'\n",
    "_INPUT2='{\"name\":\"input2\",\"type\":0,\"uri\":\"tmp_cd8aab065ed34729b31cffa65790616c\"}'\n",
    "_INPUT3='{\"name\":\"input3\",\"type\":0,\"uri\":\"tmp_770229d182cc4158a9e3cb65e0ba255b\"}'\n",
    "_INPUT4='{\"name\":\"input4\",\"type\":0,\"uri\":\"tmp_4011092c56f24d4aac41d2869c26f0fd\"}'\n",
    "\n",
    "# 输出数据的参数\n",
    "_OUTPUT='[{\"name\":\"output1\",\"type\":25,\"uri\":\"tmp_b7de8487d00743beb4a40c24e924f719\"},{\"name\":\"output2\",\"type\":0,\"uri\":\"tmp_2629f8357a204691bd88ca6bef528071\"},{\"name\":\"output3\",\"type\":0,\"uri\":\"tmp_a9c94ec2d984403fa39f1a403a95c88c\"},{\"name\":\"output4\",\"type\":0,\"uri\":\"tmp_b4f203fbc641427a84e396907493e9ea\"}]'\n",
    "\n",
    "# 自定义参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入使用库\n",
    "import pandas as pd\n",
    "from ustciscrLab_A import get_score\n",
    "from ustciscrBDL_B import get_score as get_scoreB\n",
    "import wfio\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取训练集数据 所有带标签数据\n",
    "def get_train_data():\n",
    "    train_data = wfio.read_dataframe(_INPUT1)\n",
    "    #删除第一行中文\n",
    "#     train_data = train_data.drop(0,axis=0,inplace=False)\n",
    "    return train_data\n",
    "\n",
    "#读取训练集数据 所有带标签数据\n",
    "def get_train_data2():\n",
    "    train_data = wfio.read_dataframe(_INPUT3)\n",
    "    #删除第一行中文\n",
    "#     train_data = train_data.drop(0,axis=0,inplace=False)\n",
    "    return train_data\n",
    "\n",
    "# train_data = get_train_data()\n",
    "# print(train_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       2017q1fphdsl 2017q1jxje 2017q1jxsl 2017q1kpje 2017q1kps 2017q1kpse  \\\n",
      "count         10000      10000      10000      10000     10000      10000   \n",
      "unique          188       2477          5       2477        98       2477   \n",
      "top               0          0          0          0         0          0   \n",
      "freq           7524       7524       7524       7524      7524       7524   \n",
      "\n",
      "       2017q1kpsl 2017q1rkse 2017q2fphdsl 2017q2jxje  ...         nsrmc  \\\n",
      "count       10000      10000        10000      10000  ...         10000   \n",
      "unique          5       1682          187       2502  ...          7563   \n",
      "top             0          0            0          0  ...  芜湖市商软冠联贸有限公司   \n",
      "freq         7524       8317         7499       7499  ...            17   \n",
      "\n",
      "       scjydz      xy   xydl    xyml   xyzl     yc   zcdz   zczby  \\\n",
      "count   10000   10000  10000   10000  10000  10000  10000   10000   \n",
      "unique   8312     506     77      17    274      2   8350     323   \n",
      "top        芜湖  其他专业咨询    批发业  批发和零售业  咨询与调查      0     芜湖  500000   \n",
      "freq      102     580   2668    3586    723   9000    102    2365   \n",
      "\n",
      "                                zjnsrsbh  \n",
      "count                              10000  \n",
      "unique                             10000  \n",
      "top     6af97d7deea1a1d2c76c5c512e66700b  \n",
      "freq                                   1  \n",
      "\n",
      "[4 rows x 114 columns]\n"
     ]
    }
   ],
   "source": [
    "print(get_train_data().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取测试集数据\n",
    "def get_test_data():\n",
    "    test_data = wfio.read_dataframe(_INPUT2)\n",
    "    #删除第一行中文\n",
    "#     test_data = test_data.drop(0,axis=0,inplace=False)\n",
    "    return test_data\n",
    "\n",
    "#读取测试集数据 B榜\n",
    "def get_test_data2():\n",
    "    test_data = wfio.read_dataframe(_INPUT4)\n",
    "    #删除第一行中文\n",
    "#     test_data = test_data.drop(0,axis=0,inplace=False)\n",
    "    return test_data\n",
    "# test_data = get_test_data()\n",
    "# print(test_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将标签数据和主数据对应行联系起来\n",
    "def get_label_data(train_data, label):\n",
    "    label_data = train_data[train_data['zjnsrsbh'].isin(label['zjnsrsbh'])]\n",
    "    return label_data\n",
    "\n",
    "train_data1_original = get_train_data()\n",
    "train_data2_original = get_train_data2().rename(columns = \n",
    "                                                {'bsrxmmp':'cwrysjh','bsrxm':'cwryxm','djrq':'djkyrq','fdbrxmp':'frsjh',\n",
    "                                                 'xzjd':'jdxz','zczb':'zczby','hy':'xy','hydl':'xydl','hyml':'xyml','hyzl':'xyzl'})\n",
    "test_data_original = get_test_data().rename(columns = \n",
    "                                                {'bsrxmmp':'cwrysjh','bsrxm':'cwryxm','djrq':'djkyrq','fdbrxmp':'frsjh',\n",
    "                                                 'xzjd':'jdxz','zczb':'zczby','hy':'xy','hydl':'xydl','hyml':'xyml','hyzl':'xyzl'})\n",
    "test_data2_original = get_test_data2().rename(columns = \n",
    "                                                {'bsrxmmp':'cwrysjh','bsrxm':'cwryxm','djrq':'djkyrq','fdbrxmp':'frsjh',\n",
    "                                                 'xzjd':'jdxz','zczb':'zczby','hy':'xy','hydl':'xydl','hyml':'xyml','hyzl':'xyzl'})\n",
    "\n",
    "# train_data_all_original = pd.concat([train_data_original, train_data2_original], sort=True)\n",
    "# print(train_data_all_original.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       2017q1fphdsl 2017q1jxje 2017q1jxsl 2017q1kpje 2017q1kps 2017q1kpse  \\\n",
      "count         10000      10000      10000      10000     10000      10000   \n",
      "unique          192       2518          5       2518        98       2518   \n",
      "top               0        0.0        0.0        0.0         0        0.0   \n",
      "freq           7483       7483       7483       7483      7483       7483   \n",
      "\n",
      "       2017q1kpsl 2017q1rkse 2017q2fphdsl 2017q2jxje  ...    xyml   xyzl  \\\n",
      "count       10000      10000        10000      10000  ...   10000  10000   \n",
      "unique          5       1649          189       2578  ...      19    276   \n",
      "top           0.0        0.0            0        0.0  ...  批发和零售业   群众团体   \n",
      "freq         7483       8352         7423       7423  ...    2757   1709   \n",
      "\n",
      "         jyfw          nsrmc scjydz            jdxz     yc   zcdz  zczby  \\\n",
      "count   10000          10000  10000           10000  10000  10000  10000   \n",
      "unique   6185           9207   6133              46      2   5972    341   \n",
      "top            芜湖市华泰通安商贸有限公司         芜湖市镜湖区滨江公共服务中心      0                 \n",
      "freq     3409             11   2933            2755   9000   3135   3951   \n",
      "\n",
      "                                zjnsrsbh  \n",
      "count                              10000  \n",
      "unique                             10000  \n",
      "top     745ade254260a798fa53e8da7048910c  \n",
      "freq                                   1  \n",
      "\n",
      "[4 rows x 114 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_data2_original.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def normalize(df, key):\n",
    "    #规格化\n",
    "#     s = (df[key] - df[key].min())/(df[key].max() - df[key].min())\n",
    "    s = df[key]\n",
    "    return s\n",
    "\n",
    "#  数据转换 只取8列*4季度*3年 \n",
    "def translate_data(old_data1):\n",
    "    old_data = old_data1.copy()\n",
    "    new_data = pd.DataFrame()\n",
    "    key = 'zjnsrsbh'\n",
    "    new_data[key] = old_data[key]\n",
    "    key = 'fddbrxm'\n",
    "    new_data[key] = old_data[key]\n",
    "    key = 'cwryxm'\n",
    "    new_data[key] = old_data[key]\n",
    "    key = 'nsrmc'\n",
    "    new_data[key] = old_data[key]\n",
    "    \n",
    "    \n",
    "    key = 'CYRS'.lower()\n",
    "    old_data[key] = old_data[key].replace('','0').astype('float')\n",
    "#     old_data[key] = preprocessing.scale(old_data[key]) # 标准化\n",
    "    old_data[key] = normalize(old_data, key) # 归一化\n",
    "    new_data[key] = old_data[key]\n",
    "    \n",
    "    key = 'zczby'.lower()\n",
    "    old_data[key] = old_data[key].replace('','0').astype('float')\n",
    "#     print(old_data[key][0:100])\n",
    "#     old_data[key] = preprocessing.scale(old_data[key])\n",
    "#     old_data[key] = normalize(old_data, key)\n",
    "#     new_data[key] = old_data[key]\n",
    "#     mm = MinMaxScaler()\n",
    "#     train_data = mm.fit_transform(train_data) # 归一化\n",
    "    hz = ['FPHDSL', 'JXJE', 'JXSL', 'KPJE', 'KPS', 'KPSE', 'KPSL', 'RKSE']\n",
    "    for i in range(3):\n",
    "        for j in range(4):\n",
    "            for h in hz:\n",
    "                key = '201'+str(7+i)+'q'+str(j+1)+h.lower()\n",
    "                old_data[key] = old_data[key].replace('','0').replace('\"\"\"\"\"\"\"\"\"\"\"\"\"\"', '0')\n",
    "                old_data[key] = old_data[key].astype('float')\n",
    "#                 old_data[key] = preprocessing.scale(old_data[key])\n",
    "                old_data[key] = normalize(old_data, key)\n",
    "                new_data[key] = old_data[key]\n",
    "#     print(new_data.describe())\n",
    "#     new_data = preprocessing.scale(new_data)\n",
    "#     print(new_data.std(axis=0))\n",
    "    return new_data\n",
    "# new_data = translate_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 检查生僻字模块\n",
    "import re\n",
    "def is_rare_name(string):\n",
    "    pattern = re.compile(u\"[~!@#$%^&* ]\")\n",
    "    match = pattern.search(string)\n",
    "    if match:\n",
    "        return True\n",
    "    try:\n",
    "        string.encode(\"gb2312\")\n",
    "    except UnicodeEncodeError:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# 提取季度数据累加和\n",
    "def total_quartdata(trainset,quartlist):\n",
    "    year = 2017\n",
    "    while year < 2020:\n",
    "        for i in range(4):\n",
    "            for key in quartlist:\n",
    "                #初始值\n",
    "                if year == 2017 and i == 0:\n",
    "                    trainset[key] = trainset[str(year)+'q'+str(i+1)+key]\n",
    "                else:\n",
    "                    trainset[key] += trainset[str(year)+'q'+str(i+1)+key]\n",
    "        year = year+1\n",
    "\n",
    "\n",
    "# 根据先验知识 提取额外的特征\n",
    "def get_features(trainset):\n",
    "    # 定义需要累加的列名列表\n",
    "    quartlist = ['fphdsl','jxje','kpje','kps','kpse','rkse']\n",
    "    total_quartdata(trainset, quartlist)\n",
    "    for key in quartlist:\n",
    "        trainset[key] = normalize(trainset, key)\n",
    "\n",
    "    # 计算开票数/(核定数+1)\n",
    "    trainset['kpbhd'] = trainset['kps']/(trainset['fphdsl']+1)\n",
    "    trainset['kpbhd'] = normalize(trainset, 'kpbhd')\n",
    "\n",
    "    # 平均每张票金额\n",
    "    trainset['fppjje'] = trainset['kpje']/(trainset['kps']+1)\n",
    "    trainset['fppjje'] = normalize(trainset, 'fppjje')\n",
    "\n",
    "    # 开票金额与开票数标准差\n",
    "    trainset['kpjebzc'] = 0\n",
    "    trainset['kpslbzc'] = 0\n",
    "    year = 2017\n",
    "    trainset['pjkpje'] = trainset['kpje']/12\n",
    "    trainset['pjkpsl'] = trainset['kps']/12\n",
    "    trainset['pjkpje'] = normalize(trainset, 'pjkpje')\n",
    "    trainset['pjkpsl'] = normalize(trainset, 'pjkpsl')\n",
    "    while year < 2020:\n",
    "        for i in range(4):\n",
    "            trainset['kpjebzc'] += (trainset[str(year)+'q'+str(i+1)+'kpje']-trainset['pjkpje'])*(trainset[str(year)+'q'+str(i+1)+'kpje']-trainset['pjkpje'])\n",
    "            trainset['kpslbzc'] += (trainset[str(year)+'q'+str(i+1)+'kps']-trainset['pjkpsl'])*(trainset[str(year)+'q'+str(i+1)+'kps']-trainset['pjkpsl'])\n",
    "        year = year+1\n",
    "    trainset['kpjebzc'] = np.sqrt(trainset['kpjebzc']/12)\n",
    "    trainset['kpslbzc'] = np.sqrt(trainset['kpslbzc']/12)\n",
    "    trainset['kpjebzc'] = normalize(trainset, 'kpjebzc')\n",
    "    trainset['kpslbzc'] = normalize(trainset, 'kpslbzc')\n",
    "    \n",
    "    # 人名相同\n",
    "    # 构造字典\n",
    "    namelist = trainset['fddbrxm'].tolist()+trainset['cwryxm'].tolist()\n",
    "    namelist.sort()\n",
    "    i = len(namelist)-1\n",
    "    namedic = {}\n",
    "    namedic[namelist[i]] = 1\n",
    "    flag = namelist[i]\n",
    "    while i >= 0:\n",
    "        i = i-1\n",
    "        if namelist[i] == flag:\n",
    "            namedic[namelist[i]] += 1\n",
    "        else:\n",
    "            namedic[namelist[i]] = 1\n",
    "            flag = namelist[i]\n",
    "\n",
    "    # 法人或者财务人员与其他公司重合\n",
    "    frorcwchlist = []\n",
    "    fddbrxmlist = trainset['fddbrxm'].tolist()\n",
    "    cwryxmlist = trainset['cwryxm'].tolist()\n",
    "    for i in range(len(trainset)):\n",
    "        temp = 0\n",
    "        if fddbrxmlist[i] == '' or cwryxmlist[i] == '':\n",
    "            pass\n",
    "        else:\n",
    "            temp += namedic[fddbrxmlist[i]]\n",
    "            temp += namedic[cwryxmlist[i]]\n",
    "        frorcwchlist.append(temp)\n",
    "    trainset['frorcwch'] = frorcwchlist\n",
    "    trainset['frorcwch'] = normalize(trainset, 'frorcwch')\n",
    "\n",
    "    # 企业名称生僻字\n",
    "    qymcspzlist = []\n",
    "    qymclist = trainset['nsrmc'].tolist()\n",
    "    for i in range(len(trainset)):\n",
    "        temp = 0\n",
    "        if is_rare_name(qymclist[i]):\n",
    "            temp = 1\n",
    "        qymcspzlist.append(temp)\n",
    "    trainset['qymcspz'] = qymcspzlist\n",
    "    trainset['qymcspz'] = normalize(trainset, 'qymcspz')\n",
    "    \n",
    "    return trainset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理训练集不均衡 扩充正例\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "droplist = ['zjnsrsbh', 'fddbrxm', 'cwryxm', 'nsrmc']\n",
    "\n",
    "x_train1 = translate_data(train_data1_original)\n",
    "# x_train1 = get_features(x_train1)\n",
    "for key in droplist:\n",
    "    x_train1 = x_train1.drop([key], axis=1, inplace=False) # 删除非数值\n",
    "\n",
    "x_train2 = translate_data(train_data2_original)\n",
    "# x_train2 = get_features(x_train2)\n",
    "for key in droplist:\n",
    "    x_train2 = x_train2.drop([key], axis=1, inplace=False) # 删除非数值\n",
    "\n",
    "x_train = x_train1\n",
    "x_train = pd.concat([x_train1, x_train2])  # 连接两个测试集的特征数据\n",
    "print(x_train.describe())\n",
    "\n",
    "y_train1 = train_data1_original['yc']\n",
    "y_train2 = train_data2_original['yc']\n",
    "\n",
    "y_train = y_train1\n",
    "y_train = pd.concat([train_data1_original['yc'], train_data2_original['yc']])  # 连接两个测试集的标签数据\n",
    "\n",
    "test_data = test_data_original\n",
    "test_data2 = test_data2_original\n",
    "\n",
    "oversampler=SMOTE(random_state=42, k_neighbors=5)\n",
    "# X, Y = x_train[:], y_train[:]\n",
    "X, Y=oversampler.fit_sample(x_train[:], y_train[:])\n",
    "print('X:{}, Y:{}'.format(len(X), len(Y)))\n",
    "print('y_train=1:{}'.format(len(y_train[:][y_train[:]=='1'])))\n",
    "print('Y=1:{}'.format(len(Y[:][Y[:]=='1'])))\n",
    "x_test = X\n",
    "y_test = Y\n",
    "\n",
    "# 采样 训练集和测试集 打乱数据 \n",
    "# x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "# X = x_train\n",
    "# Y = y_train\n",
    "# print('x_train:{} x_test:{}'.format(len(x_train), len(x_test)))\n",
    "# print('y_train=1:{}'.format(len(y_train[:][y_train[:]=='1'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n",
      "The accuracy of the MLP is: 0.9823333333333333, iter times: 13\n",
      "                 0            1\n",
      "count  6000.000000  6000.000000\n",
      "mean      0.497932     0.502068\n",
      "std       0.483990     0.483990\n",
      "min       0.000000     0.000000\n",
      "25%       0.000000     0.000000\n",
      "50%       0.664165     0.335835\n",
      "75%       1.000000     1.000000\n",
      "max       1.000000     1.000000\n",
      "2699\n",
      "2871\n",
      "5569\n",
      "[0.         1.         0.         1.         1.         0.\n",
      " 1.         0.         1.         1.         0.         1.\n",
      " 0.         1.         1.         1.         1.         1.\n",
      " 1.         0.         0.         1.         1.         1.\n",
      " 0.         0.         1.         0.         1.         0.33117455\n",
      " 1.         1.         0.         1.         0.         1.\n",
      " 0.         1.         1.         1.         0.         1.\n",
      " 0.         0.         1.         0.         1.         1.\n",
      " 0.         0.33583461 0.         0.         0.         0.\n",
      " 0.         0.16591471 1.         1.         1.         0.\n",
      " 1.         1.         0.         0.         1.         1.\n",
      " 0.         0.         0.         1.         1.         0.34052686\n",
      " 0.         0.         1.         1.         1.         0.\n",
      " 1.         1.         1.         0.         0.         0.\n",
      " 0.         1.         0.         0.         0.         0.\n",
      " 1.         0.         1.         1.         1.         0.\n",
      " 1.         1.         0.         0.        ]\n",
      "                 0            1\n",
      "count  6000.000000  6000.000000\n",
      "mean      0.499193     0.500807\n",
      "std       0.484748     0.484748\n",
      "min       0.000000     0.000000\n",
      "25%       0.000000     0.000000\n",
      "50%       0.659473     0.340527\n",
      "75%       1.000000     1.000000\n",
      "max       1.000000     1.000000\n",
      "2720\n",
      "2870\n",
      "5587\n",
      "[1.         0.34052686 1.         0.         1.         1.\n",
      " 0.         1.         0.         0.         1.         1.\n",
      " 0.         0.         1.         0.         0.         0.\n",
      " 0.         0.         1.         0.         1.         0.\n",
      " 0.         0.         0.         1.         1.         0.34052686\n",
      " 0.         1.         1.         1.         0.         0.\n",
      " 1.         1.         0.         1.         0.         0.\n",
      " 1.         0.33350051 0.         0.34052686 0.         0.\n",
      " 0.         1.         0.         1.         1.         1.\n",
      " 1.         0.         1.         0.         1.         1.\n",
      " 0.         1.         0.         1.         0.         1.\n",
      " 1.         0.34052686 1.         0.         1.         1.\n",
      " 0.         1.         0.         0.         1.         0.\n",
      " 0.         1.         1.         1.         0.         0.\n",
      " 0.01147957 1.         0.         0.         1.         0.\n",
      " 0.34052686 0.         1.         1.         1.         0.\n",
      " 1.         0.         1.         1.        ]\n",
      "User-id:10\n",
      "程序开始运行时间为：2019-12-21 14:42:51.228733\n",
      "程序结束运行时间为：2019-12-21 14:42:52.641936\n",
      "程序运行时间（去除打分耗时）为：1.413203\n",
      "程序的准确率为：95.62995654\n",
      "最终得分为：96.06696089\n",
      "User-id:10\n",
      "程序开始运行时间为：2019-12-21 14:42:51.228814\n",
      "程序结束运行时间为：2019-12-21 14:42:52.774084\n",
      "程序运行时间（去除打分耗时）为：1.54527\n",
      "程序的准确率为：95.74557234\n",
      "最终得分为：96.17101511\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import scipy\n",
    "\n",
    "# 使用MLP训练 分类\n",
    "def mlp_train():\n",
    "    _x_train = X\n",
    "    _y_train = Y.astype('float')\n",
    "    _x_test = x_test\n",
    "    _y_test = y_test.astype('float')\n",
    "#     print(len(_x_train))\n",
    "    clf = MLPClassifier(activation='identity', solver='adam', hidden_layer_sizes=(9, ),\n",
    "                    learning_rate='constant', learning_rate_init=0.01, alpha=1e-4,\n",
    "                    tol=1e-3, max_iter=500, random_state=43, early_stopping=False, shuffle=True)\n",
    "    clf.fit(_x_train[:], _y_train[:])\n",
    "    print(type(_y_train[0]))\n",
    "    prediction = clf.predict(_x_test[:])\n",
    "    print('The accuracy of the MLP is: {0}, iter times: {1}'.\n",
    "          format(metrics.accuracy_score(prediction, _y_test[:]), clf.n_iter_))\n",
    "#     print('w: {}'.format(sorted(abs(clf.coefs_[0]), reverse=True)))\n",
    "#     print(clf.coefs_[0])\n",
    "#     print(clf.intercepts_)\n",
    "#     num_train = 18000\n",
    "#     print(len(y_train[:num_train][y_train[:num_train] == '0']), len(y_train[:num_train][y_train[:num_train] == '1']))\n",
    "    return clf\n",
    "\n",
    "# MLP参数网格搜索\n",
    "def mlp_search_parameters():\n",
    "    _x_train = X\n",
    "    _y_train = Y\n",
    "    _x_test = x_test\n",
    "    _y_test = y_test\n",
    "    mlp = MLPClassifier(activation='relu', solver='adam', hidden_layer_sizes=(9, ),\n",
    "                    learning_rate='constant', learning_rate_init=0.01, alpha=1e-3,\n",
    "                    tol=1e-3, max_iter=500, random_state=43, early_stopping=False, shuffle=True)\n",
    "    \n",
    "    parameters = {'learning_rate_init':np.linspace(0.0001, 0.1, 10), 'hidden_layer_sizes': ((8, ), (9, ), (10, ))} #超参数选择空间\n",
    "    clf = RandomizedSearchCV(mlp, parameters, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    \n",
    "    clf.fit(_x_train[:], _y_train[:])\n",
    "    print(clf.best_estimator_)#最好的模型\n",
    "    print(clf.best_score_)#最好模型的得分\n",
    "    print(clf.best_params_)#最好模型的参数\n",
    "    \n",
    "    prediction = clf.predict(_x_test[:])\n",
    "    print('The accuracy of the MLP is: {0}'.\n",
    "          format(metrics.accuracy_score(prediction, _y_test[:])))\n",
    "    return clf\n",
    "\n",
    "# 使用SVM训练\n",
    "def svm_train():\n",
    "    _x_train = X\n",
    "    _y_train = Y\n",
    "    clf = svm.SVC(C=1.0, kernel='linear', tol=1e-4, gamma='auto', probability=True)\n",
    "    clf.fit(_x_train[:], _y_train[:])\n",
    "    \n",
    "    prediction = clf.predict(_x_train[:])\n",
    "    print('The accuracy of the SVM is: {0}'.\n",
    "          format(metrics.accuracy_score(prediction, _y_train[:])))\n",
    "    return clf\n",
    "\n",
    "# 决策树训练\n",
    "def tree_train():\n",
    "    _x_train = X\n",
    "    _y_train = Y\n",
    "    _x_test = x_test\n",
    "    _y_test = y_test\n",
    "    clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=25, min_samples_split=10,\n",
    "                                      min_samples_leaf=1, max_features=43, random_state=42)\n",
    "    clf = clf.fit(_x_train, _y_train)\n",
    "    prediction = clf.predict(_x_test[:])\n",
    "    print('The accuracy of the tree is: {0}'.\n",
    "          format(metrics.accuracy_score(prediction, _y_test[:])))\n",
    "    print('feature_importances_:\\n{}'.format(str(_x_train.columns.tolist()) + ':' + str(clf.feature_importances_[:])))\n",
    "    return clf\n",
    "\n",
    "# 决策树预测\n",
    "def tree_predict(clf):\n",
    "    prob = {\n",
    "        \"zjnsrsbh\": test_data['zjnsrsbh'],\n",
    "    }\n",
    "    verify_data = pd.DataFrame(prob)\n",
    "    \n",
    "    _x_test = translate_data(test_data).drop(['zjnsrsbh'], axis=1, inplace=False).astype('float')\n",
    "#     _x_test = get_features(_x_test)\n",
    "    prediction = clf.predict_proba(_x_test) # 输出概率\n",
    "    print(pd.DataFrame(prediction).describe())\n",
    "    prediction = prediction[:,1]\n",
    "    \n",
    "#     print(len(prediction[prediction[:] == 1.0]))  # 输出标签\n",
    "    print(len(prediction[prediction[:] == 1.0])+len(prediction[prediction[:] == 0.0]))  # 输出标签\n",
    "#     print(prediction[0:100])\n",
    "    verify_data['Probability'] = prediction[:]   # 输出标签\n",
    "    return verify_data\n",
    "\n",
    "    \n",
    "def mlp_predict(clf):\n",
    "    prob = {\n",
    "        \"zjnsrsbh\": test_data['zjnsrsbh'],\n",
    "    }\n",
    "    verify_data = pd.DataFrame(prob)\n",
    "    \n",
    "    _x_test = translate_data(test_data)\n",
    "#     _x_test = get_features(_x_test)\n",
    "    for key in droplist:\n",
    "        _x_test = _x_test.drop([key], axis=1, inplace=False) # 删除非数值\n",
    "    _x_test = _x_test.astype('float')\n",
    "#     prediction = clf.predict(_x_test) # 输出标签\n",
    "    prediction = clf.predict_proba(_x_test)  # 输出概率\n",
    "    print(pd.DataFrame(prediction).describe())\n",
    "    prediction = prediction[:,1]\n",
    "#     prediction[prediction<0.2] = 0\n",
    "#     prediction[prediction>0.7] = 1\n",
    "    \n",
    "    print(len(prediction[prediction[:] < 0.1]))  # 输出标签\n",
    "    print(len(prediction[prediction[:] > 0.9]))  # 输出标签\n",
    "    print(len(prediction[prediction[:] == 1.0])+len(prediction[prediction[:] == 0.0]))  # 输出标签\n",
    "#     print(len(prediction[prediction[:,1] == 1.0])+len(prediction[prediction[:,1] == 0.0]))  # 输出概率\n",
    "#     print(len(prediction), len(verify_data))\n",
    "    print(prediction[0:100])\n",
    "    verify_data['Probability'] = prediction[:]   # 输出标签\n",
    "#     verify_data['Probability'] = prediction[:,1]  # 输出概率\n",
    "    return verify_data\n",
    "\n",
    "# B榜预测\n",
    "def mlp_predict_B(clf):\n",
    "    prob = {\n",
    "        \"zjnsrsbh\": test_data2['zjnsrsbh'],\n",
    "    }\n",
    "    verify_data = pd.DataFrame(prob)\n",
    "    \n",
    "    _x_test = translate_data(test_data2)\n",
    "#     _x_test = get_features(_x_test)\n",
    "    for key in droplist:  \n",
    "        _x_test = _x_test.drop([key], axis=1, inplace=False) # 删除非数值\n",
    "    _x_test = _x_test.astype('float')\n",
    "#     prediction = clf.predict(_x_test) # 输出标签\n",
    "    prediction = clf.predict_proba(_x_test)  # 输出概率\n",
    "    print(pd.DataFrame(prediction).describe())\n",
    "    prediction = prediction[:,1]\n",
    "#     prediction[prediction<0.2] = 0\n",
    "#     prediction[prediction>0.7] = 1\n",
    "    \n",
    "    print(len(prediction[prediction[:] < 0.1]))  # 输出标签\n",
    "    print(len(prediction[prediction[:] > 0.9]))  # 输出标签\n",
    "    print(len(prediction[prediction[:] == 1.0])+len(prediction[prediction[:] == 0.0]))  # 输出标签\n",
    "#     print(len(prediction[prediction[:,1] == 1.0])+len(prediction[prediction[:,1] == 0.0]))  # 输出概率\n",
    "#     print(len(prediction), len(verify_data))\n",
    "    print(prediction[0:100])\n",
    "    verify_data['Probability'] = prediction[:]   # 输出标签\n",
    "#     verify_data['Probability'] = prediction[:,1]  # 输出概率\n",
    "    return verify_data\n",
    "    \n",
    "    \n",
    "# clf = mlp_search_parameters() # MLP参数网格搜索\n",
    "clf = mlp_train()  # MLP训练\n",
    "# clf = svm_train()  #  SVM训练\n",
    "# clf = tree_train()  # 决策树训练\n",
    "get_score.post_user_id('10')\n",
    "get_scoreB.post_user_id('10')\n",
    "\n",
    "# 提交验证集进行打榜\n",
    "verify_data = mlp_predict(clf)  # mlp预测\n",
    "verify_dataB = mlp_predict_B(clf)  # mlp预测\n",
    "# print(verify_data.describe())\n",
    "# print(verify_data[:100])\n",
    "get_score.post_verify_data(verify_data)\n",
    "get_scoreB.post_verify_data(verify_dataB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用测试集（训练集其中的1000条数据）来实现打分，传输自己的结果和测试集数据\n",
    "#提交选手id，这一步必须先于模型训练之前，否则判断坐标\n",
    "get_score.post_user_id('10')\n",
    "'''\n",
    "提交随机产生的答案和测试集数据，都是dataframe格式，两者有相同列'zjnsrsbh'，用于拼接\n",
    "其中user_test_data只有两个字段'zjnsrsbh'和'Probability','Probability'中应为自己模型训练出的结果概率，\n",
    "概率值应为[0,1]之间\n",
    "返回结果：选手id，程序开始运行时间，程序结束运行时间，程序耗时，代码分，最终分（代码分和时间分）\n",
    "测试集分数不参与打榜\n",
    "'''\n",
    "_x_train1 = translate_data(train_data1_original)\n",
    "_x_train2 = translate_data(train_data2_original)\n",
    "_x_train = pd.concat([_x_train1, _x_train2])  # 连接两个测试集的特征数据\n",
    "prediction = clf.predict_proba(_x_train[:].drop(['zjnsrsbh'], axis=1, inplace=False))\n",
    "prediction = prediction[:, 1]\n",
    "prediction[prediction>=0.5] = 0.99\n",
    "prediction[prediction<0.5] = 0.01\n",
    "prob = {\n",
    "        \"zjnsrsbh\": _x_train['zjnsrsbh'],\n",
    "    }\n",
    "verify_data = pd.DataFrame(prob)\n",
    "verify_data['Probability'] = prediction\n",
    "\n",
    "col_n = ['yc', 'zjnsrsbh']\n",
    "a = pd.DataFrame(train_data1_original,columns = col_n)\n",
    "b = pd.DataFrame(train_data2_original,columns = col_n)\n",
    "_y = pd.concat([a, b])\n",
    "\n",
    "get_score.post_test_data(verify_data, _y)\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "print('交叉熵损失：%.8f' % (100*(1-log_loss(_y['yc'], prediction))))  # 交叉熵损失 \n",
    "c = [1, 1, 1, 1, 0]\n",
    "d = [1, 1, 1, 0.001, 0]\n",
    "print(log_loss(c, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the MLP is: 0.997, iter times: 21\n",
      "                 0             1\n",
      "count  6000.000000  6.000000e+03\n",
      "mean      0.670840  3.291600e-01\n",
      "std       0.430898  4.308985e-01\n",
      "min       0.000000  2.677151e-18\n",
      "25%       0.050677  2.172468e-05\n",
      "50%       0.964497  3.550285e-02\n",
      "75%       0.999978  9.493234e-01\n",
      "max       1.000000  1.000000e+00\n",
      "3249\n",
      "1574\n",
      "16\n",
      "[5.31676207e-03 9.99729606e-01 4.07170459e-06 9.97427870e-01\n",
      " 1.00000000e+00 7.46051043e-04 2.64607274e-01 1.08484692e-02\n",
      " 1.00000000e+00 2.36249355e-02 3.76423588e-07 9.99976277e-01\n",
      " 1.53232806e-11 9.99998988e-01 8.03728187e-01 1.15098803e-02\n",
      " 9.99999693e-01 4.97816014e-02 1.00000000e+00 1.06163143e-07\n",
      " 3.61594817e-03 9.99981063e-01 2.01224978e-01 3.98486485e-01\n",
      " 5.90565113e-07 2.03664920e-02 9.99999771e-01 1.14680920e-03\n",
      " 9.99994282e-01 1.07062944e-01 1.00000000e+00 8.82009778e-01\n",
      " 1.76004519e-08 9.98847320e-01 6.93786624e-06 9.99787606e-01\n",
      " 1.13120033e-05 9.99998054e-01 8.45993497e-01 7.41008460e-02\n",
      " 2.18841467e-03 3.74673840e-03 1.48134249e-05 1.14081310e-06\n",
      " 9.99993108e-01 7.37430628e-04 9.73892740e-01 5.57718088e-02\n",
      " 1.10047684e-09 1.07063699e-01 5.88171680e-04 2.10867809e-04\n",
      " 1.28532839e-07 5.66057991e-10 1.17128029e-05 1.07011952e-01\n",
      " 1.00000000e+00 1.00000000e+00 6.32297644e-02 2.20622201e-06\n",
      " 1.93417931e-01 6.92857835e-01 5.13630978e-07 1.36676102e-03\n",
      " 2.62000696e-01 8.39525478e-01 2.48568954e-04 1.66654685e-07\n",
      " 2.07040221e-03 1.00000000e+00 9.93036832e-01 1.07065017e-01\n",
      " 2.43006222e-02 5.15073541e-04 2.02341612e-01 9.99980079e-01\n",
      " 9.97857805e-01 3.33082337e-05 1.27147952e-01 2.72340577e-03\n",
      " 9.99999227e-01 4.09327740e-09 9.22212357e-05 6.98199773e-03\n",
      " 1.64303168e-10 9.98109065e-01 1.00301076e-06 5.81747457e-13\n",
      " 9.93515887e-08 1.00348995e-05 9.99999997e-01 3.47311347e-04\n",
      " 9.12591743e-01 9.99998945e-01 9.99998432e-01 4.83795992e-05\n",
      " 1.00000000e+00 3.19084035e-02 1.27432985e-05 1.74322273e-06]\n",
      "User-id:10\n",
      "程序开始运行时间为：2019-12-19 15:26:14.504391\n",
      "程序结束运行时间为：2019-12-19 15:26:14.914066\n",
      "程序运行时间（去除打分耗时）为：0.409675\n",
      "程序的准确率为：45.58513509\n",
      "最终得分为：51.02662159\n"
     ]
    }
   ],
   "source": [
    "clf = mlp_train()  #训练\n",
    "\n",
    "#提交选手id，这一步必须先于模型训练之前\n",
    "#用训练集所有的数据（9999）条来打分，只传输自己得出的结果\n",
    "#其中user_verify_data只有两个字段'zjnsrsbh'和'Probability','Probability'中应为自己模型训练出的结果概率，\n",
    "#概率值应为[0,1]之间\n",
    "get_score.post_user_id('10')\n",
    "\n",
    "# 提交验证集进行打榜\n",
    "verify_data = mlp_predict(clf)  #预测\n",
    "get_score.post_verify_data(verify_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: imbalanced-learn in /opt/conda/lib/python3.7/site-packages (0.6.1)\n",
      "Collecting scikit-learn>=0.22\n",
      "\u001b[?25l  Downloading http://mirrors.aliyun.com/pypi/packages/19/96/8034e350d4550748277e514d0d6d91bdd36be19e6c5f40b8af0d74cb0c84/scikit_learn-0.22-cp37-cp37m-manylinux1_x86_64.whl (7.0MB)\n",
      "\u001b[K     |████████████████████████████████| 7.0MB 3.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.17 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn) (1.16.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn) (0.13.2)\n",
      "Installing collected packages: scikit-learn\n",
      "  Found existing installation: scikit-learn 0.21.3\n",
      "    Uninstalling scikit-learn-0.21.3:\n",
      "      Successfully uninstalled scikit-learn-0.21.3\n",
      "Successfully installed scikit-learn-0.22\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting scikit-learn==0.21.3\n",
      "\u001b[?25l  Downloading http://mirrors.aliyun.com/pypi/packages/9f/c5/e5267eb84994e9a92a2c6a6ee768514f255d036f3c8378acfa694e9f2c99/scikit_learn-0.21.3-cp37-cp37m-manylinux1_x86_64.whl (6.7MB)\n",
      "\u001b[K     |████████████████████████████████| 6.7MB 3.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.21.3) (1.16.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.21.3) (0.13.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.21.3) (1.3.0)\n",
      "\u001b[31mERROR: imbalanced-learn 0.6.1 has requirement scikit-learn>=0.22, but you'll have scikit-learn 0.21.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: scikit-learn\n",
      "  Found existing installation: scikit-learn 0.22\n",
      "    Uninstalling scikit-learn-0.22:\n",
      "      Successfully uninstalled scikit-learn-0.22\n",
      "Successfully installed scikit-learn-0.21.3\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==0.21.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.3.1+cpu\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/cpu/torch-1.3.1%2Bcpu-cp37-cp37m-linux_x86_64.whl (111.8MB)\n",
      "\u001b[K     |████████████████████████████████| 111.8MB 2.1MB/s \n",
      "\u001b[?25hCollecting torchvision==0.4.2+cpu\n",
      "\u001b[?25l  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.4.2%2Bcpu-cp37-cp37m-linux_x86_64.whl (13.5MB)\n",
      "\u001b[K     |████████████████████████████████| 13.6MB 780kB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==1.3.1+cpu) (1.16.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from torchvision==0.4.2+cpu) (1.12.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.4.2+cpu) (6.2.1)\n",
      "Installing collected packages: torch, torchvision\n",
      "Successfully installed torch-1.3.1+cpu torchvision-0.4.2+cpu\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch==1.3.1+cpu torchvision==0.4.2+cpu -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "nh = 16\n",
    "\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn=nn.LSTM(\n",
    "            input_size=8,\n",
    "            hidden_size=nh,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.out=nn.Sequential(\n",
    "            nn.Linear(in_features=nh*2,out_features=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        # 一下关于shape的注释只针对单项\n",
    "        # output: [batch_size, time_step, hidden_size]\n",
    "        # h_n: [num_layers,batch_size, hidden_size] # 虽然LSTM的batch_first为True,但是h_n/c_n的第一维还是num_layers\n",
    "        # c_n: 同h_n\n",
    "        output,(h_n,c_n)=self.rnn(x)\n",
    "#         print(output.size())\n",
    "        # output_in_last_timestep=output[:,-1,:] # 也是可以的\n",
    "        output_in_last_timestep=h_n[-1,:,:]\n",
    "        # print(output_in_last_timestep.equal(output[:,-1,:])) #ture\n",
    "        x=self.out(output_in_last_timestep)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = './model/lstm_model_h64.pth'\n",
    "PT_MODEL_NAME = './model/lstm_model_h64.pt'\n",
    "DEVICE = 'cpu'\n",
    "EPOCHS = 5\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 加载数据\n",
    "    dataloader = get_train_data_loader(BATCH_SIZE)\n",
    "    # showSample(dataloader)\n",
    "    \n",
    "    test_dataloader = get_train_data_loader(6000)\n",
    "    testdata_iter=iter(test_dataloader)\n",
    "    test_x,test_y=testdata_iter.next()\n",
    "    print(test_x.shape)\n",
    "    test_x=test_x.view(-1, 12, 8)\n",
    "    \n",
    "    # 2. 网络搭建\n",
    "    try:\n",
    "        crnn = torch.jit.load(PT_MODEL_NAME, map_location=DEVICE)  # 通过jit读取模型\n",
    "        print('load {}'.format(PT_MODEL_NAME))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        net=RNN()\n",
    "        try:\n",
    "            net.load_state_dict(torch.load(MODEL_NAME, map_location=DEVICE))\n",
    "            print('load {}'.format(MODEL_NAME))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    # 3. 网络的训练\n",
    "    optimizer=torch.optim.Adam(net.parameters(),lr=0.001)\n",
    "    loss_F=torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(EPOCHS): # 数据集只迭代一次\n",
    "        for step, (x, y) in enumerate(dataloader):\n",
    "            x_ = x.view(-1, 12, 8)\n",
    "            pred=net(x_)\n",
    "#             print(pred)\n",
    "#             loss=loss_F(pred, y) # 计算loss\n",
    "            pred = pred.view(-1)\n",
    "            loss = nn.functional.binary_cross_entropy(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if step%50==49: # 每50步，计算精度\n",
    "                with torch.no_grad():\n",
    "                    test_pred=net(test_x)\n",
    "#                     prob=torch.nn.functional.softmax(test_pred,dim=1)\n",
    "#                     pred_cls=torch.argmax(prob,dim=1)\n",
    "#                     print(list(zip(pred_cls.tolist(), test_y.tolist())))\n",
    "                    test_pred = test_pred.view(-1)\n",
    "                    acc = 1 - nn.functional.binary_cross_entropy(test_pred, test_y)\n",
    "#                     acc=(pred_cls==test_y).sum().numpy()/pred_cls.size()[0]\n",
    "                    print(f\"{epoch}-{step}: accuracy:{acc}\")\n",
    "            if step%100==99: # 每100步，提交一次\n",
    "                with torch.no_grad():\n",
    "                    get_score.post_user_id('10')\n",
    "                    # 提交验证集进行打榜\n",
    "                    verify_data = lstm_predict(test_data, net)\n",
    "                    get_score.post_verify_data(verify_data)\n",
    "                    \n",
    "                    get_scoreB.post_user_id('10')\n",
    "                    # 提交验证集进行打榜\n",
    "                    verify_dataB = lstm_predict(test_data2, net)\n",
    "                    get_scoreB.post_verify_data(verify_dataB)\n",
    "                    \n",
    "                    torch.save(net.state_dict(), MODEL_NAME)  # save\n",
    "#                     model = torch.jit.script(net)  # 保存模型\n",
    "#                     model.save(PT_MODEL_NAME)\n",
    "                    print(\"save model\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove('./mnist/MNIST/raw/train-images-idx3-ubyte.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.5, ], std=[0.5, ])\n",
    "])\n",
    "\n",
    "class mydataset(Dataset):\n",
    "\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.x = x\n",
    "        self.y = y.astype('float') # float型的结果\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x.loc[idx][1:].tolist() # 96维特征\n",
    "        y = self.y[idx]\n",
    "        x = torch.from_numpy(np.array(x)).float()\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "    \n",
    "def get_train_data_loader(batch_size):\n",
    "    dataset = mydataset(X, Y, transform=transform)\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True,\n",
    "                      pin_memory=False)\n",
    "\n",
    "# def get_test_data_loader(batch_size):\n",
    "#     dataset = mydataset(test_data, transform=transform)\n",
    "\n",
    "#     return DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[0][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B榜预测\n",
    "def lstm_predict(_test_data, model):\n",
    "    prob = {\n",
    "        \"zjnsrsbh\": _test_data['zjnsrsbh'],\n",
    "    }\n",
    "    verify_data = pd.DataFrame(prob)\n",
    "    \n",
    "    _x_test = translate_data(_test_data)\n",
    "#     _x_test = get_features(_x_test)\n",
    "    for key in droplist:  \n",
    "        _x_test = _x_test.drop([key], axis=1, inplace=False) # 删除非数值\n",
    "    _x_test = _x_test.astype('float')\n",
    "    _x_test = torch.tensor(_x_test.iloc[:,1:].values, dtype=torch.float32)\n",
    "    _x_test=_x_test.view(-1, 12, 8)\n",
    "#     prediction = clf.predict(_x_test) # 输出标签\n",
    "    test_pred = model(_x_test)  # 输出概率\n",
    "    test_pred = test_pred.view(-1)\n",
    "#     prob=torch.nn.functional.softmax(test_pred,dim=1)\n",
    "#     prediction=torch.argmax(prob,dim=1)\n",
    "    prediction = test_pred.numpy()\n",
    "    print(pd.DataFrame(prediction).describe())\n",
    "#     prediction = prediction[:,1]\n",
    "#     prediction[prediction<0.2] = 0\n",
    "#     prediction[prediction>0.7] = 1\n",
    "    \n",
    "    print(len(prediction[prediction[:] < 0.1]))  # 输出标签\n",
    "    print(len(prediction[prediction[:] > 0.9]))  # 输出标签\n",
    "    print(len(prediction[prediction[:] == 1.0])+len(prediction[prediction[:] == 0.0]))  # 输出标签\n",
    "#     print(len(prediction[prediction[:,1] == 1.0])+len(prediction[prediction[:,1] == 0.0]))  # 输出概率\n",
    "#     print(len(prediction), len(verify_data))\n",
    "    print(prediction[0:10])\n",
    "    verify_data['Probability'] = prediction[:]   # 输出标签\n",
    "#     verify_data['Probability'] = prediction[:,1]  # 输出概率\n",
    "    return verify_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n"
     ]
    }
   ],
   "source": [
    "_x_test = translate_data(test_data2)\n",
    "#     _x_test = get_features(_x_test)\n",
    "for key in droplist:  \n",
    "    _x_test = _x_test.drop([key], axis=1, inplace=False) # 删除非数值\n",
    "_x_test = _x_test.astype('float')\n",
    "\n",
    "# 判断96维全0 个数\n",
    "def len_zero(_data):\n",
    "    l = 0\n",
    "    iszero = True\n",
    "    hz = ['FPHDSL', 'JXJE', 'JXSL', 'KPJE', 'KPS', 'KPSE', 'KPSL', 'RKSE']\n",
    "    for idx in range(len(_data)):\n",
    "#         for i in range(3):\n",
    "#             for j in range(4):\n",
    "#                 for h in hz:\n",
    "#                     key = '201'+str(7+i)+'q'+str(j+1)+h.lower()\n",
    "        iszero = True\n",
    "        if len(_data.loc[idx][:][_data.loc[idx][:] !=0 ]) > 0:\n",
    "            iszero = False\n",
    "        if iszero:      \n",
    "            l += 1\n",
    "    return l\n",
    "print(len_zero(_x_test))\n",
    "# print(_x_test.iloc[0:1, [1,2,3]])\n",
    "# _x_test = torch.tensor(_x_test.iloc[:,1:].values, dtype=torch.float32)\n",
    "# _x_test=_x_test.view(-1, 12, 8)\n",
    "# print(_x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mmnist\u001b[0m/  \u001b[01;34mmodel\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nbuser\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = open(MODEL_NAME, mode=\"w\", encoding=\"utf-8\")\n",
    "fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nbuser/model\n"
     ]
    }
   ],
   "source": [
    "cd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "       19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52,\n",
       "       53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69,\n",
       "       70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86,\n",
       "       87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97,  0,  2,  3])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(np.arange(2, 98, 1), [0, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
