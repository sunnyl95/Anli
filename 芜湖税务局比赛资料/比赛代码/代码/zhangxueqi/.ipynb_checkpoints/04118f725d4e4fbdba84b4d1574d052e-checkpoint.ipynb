{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# 输入数据的参数\n",
    "_INPUT1='{\"name\":\"input1\",\"type\":0,\"uri\":\"tmp_71de07c874cb4994ba419ab1bc8a6d0d\"}'\n",
    "_INPUT2='{\"name\":\"input2\",\"type\":0,\"uri\":\"tmp_06ca9e3c90294a659a4898a6670d6a1d\"}'\n",
    "_INPUT3='{\"name\":\"input3\",\"type\":0,\"uri\":\"tmp_0eea4357f4e0437f8805eee02d1bcee2\"}'\n",
    "\n",
    "# 输出数据的参数\n",
    "_OUTPUT='[{\"name\":\"output1\",\"type\":0,\"uri\":\"tmp_c2f4d991560c4347b75b4caf03f407ad\"},{\"name\":\"output2\",\"type\":0,\"uri\":\"tmp_2afb265db33643b988574f03d27155b5\"},{\"name\":\"output3\",\"type\":0,\"uri\":\"tmp_0a0e7d87282d47debe9feab7362c2f0a\"},{\"name\":\"output4\",\"type\":0,\"uri\":\"tmp_da62af6ca4464ef8beae640544d08600\"}]'\n",
    "\n",
    "# 自定义参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "# 引入使用库\n",
    "import pandas as pd\n",
    "# from ustciscrLab import get_score\n",
    "import re\n",
    "import wfio\n",
    "import random\n",
    "\n",
    "import jieba\n",
    "import io\n",
    "from gensim.models import word2vec\n",
    "#import numpy as numpy\n",
    "from datetime import datetime \n",
    "from datetime import timedelta\n",
    "\n",
    "# import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "from ustciscrBDL_B import get_score\n",
    "import wfio\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.7/site-packages (3.8.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.9.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.16.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (1.9.190)\n",
      "Requirement already satisfied: boto>=2.32 in /opt/conda/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.6.16)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.2.1)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.190 in /opt/conda/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.12.190)\n",
      "Requirement already satisfied: docutils>=0.10 in /opt/conda/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.190->boto3->smart-open>=1.8.1->gensim) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.190->boto3->smart-open>=1.8.1->gensim) (2.8.0)\n"
     ]
    }
   ],
   "source": [
    "# # # # # 使用第三方库使用 !pip install 进行安装\n",
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取测试集数据\n",
    "def get_test_data():\n",
    "    test_data = wfio.read_dataframe(_INPUT3)\n",
    "    #删除第一行中文\n",
    "    #test_data = test_data.drop(0,axis=0,inplace=False)\n",
    "    return test_data\n",
    "#读取训练集数据\n",
    "def get_train_data():\n",
    "    train_data = wfio.read_dataframe(_INPUT1)\n",
    "    #删除第一行中文\n",
    "    #train_data = train_data.drop(0,axis=0,inplace=False)\n",
    "    return train_data\n",
    "#读取数据集\n",
    "ah = get_train_data()\n",
    "train_data = get_train_data()\n",
    "test_data = get_test_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(i for i in jieba.cut(\"广告业\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.736 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:191: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vecData's shape = (10000, 2)\n",
      "vecTest's shape = (6000, 2)\n"
     ]
    }
   ],
   "source": [
    "common_used_numerals_tmp = {'零': 0, '一': 1, '二': 2, '两': 2, '三': 3, '四': 4, '五': 5, '六': 6, '七': 7, '八': 8, '九': 9,\n",
    "                            '十': 10, '百': 100, '千': 1000, '万': 10000, '亿': 100000000}\n",
    "common_used_numerals = {}\n",
    "for key in common_used_numerals_tmp:\n",
    "    common_used_numerals[key] = common_used_numerals_tmp[key]\n",
    "\n",
    "\n",
    "\n",
    "def chinese2digits(uchars_chinese):\n",
    "    total = 0\n",
    "    r = 1  # 表示单位：个十百千...\n",
    "    for i in range(len(uchars_chinese) - 1, -1, -1):\n",
    "        val = common_used_numerals.get(uchars_chinese[i])\n",
    "        if val >= 10 and i == 0:  # 应对 十三 十四 十*之类\n",
    "            if val > r:\n",
    "                r = val\n",
    "                total = total + val\n",
    "            else:\n",
    "                r = r * val\n",
    "                # total =total + r * x\n",
    "        elif val >= 10:\n",
    "            if val > r:\n",
    "                r = val\n",
    "            else:\n",
    "                r = r * val\n",
    "        else:\n",
    "            total = total + r * val\n",
    "    return total\n",
    "\n",
    "\n",
    "num_str_start_symbol = ['一', '二', '两', '三', '四', '五', '六', '七', '八', '九',\n",
    "                        '十']\n",
    "more_num_str_symbol = ['零', '一', '二', '两', '三', '四', '五', '六', '七', '八', '九', '十', '百', '千', '万', '亿']\n",
    "\n",
    "def changeChineseNumToArab(oriStr):\n",
    "    lenStr = len(oriStr);\n",
    "    aProStr = ''\n",
    "    if lenStr == 0:\n",
    "        return aProStr;\n",
    "\n",
    "    hasNumStart = False;\n",
    "    numberStr = ''\n",
    "    for idx in range(lenStr):\n",
    "        if oriStr[idx] in num_str_start_symbol:\n",
    "            if not hasNumStart:\n",
    "                hasNumStart = True;\n",
    "\n",
    "            numberStr += oriStr[idx]\n",
    "        else:\n",
    "            if hasNumStart:\n",
    "                if oriStr[idx] in more_num_str_symbol:\n",
    "                    numberStr += oriStr[idx]\n",
    "                    continue\n",
    "                else:\n",
    "                    numResult = str(chinese2digits(numberStr))\n",
    "                    numberStr = ''\n",
    "                    hasNumStart = False;\n",
    "                    aProStr += numResult\n",
    "\n",
    "            aProStr += oriStr[idx]\n",
    "            pass\n",
    "\n",
    "    if len(numberStr) > 0:\n",
    "        resultNum = chinese2digits(numberStr)\n",
    "        aProStr += str(resultNum)\n",
    "\n",
    "    return aProStr\n",
    "\n",
    "# def removeSpace(s):\n",
    "#     s = s.replace(\" \", \"\")\n",
    "\n",
    "def strWhole2Half(ustring):\n",
    "    \"\"\"全角转半角\"\"\"\n",
    "    rstring = \"\"\n",
    "    for uchar in ustring:\n",
    "        inside_code=ord(uchar)\n",
    "        if inside_code == 12288:                              #全角空格直接转换            \n",
    "            inside_code = 32 \n",
    "        elif (inside_code >= 65281 and inside_code <= 65374): #全角字符（除空格）根据关系转化\n",
    "            inside_code -= 65248\n",
    "\n",
    "        rstring += chr(inside_code)\n",
    "    return rstring\n",
    "\n",
    "def change_split_and_brackets(s):\n",
    "    s = re.sub(r'(\\d+)\\s*[—]\\s*(?=\\d+)',r'\\1#',s)\n",
    "    s = re.sub(r'\\(.*\\)',r'',s)\n",
    "    s = re.sub(r'安徽省',r'',s)\n",
    "    s = re.sub(r'芜湖市',r'',s)\n",
    "    s = s.replace(\" \", \"\")\n",
    "    s = strWhole2Half(s)\n",
    "    return s\n",
    "\n",
    "def change_train_data(train_data):\n",
    "    #读取训练集的'生产经营地址'字段 （雪琦的丑丑的代码）\n",
    "    prob = {\n",
    "        \"bsrxm\": train_data['bsrxm'],\n",
    "        \"hy\": train_data['hy'],\n",
    "        \"hydl\": train_data['hydl'],\n",
    "        \"hyml\": train_data['hyml'],\n",
    "        \"scjydz\": train_data['scjydz'],\n",
    "        \"hyzl\": train_data['hyzl'],\n",
    "#         \"jyfw\": train_data['jyfw'],\n",
    "        \"nsrmc\": train_data['nsrmc'],\n",
    "        \"xzjd\": train_data['xzjd'],\n",
    "        \"zcdz\": train_data['zcdz']\n",
    "    }\n",
    "   # prob_list = []\n",
    "   # train_data = pd.DataFrame(prob)\n",
    "    rows = train_data.shape[0]\n",
    "   # print (rows)\n",
    "   # print(prob[\"scjydz\"])\n",
    "    word = [([0]) for i in range(rows)]                   # 列数\n",
    "    for i in range(0,rows):\n",
    "#         a = prob[\"scjydz\"][i]\n",
    "#         a = change_split_and_brackets(a)\n",
    "#         a = changeChineseNumToArab(a)\n",
    "#         b = prob[\"zcdz\"][i]\n",
    "#         b = change_split_and_brackets(b)\n",
    "#         b = changeChineseNumToArab(b)\n",
    "#         c = prob[\"nsrmc\"][i]\n",
    "#         c = change_split_and_brackets(c)\n",
    "#         c = changeChineseNumToArab(c)\n",
    "#         d = prob[\"xzjd\"][i]\n",
    "#         d = change_split_and_brackets(d)\n",
    "#         d = changeChineseNumToArab(d)\n",
    "        word[i][0]=prob[\"hy\"][i]                            # 行业\n",
    "#         word[i][1]=prob[\"hydl\"][i]                          # 行业DL\n",
    "#         word[i][2]=prob[\"hyml\"][i]                          # 行业ML\n",
    "#         word[i][3]=a                                        # 地址\n",
    "#         word[i][4]=prob[\"hyzl\"][i]                          # 行业ZL\n",
    "# #         word[i][5]=prob[\"jyfw\"][i]\n",
    "# # ================\n",
    "#         word[i][5]=c                                        # 公司名称\n",
    "#         word[i][6]=d                                        # 服务中心？\n",
    "# #         ==============\n",
    "#         word[i][7]=b                                        # 地址\n",
    "#         word[i][1] = prob[\"bsrxm\"][i]                       # 财务人员姓名\n",
    "#     print(word[0])\n",
    "  #  train_data['生产经营地址'] = prob_list\n",
    "    return word\n",
    "# word = change_train_data()\n",
    "\n",
    "strData = change_train_data(train_data)\n",
    "\n",
    "#=====================================================重新训练的话就把它打开========================================================================\n",
    "with open(\"seg2Vec.txt\", 'w') as f:\n",
    "#     print(f.readlines())\n",
    "    pass\n",
    "#===================================================================================================\n",
    "sentenceList = [0] * len(strData)\n",
    "# segList = [0] * len(strData)\n",
    "# create text dictionary\n",
    "for i in range(len(strData)):\n",
    "    strr = ''\n",
    "    sentenceList[i] = strr.join(strData[i])\n",
    "    segList = jieba.cut(sentenceList[i])\n",
    "    with open(\"seg2Vec.txt\", 'a') as f:\n",
    "        f.write(' '.join(segList))\n",
    "# print(segList)\n",
    "\n",
    "def mapToEmbedding():\n",
    "    num_features = 2\n",
    "    min_word_count = 1\n",
    "    num_threads = 20\n",
    "    context_size = 10\n",
    "    downSample = 1e-3\n",
    "    sentencesIter = word2vec.Text8Corpus(\"seg2Vec.txt\")\n",
    "#     model = word2vec.Word2Vec(min_count=1)\n",
    "#     model.build_vocab(sentencesIter)\n",
    "#     model.train(sentencesIter, total_examples=model.corpus_count,epochs=model.iter)\n",
    "\n",
    "    model = word2vec.Word2Vec(sentencesIter, workers=num_threads, size=num_features, min_count=min_word_count, window=context_size, sg=1, sample=downSample)\n",
    "    model.init_sims(replace=True)\n",
    "    return model\n",
    "embedModel = mapToEmbedding()\n",
    "\n",
    "def text2Vec(textFeatures):\n",
    "    vecFeatures = [[np.zeros((2,))]*9 for i in range(len(textFeatures))]\n",
    "    vecCopy = [[np.zeros((2,))]*9 for i in range(len(textFeatures))]\n",
    "    vec = [[] for i in range(len(textFeatures))]\n",
    "    for i in range(len(textFeatures)):\n",
    "        for j in range(len(textFeatures[i])):\n",
    "            try:\n",
    "                setText = list(jieba.cut(textFeatures[i][j]))\n",
    "            except:\n",
    "                continue\n",
    "            setText = list(jieba.cut(textFeatures[i][j]))\n",
    "            for k in setText:\n",
    "                try:\n",
    "                    vecFeatures[i][j] += embedModel[k]\n",
    "                except:\n",
    "                    pass\n",
    "            vecCopy[i][j] = vecFeatures[i][j]\n",
    "            vec[i] = vec[i] + vecFeatures[i][j].tolist()\n",
    "    return vec\n",
    "strData = change_train_data(train_data)\n",
    "strTestData = change_train_data(test_data)\n",
    "vecData = np.array(text2Vec(strData))\n",
    "vecTest = np.array(text2Vec(strTestData))\n",
    "print(\"vecData's shape = \" + str(vecData.shape))\n",
    "print(\"vecTest's shape = \" + str(vecTest.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10000)\n"
     ]
    }
   ],
   "source": [
    "# 杨老哥的地址匹配\n",
    "def handle_data(data_original):#dataframe\n",
    "    data_original=data_original.fillna(0)\n",
    "    data_original.replace('\"\"\"\"\"\"\"\"\"\"\"\"\"\"', 0, inplace=True)\n",
    "    data_original.replace('', 0, inplace=True)\n",
    "    result = []\n",
    "    for row in data_original.itertuples():\n",
    "        a= getattr(row, 'scjydz')\n",
    "        b= getattr(row, 'zcdz') # 输出每一行\n",
    "        if a != 0 and b==0:\n",
    "            result.append(1)\n",
    "            continue\n",
    "        if a == 0 and b==0:\n",
    "            result.append(1)\n",
    "            continue\n",
    "        if is_endswith1(a,b):\n",
    "            result.append(1)\n",
    "            continue\n",
    "        a = change_split_and_brackets1(a)\n",
    "        b = change_split_and_brackets1(b)\n",
    "        if is_self_construct1(b):\n",
    "            result.append(1)\n",
    "            continue\n",
    "        a = changeChineseNumToArab1(a)\n",
    "        b = changeChineseNumToArab1(b)\n",
    "        if hasNumbers1(a) and hasNumbers1(b):\n",
    "            a,b =cut_two_sentence1(a,b)\n",
    "            if is_number_alright1(a,b):\n",
    "                result.append(1)\n",
    "            else:\n",
    "                result.append(0)\n",
    "            continue\n",
    "        else:\n",
    "            a = del_numbers1(a)\n",
    "            b = del_numbers1(b)\n",
    "            a,b =cut_two_sentence1(a,b)\n",
    "            if is_subset1(a,b):\n",
    "                result.append(1)\n",
    "            else:\n",
    "               result.append(0)\n",
    "#     print(result)\n",
    "    return result\n",
    "  \n",
    "\n",
    "def change_split_and_brackets1(s):\n",
    "    s = re.sub(r'(\\d+)\\s*[—]\\s*(?=\\d+)',r'\\1#',s)\n",
    "    s = re.sub(r'\\(.*\\)',r'',s)\n",
    "    s = strWhole2Half(s)\n",
    "    return s\n",
    "\n",
    "def is_endswith1(a,b):\n",
    "    return a.endswith(b) or b.endswith(a)\n",
    "\n",
    "def cut_two_sentence1(a,b):\n",
    "    seg_list = jieba.cut(a)\n",
    "    a = [item for item in seg_list]\n",
    "    seg_list = jieba.cut(b)\n",
    "    b = [item for item in seg_list]\n",
    "    return a,b\n",
    "\n",
    "def is_subset1(a,b):\n",
    "    a = set(a)\n",
    "    b = set(b)\n",
    "    return a.issubset(b) or b.issubset(a)\n",
    "\n",
    "def is_self_construct1(b):\n",
    "    return '自' in b and '建' in b\n",
    "\n",
    "def del_numbers1(s):\n",
    "    s = re.sub('(\\d+|#|-|楼|单元|室|号|幢|栋)', '', s)\n",
    "    return s\n",
    "\n",
    "def hasNumbers1(inputString):\n",
    "    return bool(re.search(r'\\d', inputString))\n",
    "\n",
    "def is_number_alright1(a,b):#传入的是分词后的两个带数字的list数据 判断两个字符串的数字是否一致\n",
    "    num_a = set()\n",
    "    num_b = set()\n",
    "    for item in a:\n",
    "       if item.isdigit():\n",
    "           num_a.add(item)\n",
    "\n",
    "    for item in b:\n",
    "        if item.isdigit():\n",
    "            num_b.add(item)\n",
    "    if len(num_a)==0 or len(num_b)==0:\n",
    "        return False\n",
    "    return (num_a.issubset(num_b) or num_b.issubset(num_a))\n",
    "\n",
    "def changeChineseNumToArab1(oriStr):\n",
    "    lenStr = len(oriStr);\n",
    "    aProStr = ''\n",
    "    if lenStr == 0:\n",
    "        return aProStr;\n",
    "\n",
    "    hasNumStart = False;\n",
    "    numberStr = ''\n",
    "    for idx in range(lenStr):\n",
    "        if oriStr[idx] in num_str_start_symbol:\n",
    "            if not hasNumStart:\n",
    "                hasNumStart = True;\n",
    "\n",
    "            numberStr += oriStr[idx]\n",
    "        else:\n",
    "            if hasNumStart:\n",
    "                if oriStr[idx] in more_num_str_symbol:\n",
    "                    numberStr += oriStr[idx]\n",
    "                    continue\n",
    "                else:\n",
    "                    numResult = str(chinese2digits(numberStr))\n",
    "                    numberStr = ''\n",
    "                    hasNumStart = False;\n",
    "                    aProStr += numResult\n",
    "\n",
    "            aProStr += oriStr[idx]\n",
    "            pass\n",
    "\n",
    "    if len(numberStr) > 0:\n",
    "        resultNum = chinese2digits(numberStr)\n",
    "        aProStr += str(resultNum)\n",
    "\n",
    "    return aProStr\n",
    "\n",
    "addressMarkTrain = np.array([handle_data(train_data)])\n",
    "addressMarkTest = np.array([handle_data(test_data)])\n",
    "\n",
    "print(addressMarkTrain.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一堆杂七杂八的组件 \n",
    "\n",
    "def sigmoid(Z):\n",
    "   \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "   \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters     \n",
    "\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "   \n",
    "    Z = W.dot(A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    " \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "   \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "   \n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p\n",
    "\n",
    "def cleanData(acfg,label):\n",
    "    '''\n",
    "    把数据正负样例变到差不多\n",
    "    '''\n",
    "    perm = np.random.permutation(label.shape[1])\n",
    "#     print(perm.shape)\n",
    "    delIndex = []    # store those indexes to delete.\n",
    "    count = 0\n",
    "    negcount = 0\n",
    "#     sizeData = label.shape[1]\n",
    "    for i in perm:\n",
    "        if label[0,i]:\n",
    "            count = count + 1\n",
    "        else:\n",
    "            if(count < negcount):\n",
    "#             if(count < negcount):\n",
    "                delIndex.append(i)\n",
    "            else:\n",
    "                negcount = negcount + 1\n",
    "\n",
    "    cleanAcfg = np.delete(acfg, delIndex,axis=1)\n",
    "    cleanLabel = np.delete(label, delIndex,axis =1)\n",
    "    return cleanAcfg, cleanLabel\n",
    "\n",
    "def addDataBalance(acfg,label):\n",
    "    '''\n",
    "    把训练数据正负样例变到差不多, 不过是添加少样例的方式。\n",
    "    '''\n",
    "    positive_times = int((len(label[0]) - np.sum(label[0,:]))/np.sum(label[0,:]))   # 增长的倍数\n",
    "    tmpData = np.zeros((len(acfg[:,0]), 1))\n",
    "    tmplb = np.zeros((len(label[:,0]), 1))\n",
    "\n",
    "    perNum = len(label[0])\n",
    "    for i in range(perNum):\n",
    "        if label[0,i]:\n",
    "            # repeat target data\n",
    "            for times in range(positive_times):\n",
    "                tmpData = np.hstack([tmpData, np.array([acfg[:, i].tolist()]).T])\n",
    "                tmplb = np.hstack([tmplb, np.array([label[:, i].tolist()]).T])\n",
    "        else:\n",
    "            tmpData = np.hstack([tmpData, np.array([acfg[:, i].tolist()]).T])\n",
    "            tmplb = np.hstack([tmplb, np.array([label[:, i].tolist()]).T])\n",
    "    tmpData = np.delete(tmpData, 0, axis=1)\n",
    "    tmplb = np.delete(tmplb, 0, axis=1)\n",
    "    return tmpData, tmplb\n",
    "\n",
    "   \n",
    "\n",
    "def partitionData(data, label, perm):\n",
    "    ''' 洗洗牌, 你这数据太有规律了\n",
    "    # data: 训练指标; label: 标签\n",
    "    # perm : random后的序列\n",
    "    '''\n",
    "    C = data.shape[1]\n",
    "#     startea = 0.0\n",
    "    ret = []\n",
    "    cur_data = []\n",
    "    cur_label = []\n",
    "    count = 0\n",
    "#         endea = startea + part*C\n",
    "    for dataParts in range(C):\n",
    "        cur_data.append([])\n",
    "        cur_label.append([])\n",
    "        cur_data[count] = list(data[:,perm[dataParts]])\n",
    "        cur_label[count] = list(label[:,perm[dataParts]])\n",
    "        count = count+1\n",
    "    ret.append(cur_data)\n",
    "    ret.append(cur_label)\n",
    "#         startea = endea\n",
    "    return ret\n",
    "\n",
    "def normalization(data):\n",
    "    divide = np.max(data, axis=1) - np.min(data, axis=1)\n",
    "    normalData = np.zeros((data.shape))\n",
    "    for i in range(len(data[0])):\n",
    "        tmpX = data[:,i] - np.min(data, axis=1)\n",
    "        normalData[:,i] = tmpX/divide\n",
    "    return normalData\n",
    "#     return np.array([(data[:,i] - np.min(data, axis=1))/(np.max(data, axis=1) - np.min(data, axis=1)) for i in range(len(data[0]))])\n",
    "\n",
    "def create_feature(data, test):\n",
    "    rows1 = data.shape[0]   \n",
    "    rows2 = test.shape[0]\n",
    "    data_f = np.zeros(shape=(rows1,48))\n",
    "    test_f = np.zeros(shape=(rows2,48))\n",
    "    data_f =  np.array(data_f).astype(np.float)\n",
    "    test_f =  np.array(test_f).astype(np.float)\n",
    "    for i in range(0,rows1):\n",
    "        for j in range(0,12):\n",
    "            if data[i][j*8+2] == 0.03:\n",
    "               # print(data[i][j*8+7])\n",
    "               # print(data[i][j*8+5])\n",
    "                data_f[i][j] = float(data[i][j*8+7]-data[i][j*8+5])\n",
    "                data_f[i][12+j] = data[i][j*8]-data[i][j*8+4]\n",
    "              #  print(data_f[i][12+j]) \n",
    "            else:\n",
    "              #  print(data[i][j*8+7])\n",
    "            #  print(data[i][j*8+5])\n",
    "                data_f[i][j] = data[i][j*8+7]-data[i][j*8+5]+data[i][j*8+1]*data[i][j*8+2]\n",
    "                data_f[i][12+j] = data[i][j*8]-data[i][j*8+4]\n",
    "               # print(data_f[i][j])\n",
    "            if(data[i][j*8+3] == 0):\n",
    "                data_f[i][24+j] = 0\n",
    "            else:\n",
    "                data_f[i][24+j] = data[i][j*8+3]/data[i][j*8+4]\n",
    "            #print(data_f[i][24+j])\n",
    "            data_f[i][36+j] = data[i][j*8+3]-data[i][j*8+1]\n",
    "            #print(data_f[i][36+j])\n",
    "    for i in range(0,rows2):\n",
    "        for j in range(0,12):       \n",
    "            if test[i][j*8+2] == 0.03:\n",
    "                test_f[i][j] = test[i][j*8+7]-test[i][j*8+5]\n",
    "                test_f[i][12+j] = test[i][j*8]-test[i][j*8+4]\n",
    "                #print(\"1\")\n",
    "            else:\n",
    "                test_f[i][j] = test[i][j*8+7]-test[i][j*8+5]+test[i][j*8+1]*test[i][j*8+2]\n",
    "                test_f[i][12+j] = test[i][j*8]-test[i][j*8+4]\n",
    "            if(test[i][j*8+3] == 0):\n",
    "                test_f[i][24+j] = 0\n",
    "            else:\n",
    "                test_f[i][24+j] = test[i][j*8+3]/test[i][j*8+4]\n",
    "            test_f[i][36+j] = test[i][j*8+3]-test[i][j*8+1]\n",
    "    return data_f, test_f\n",
    "\n",
    "\n",
    "\n",
    "def process(data_original):\n",
    "    #获取注册日期的时间\n",
    "    temp = list(data_original[\"djrq\"])\n",
    "    dates = []\n",
    "    for item in temp:\n",
    "        dates.append(int(round(datetime.strptime(item, '%Y-%m-%d').timestamp())))\n",
    "#     print(max(dates))\n",
    "#     print(min(dates))\n",
    "    #获取第一次交税的时间\n",
    "    time = datetime.strptime(\"2017-01-01\", '%Y-%m-%d')\n",
    "    result = np.zeros((len(dates),1))\n",
    "    result [result == 0] = 1\n",
    "    print(result)\n",
    "    data_original =np.array(data_original) \n",
    "    data_original =np.array(data_original[:,:96])\n",
    "    data_original = np.array(data_original).astype(np.float)\n",
    "    for i in range(0,data_original.shape[1],8):\n",
    "        time_int = int(round(time.timestamp()))\n",
    "        temp1 = data_original[:,i]\n",
    "#         print(temp1)\n",
    "        temp1[temp1 != 0] = time_int\n",
    "#         print(temp1)\n",
    "        for j in range(len(result)):\n",
    "            if result[j][0] == 1 and temp1[j]!= 0:\n",
    "                result[j][0] = temp1[j]\n",
    "        time = time + timedelta(days=90)\n",
    "    result [result == 1] = 0\n",
    "    result = list(result.squeeze())#result即为第一次缴费时间\n",
    "    sub = []\n",
    "#     print(result)\n",
    "    for i in range(len(result)):\n",
    "        dates[i] = dates[i]/100\n",
    "        if result[i] != 0:\n",
    "            result[i] = result[i] /100\n",
    "            sub.append(result[i]-dates[i])\n",
    "        else:\n",
    "            sub.append(0)\n",
    "    dates = np.array([dates])\n",
    "    result = np.array([result])\n",
    "    sub = np.array([sub])\n",
    "    return dates,result,sub #注册日期，第一次缴费日期，差值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据集\n",
    "ah = get_train_data()\n",
    "train_data = get_train_data()\n",
    "test_data = get_test_data()\n",
    "train_data = train_data.fillna(0)\n",
    "train_data.replace('\"\"\"\"\"\"\"\"\"\"\"\"\"\"', 0, inplace=True)\n",
    "train_data.replace('', 0, inplace=True)\n",
    "test_data.replace('\"\"\"\"\"\"\"\"\"\"\"\"\"\"', 0, inplace=True)\n",
    "test_data.replace('', 0, inplace=True)\n",
    "label = np.array([train_data[\"yc\"].values.tolist()])\n",
    "data = np.array(train_data[['2017q1fphdsl', '2017q1jxje', '2017q1jxsl', '2017q1kpje', '2017q1kps', '2017q1kpse', '2017q1kpsl', '2017q1rkse', '2017q2fphdsl', '2017q2jxje', '2017q2jxsl', '2017q2kpje', '2017q2kps', '2017q2kpse', '2017q2kpsl', '2017q2rkse', '2017q3fphdsl', '2017q3jxje', '2017q3jxsl', '2017q3kpje', '2017q3kps', '2017q3kpse', '2017q3kpsl', '2017q3rkse', '2017q4fphdsl', '2017q4jxje', '2017q4jxsl', '2017q4kpje', '2017q4kps', '2017q4kpse', '2017q4kpsl', '2017q4rkse', '2018q1fphdsl', '2018q1jxje', '2018q1jxsl', '2018q1kpje', '2018q1kps', '2018q1kpse', '2018q1kpsl', '2018q1rkse', '2018q2fphdsl', '2018q2jxje', '2018q2jxsl', '2018q2kpje', '2018q2kps', '2018q2kpse', '2018q2kpsl', '2018q2rkse', '2018q3fphdsl', '2018q3jxje', '2018q3jxsl', '2018q3kpje', '2018q3kps', '2018q3kpse', '2018q3kpsl', '2018q3rkse', '2018q4fphdsl', '2018q4jxje', '2018q4jxsl', '2018q4kpje', '2018q4kps', '2018q4kpse', '2018q4kpsl', '2018q4rkse', '2019q1fphdsl', '2019q1jxje', '2019q1jxsl', '2019q1kpje', '2019q1kps', '2019q1kpse', '2019q1kpsl', '2019q1rkse', '2019q2fphdsl', '2019q2jxje', '2019q2jxsl', '2019q2kpje', '2019q2kps', '2019q2kpse', '2019q2kpsl', '2019q2rkse', '2019q3fphdsl', '2019q3jxje', '2019q3jxsl', '2019q3kpje', '2019q3kps', '2019q3kpse', '2019q3kpsl', '2019q3rkse', '2019q4fphdsl', '2019q4jxje', '2019q4jxsl', '2019q4kpje', '2019q4kps', '2019q4kpse', '2019q4kpsl', '2019q4rkse']].values.tolist())\n",
    "testX = np.array(test_data[['2017q1fphdsl', '2017q1jxje', '2017q1jxsl', '2017q1kpje', '2017q1kps', '2017q1kpse', '2017q1kpsl', '2017q1rkse', '2017q2fphdsl', '2017q2jxje', '2017q2jxsl', '2017q2kpje', '2017q2kps', '2017q2kpse', '2017q2kpsl', '2017q2rkse', '2017q3fphdsl', '2017q3jxje', '2017q3jxsl', '2017q3kpje', '2017q3kps', '2017q3kpse', '2017q3kpsl', '2017q3rkse', '2017q4fphdsl', '2017q4jxje', '2017q4jxsl', '2017q4kpje', '2017q4kps', '2017q4kpse', '2017q4kpsl', '2017q4rkse', '2018q1fphdsl', '2018q1jxje', '2018q1jxsl', '2018q1kpje', '2018q1kps', '2018q1kpse', '2018q1kpsl', '2018q1rkse', '2018q2fphdsl', '2018q2jxje', '2018q2jxsl', '2018q2kpje', '2018q2kps', '2018q2kpse', '2018q2kpsl', '2018q2rkse', '2018q3fphdsl', '2018q3jxje', '2018q3jxsl', '2018q3kpje', '2018q3kps', '2018q3kpse', '2018q3kpsl', '2018q3rkse', '2018q4fphdsl', '2018q4jxje', '2018q4jxsl', '2018q4kpje', '2018q4kps', '2018q4kpse', '2018q4kpsl', '2018q4rkse', '2019q1fphdsl', '2019q1jxje', '2019q1jxsl', '2019q1kpje', '2019q1kps', '2019q1kpse', '2019q1kpsl', '2019q1rkse', '2019q2fphdsl', '2019q2jxje', '2019q2jxsl', '2019q2kpje', '2019q2kps', '2019q2kpse', '2019q2kpsl', '2019q2rkse', '2019q3fphdsl', '2019q3jxje', '2019q3jxsl', '2019q3kpje', '2019q3kps', '2019q3kpse', '2019q3kpsl', '2019q3rkse', '2019q4fphdsl', '2019q4jxje', '2019q4jxsl', '2019q4kpje', '2019q4kps', '2019q4kpse', '2019q4kpsl', '2019q4rkse']].values.tolist())\n",
    "data =  np.array(data).astype(np.float)\n",
    "label =  np.array(label).astype(np.float)\n",
    "testX =  np.array(testX).astype(np.float)\n",
    "# train_data = np.array(train_data).astype(np.float)\n",
    "# test_data = np.array(test_data).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "XQ: (10000, 48)(6000, 48)\n",
      "YYG_train: (1, 10000)(1, 10000)(1, 10000)\n",
      "YYG_test: (1, 6000)(1, 6000)(1, 6000)\n"
     ]
    }
   ],
   "source": [
    "data_f, test_f = create_feature(data, testX)\n",
    "train_dates, trainRR, train_sub = process(train_data)\n",
    "test_dates, testRR, test_sub = process(test_data)\n",
    "print(\"XQ: \" + str(data_f.shape) + str(test_f.shape))\n",
    "print(\"YYG_train: \" + str(train_dates.shape) + str(trainRR.shape) + str(train_sub.shape))\n",
    "print(\"YYG_test: \" + str(test_dates.shape) + str(testRR.shape) + str(test_sub.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin_shape: (96, 10000)(1, 10000)(96, 6000)\n"
     ]
    }
   ],
   "source": [
    "data = data.T\n",
    "testX = testX.T\n",
    "trainX = data\n",
    "trainY = label\n",
    "print(\"Origin_shape: \" + str(trainX.shape) + str(trainY.shape) + str(testX.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (145, 10000)\n",
      "test (145, 6000)\n"
     ]
    }
   ],
   "source": [
    "# trainX = np.vstack((data_f.T, train_dates, trainRR, train_sub, trainX, vecData.T, addressMarkTrain))\n",
    "# # testX = np.vstack((test_f.T, test_dates, testRR, test_sub, testX, vecTest.T, addressMarkTest))\n",
    "\n",
    "# ===================\n",
    "#  就是在上面的七个数据里面排列组合， 看什么样拼起来最好。 比如下面就是拼五个的样子。 注意trainX和testX应该上下一样。\n",
    "# ====================\n",
    "\n",
    "trainX = np.vstack((data_f.T, train_sub, trainX))\n",
    "testX = np.vstack((test_f.T, test_sub, testX))。\n",
    "\n",
    "//\n",
    "\n",
    "# print(train_dates)\n",
    "# print(trainRR)\n",
    "# print(train_sub)\n",
    "print(\"train \" + str(trainX.shape))\n",
    "print(\"test \" + str(testX.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Cleanning data, trainX's shape = (145, 10000)\n",
      "Before Cleanning data, trainY's shape = (1, 10000)\n",
      "After Cleanning data, positive = 1000.0\n",
      "After Cleanning data, trainX's shape = (145, 2001)\n",
      "After Cleanning data, trainY's shape = (1, 2001)\n"
     ]
    }
   ],
   "source": [
    "# # #========删除负例=====================================\n",
    "perm = np.random.permutation(trainY.shape[1])\n",
    "trainX, trainY = partitionData(trainX, trainY, perm)\n",
    "trainX =  np.array(trainX).astype(np.float)\n",
    "trainY =  np.array(trainY).astype(np.float)\n",
    "trainX = trainX.T\n",
    "trainY = trainY.T\n",
    "# #===============\n",
    "\n",
    "\n",
    "print(\"Before Cleanning data, trainX's shape = \" + str(trainX.shape))\n",
    "print(\"Before Cleanning data, trainY's shape = \" + str(trainY.shape))\n",
    "trainX, trainY = cleanData(trainX, trainY)\n",
    "print(\"After Cleanning data, positive = \" + str(np.sum(trainY)))\n",
    "print(\"After Cleanning data, trainX's shape = \" + str(trainX.shape))\n",
    "print(\"After Cleanning data, trainY's shape = \" + str(trainY.shape))\n",
    "\n",
    "# #==========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======增加正例===========================\n",
    "\n",
    "# print(\"Before adding data, trainX's shape = \" + str(trainX.shape))\n",
    "# print(\"Before adding data, trainY's shape = \" + str(trainY.shape))\n",
    "# trainX, trainY = addDataBalance(trainX, trainY)\n",
    "# print(\"After adding data, positive = \" + str(np.sum(trainY)))\n",
    "# print(\"After adding data, trainX's shape = \" + str(trainX.shape))\n",
    "# print(\"After adding data, trainY's shape = \" + str(trainY.shape))\n",
    "\n",
    "#===========================================\n",
    "\n",
    "# #============================================\n",
    "# perm = np.random.permutation(trainY.shape[1])\n",
    "# trainX, trainY = partitionData(trainX, trainY, perm)\n",
    "# trainX =  np.array(trainX).astype(np.float)\n",
    "# trainY =  np.array(trainY).astype(np.float)\n",
    "# trainX = trainX.T\n",
    "# trainY = trainY.T\n",
    "#===============\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145, 128, 64, 32, 8, 1)\n",
      "(145, 2001)\n",
      "(1, 2001)\n",
      "(145, 6000)\n",
      "[1. 0. 1. 1. 0. 0. 1. 1. 0. 1.]\n",
      "[[4.53105117e-10 4.53105117e-10 4.53105117e-10 ... 4.53105117e-10\n",
      "  4.53105117e-10 4.53105117e-10]\n",
      " [4.72149181e-10 1.77341802e-02 4.72149181e-10 ... 4.72149181e-10\n",
      "  4.72149181e-10 4.72149181e-10]\n",
      " [4.96573025e-10 4.96573025e-10 3.19947778e-01 ... 4.96573025e-10\n",
      "  4.96573025e-10 4.07884545e-01]]\n"
     ]
    }
   ],
   "source": [
    "n_x = trainX.shape[0]\n",
    "n_h1 = 128\n",
    "n_h2 = 64\n",
    "n_h3 = 32\n",
    "n_h4 = 8\n",
    "n_y = trainY.shape[0]\n",
    "layers_dims = (n_x, n_h1, n_h2, n_h3, n_h4, n_y)\n",
    "# print(trainX.shape[1])\n",
    "# print(trainY.shape[1])\n",
    "# 把trainX归一化一下\n",
    "trainX = normalization(trainX)\n",
    "testX = normalization(testX)\n",
    "print(layers_dims)\n",
    "# print(trainX.T)\n",
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "print(testX.shape)\n",
    "print(trainY[0][0:10])\n",
    "print(trainX[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: n_layer_model\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate=0.0045, num_iterations=24900, print_cost=False): #lr was 0.009\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "\n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.690966\n",
      "Cost after iteration 100: 0.688279\n",
      "Cost after iteration 200: 0.686692\n",
      "Cost after iteration 300: 0.685005\n",
      "Cost after iteration 400: 0.682900\n",
      "Cost after iteration 500: 0.680568\n",
      "Cost after iteration 600: 0.677943\n",
      "Cost after iteration 700: 0.675017\n",
      "Cost after iteration 800: 0.671353\n",
      "Cost after iteration 900: 0.666953\n",
      "Cost after iteration 1000: 0.661957\n",
      "Cost after iteration 1100: 0.656268\n",
      "Cost after iteration 1200: 0.649784\n",
      "Cost after iteration 1300: 0.642415\n",
      "Cost after iteration 1400: 0.633955\n",
      "Cost after iteration 1500: 0.624190\n",
      "Cost after iteration 1600: 0.613048\n",
      "Cost after iteration 1700: 0.600513\n",
      "Cost after iteration 1800: 0.586496\n",
      "Cost after iteration 1900: 0.570918\n",
      "Cost after iteration 2000: 0.553765\n",
      "Cost after iteration 2100: 0.534971\n",
      "Cost after iteration 2200: 0.514460\n",
      "Cost after iteration 2300: 0.492119\n",
      "Cost after iteration 2400: 0.467640\n",
      "Cost after iteration 2500: 0.440665\n",
      "Cost after iteration 2600: 0.410913\n",
      "Cost after iteration 2700: 0.378260\n",
      "Cost after iteration 2800: 0.343583\n",
      "Cost after iteration 2900: 0.308377\n",
      "Cost after iteration 3000: 0.275064\n",
      "Cost after iteration 3100: 0.244330\n",
      "Cost after iteration 3200: 0.216928\n",
      "Cost after iteration 3300: 0.193131\n",
      "Cost after iteration 3400: 0.172903\n",
      "Cost after iteration 3500: 0.155783\n",
      "Cost after iteration 3600: 0.141250\n",
      "Cost after iteration 3700: 0.128687\n",
      "Cost after iteration 3800: 0.118436\n",
      "Cost after iteration 3900: 0.110023\n",
      "Cost after iteration 4000: 0.102941\n",
      "Cost after iteration 4100: 0.096930\n",
      "Cost after iteration 4200: 0.091804\n",
      "Cost after iteration 4300: 0.087370\n",
      "Cost after iteration 4400: 0.083503\n",
      "Cost after iteration 4500: 0.080143\n",
      "Cost after iteration 4600: 0.077206\n",
      "Cost after iteration 4700: 0.074622\n",
      "Cost after iteration 4800: 0.072336\n",
      "Cost after iteration 4900: 0.070302\n",
      "Cost after iteration 5000: 0.068483\n",
      "Cost after iteration 5100: 0.066849\n",
      "Cost after iteration 5200: 0.065381\n",
      "Cost after iteration 5300: 0.064056\n",
      "Cost after iteration 5400: 0.062852\n",
      "Cost after iteration 5500: 0.061754\n",
      "Cost after iteration 5600: 0.060753\n",
      "Cost after iteration 5700: 0.059835\n",
      "Cost after iteration 5800: 0.058991\n",
      "Cost after iteration 5900: 0.058215\n",
      "Cost after iteration 6000: 0.057497\n",
      "Cost after iteration 6100: 0.056832\n",
      "Cost after iteration 6200: 0.056217\n",
      "Cost after iteration 6300: 0.055644\n",
      "Cost after iteration 6400: 0.055111\n",
      "Cost after iteration 6500: 0.054614\n",
      "Cost after iteration 6600: 0.054149\n",
      "Cost after iteration 6700: 0.053714\n",
      "Cost after iteration 6800: 0.053305\n",
      "Cost after iteration 6900: 0.052921\n",
      "Cost after iteration 7000: 0.052559\n",
      "Cost after iteration 7100: 0.052218\n",
      "Cost after iteration 7200: 0.051897\n",
      "Cost after iteration 7300: 0.051594\n",
      "Cost after iteration 7400: 0.051307\n",
      "Cost after iteration 7500: 0.051035\n",
      "Cost after iteration 7600: 0.050776\n",
      "Cost after iteration 7700: 0.050530\n",
      "Cost after iteration 7800: 0.050295\n",
      "Cost after iteration 7900: 0.050072\n",
      "Cost after iteration 8000: 0.049860\n",
      "Cost after iteration 8100: 0.049657\n",
      "Cost after iteration 8200: 0.049463\n",
      "Cost after iteration 8300: 0.049277\n",
      "Cost after iteration 8400: 0.049100\n",
      "Cost after iteration 8500: 0.048930\n",
      "Cost after iteration 8600: 0.048766\n",
      "Cost after iteration 8700: 0.048610\n",
      "Cost after iteration 8800: 0.048459\n",
      "Cost after iteration 8900: 0.048314\n",
      "Cost after iteration 9000: 0.048175\n",
      "Cost after iteration 9100: 0.048040\n",
      "Cost after iteration 9200: 0.047911\n",
      "Cost after iteration 9300: 0.047786\n",
      "Cost after iteration 9400: 0.047666\n",
      "Cost after iteration 9500: 0.047551\n",
      "Cost after iteration 9600: 0.047440\n",
      "Cost after iteration 9700: 0.047332\n",
      "Cost after iteration 9800: 0.047229\n",
      "Cost after iteration 9900: 0.047129\n",
      "Cost after iteration 10000: 0.047032\n",
      "Cost after iteration 10100: 0.046938\n",
      "Cost after iteration 10200: 0.046848\n",
      "Cost after iteration 10300: 0.046760\n",
      "Cost after iteration 10400: 0.046675\n",
      "Cost after iteration 10500: 0.046593\n",
      "Cost after iteration 10600: 0.046513\n",
      "Cost after iteration 10700: 0.046436\n",
      "Cost after iteration 10800: 0.046360\n",
      "Cost after iteration 10900: 0.046287\n",
      "Cost after iteration 11000: 0.046216\n",
      "Cost after iteration 11100: 0.046147\n",
      "Cost after iteration 11200: 0.046080\n",
      "Cost after iteration 11300: 0.046015\n",
      "Cost after iteration 11400: 0.045952\n",
      "Cost after iteration 11500: 0.045890\n",
      "Cost after iteration 11600: 0.045830\n",
      "Cost after iteration 11700: 0.045771\n",
      "Cost after iteration 11800: 0.045714\n",
      "Cost after iteration 11900: 0.045659\n",
      "Cost after iteration 12000: 0.045605\n",
      "Cost after iteration 12100: 0.045552\n",
      "Cost after iteration 12200: 0.045501\n",
      "Cost after iteration 12300: 0.045451\n",
      "Cost after iteration 12400: 0.045402\n",
      "Cost after iteration 12500: 0.045354\n",
      "Cost after iteration 12600: 0.045308\n",
      "Cost after iteration 12700: 0.045262\n",
      "Cost after iteration 12800: 0.045218\n",
      "Cost after iteration 12900: 0.045175\n",
      "Cost after iteration 13000: 0.045132\n",
      "Cost after iteration 13100: 0.045091\n",
      "Cost after iteration 13200: 0.045050\n",
      "Cost after iteration 13300: 0.045011\n",
      "Cost after iteration 13400: 0.044972\n",
      "Cost after iteration 13500: 0.044935\n",
      "Cost after iteration 13600: 0.044898\n",
      "Cost after iteration 13700: 0.044861\n",
      "Cost after iteration 13800: 0.044826\n",
      "Cost after iteration 13900: 0.044791\n",
      "Cost after iteration 14000: 0.044758\n",
      "Cost after iteration 14100: 0.044724\n",
      "Cost after iteration 14200: 0.044692\n",
      "Cost after iteration 14300: 0.044660\n",
      "Cost after iteration 14400: 0.044629\n",
      "Cost after iteration 14500: 0.044598\n",
      "Cost after iteration 14600: 0.044568\n",
      "Cost after iteration 14700: 0.044539\n",
      "Cost after iteration 14800: 0.044510\n",
      "Cost after iteration 14900: 0.044482\n",
      "Cost after iteration 15000: 0.044454\n",
      "Cost after iteration 15100: 0.044427\n",
      "Cost after iteration 15200: 0.044401\n",
      "Cost after iteration 15300: 0.044375\n",
      "Cost after iteration 15400: 0.044349\n",
      "Cost after iteration 15500: 0.044324\n",
      "Cost after iteration 15600: 0.044299\n",
      "Cost after iteration 15700: 0.044275\n",
      "Cost after iteration 15800: 0.044251\n",
      "Cost after iteration 15900: 0.044228\n",
      "Cost after iteration 16000: 0.044205\n",
      "Cost after iteration 16100: 0.044182\n",
      "Cost after iteration 16200: 0.044160\n",
      "Cost after iteration 16300: 0.044139\n",
      "Cost after iteration 16400: 0.044117\n",
      "Cost after iteration 16500: 0.044096\n",
      "Cost after iteration 16600: 0.044076\n",
      "Cost after iteration 16700: 0.044055\n",
      "Cost after iteration 16800: 0.044036\n",
      "Cost after iteration 16900: 0.044016\n",
      "Cost after iteration 17000: 0.043997\n",
      "Cost after iteration 17100: 0.043978\n",
      "Cost after iteration 17200: 0.043960\n",
      "Cost after iteration 17300: 0.043941\n",
      "Cost after iteration 17400: 0.043923\n",
      "Cost after iteration 17500: 0.043906\n",
      "Cost after iteration 17600: 0.043888\n",
      "Cost after iteration 17700: 0.043871\n",
      "Cost after iteration 17800: 0.043854\n",
      "Cost after iteration 17900: 0.043838\n",
      "Cost after iteration 18000: 0.043821\n",
      "Cost after iteration 18100: 0.043805\n",
      "Cost after iteration 18200: 0.043789\n",
      "Cost after iteration 18300: 0.043774\n",
      "Cost after iteration 18400: 0.043758\n",
      "Cost after iteration 18500: 0.043743\n",
      "Cost after iteration 18600: 0.043728\n",
      "Cost after iteration 18700: 0.043714\n",
      "Cost after iteration 18800: 0.043699\n",
      "Cost after iteration 18900: 0.043685\n",
      "Cost after iteration 19000: 0.043671\n",
      "Cost after iteration 19100: 0.043657\n",
      "Cost after iteration 19200: 0.043644\n",
      "Cost after iteration 19300: 0.043630\n",
      "Cost after iteration 19400: 0.043617\n",
      "Cost after iteration 19500: 0.043604\n",
      "Cost after iteration 19600: 0.043591\n",
      "Cost after iteration 19700: 0.043578\n",
      "Cost after iteration 19800: 0.043566\n",
      "Cost after iteration 19900: 0.043554\n",
      "Cost after iteration 20000: 0.043542\n",
      "Cost after iteration 20100: 0.043530\n",
      "Cost after iteration 20200: 0.043518\n",
      "Cost after iteration 20300: 0.043507\n",
      "Cost after iteration 20400: 0.043495\n",
      "Cost after iteration 20500: 0.043484\n",
      "Cost after iteration 20600: 0.043473\n",
      "Cost after iteration 20700: 0.043462\n",
      "Cost after iteration 20800: 0.043452\n",
      "Cost after iteration 20900: 0.043441\n",
      "Cost after iteration 21000: 0.043431\n",
      "Cost after iteration 21100: 0.043420\n",
      "Cost after iteration 21200: 0.043410\n",
      "Cost after iteration 21300: 0.043400\n",
      "Cost after iteration 21400: 0.043390\n",
      "Cost after iteration 21500: 0.043380\n",
      "Cost after iteration 21600: 0.043371\n",
      "Cost after iteration 21700: 0.043361\n",
      "Cost after iteration 21800: 0.043352\n",
      "Cost after iteration 21900: 0.043343\n",
      "Cost after iteration 22000: 0.043333\n",
      "Cost after iteration 22100: 0.043324\n",
      "Cost after iteration 22200: 0.043316\n",
      "Cost after iteration 22300: 0.043307\n",
      "Cost after iteration 22400: 0.043298\n",
      "Cost after iteration 22500: 0.043289\n",
      "Cost after iteration 22600: 0.043281\n",
      "Cost after iteration 22700: 0.043273\n",
      "Cost after iteration 22800: 0.043264\n",
      "Cost after iteration 22900: 0.043256\n",
      "Cost after iteration 23000: 0.043248\n",
      "Cost after iteration 23100: 0.043240\n",
      "Cost after iteration 23200: 0.043232\n",
      "Cost after iteration 23300: 0.043224\n",
      "Cost after iteration 23400: 0.043217\n",
      "Cost after iteration 23500: 0.043209\n",
      "Cost after iteration 23600: 0.043202\n",
      "Cost after iteration 23700: 0.043194\n",
      "Cost after iteration 23800: 0.043187\n",
      "Cost after iteration 23900: 0.043179\n",
      "Cost after iteration 24000: 0.043172\n",
      "Cost after iteration 24100: 0.043165\n",
      "Cost after iteration 24200: 0.043158\n",
      "Cost after iteration 24300: 0.043151\n",
      "Cost after iteration 24400: 0.043144\n",
      "Cost after iteration 24500: 0.043138\n",
      "Cost after iteration 24600: 0.043131\n",
      "Cost after iteration 24700: 0.043124\n",
      "Cost after iteration 24800: 0.043118\n",
      "Cost after iteration 24900: 0.043111\n",
      "Cost after iteration 25000: 0.043105\n",
      "Cost after iteration 25100: 0.043099\n",
      "Cost after iteration 25200: 0.043092\n",
      "Cost after iteration 25300: 0.043086\n",
      "Cost after iteration 25400: 0.043080\n",
      "Cost after iteration 25500: 0.043074\n",
      "Cost after iteration 25600: 0.043068\n",
      "Cost after iteration 25700: 0.043062\n",
      "Cost after iteration 25800: 0.043056\n",
      "Cost after iteration 25900: 0.043050\n",
      "Cost after iteration 26000: 0.043044\n",
      "Cost after iteration 26100: 0.043039\n",
      "Cost after iteration 26200: 0.043033\n",
      "Cost after iteration 26300: 0.043028\n",
      "Cost after iteration 26400: 0.043022\n",
      "Cost after iteration 26500: 0.043017\n",
      "Cost after iteration 26600: 0.043011\n",
      "Cost after iteration 26700: 0.043006\n",
      "Cost after iteration 26800: 0.043001\n",
      "Cost after iteration 26900: 0.042995\n",
      "Cost after iteration 27000: 0.042990\n",
      "Cost after iteration 27100: 0.042985\n",
      "Cost after iteration 27200: 0.042980\n",
      "Cost after iteration 27300: 0.042975\n",
      "Cost after iteration 27400: 0.042970\n",
      "Cost after iteration 27500: 0.042965\n",
      "Cost after iteration 27600: 0.042960\n",
      "Cost after iteration 27700: 0.042955\n",
      "Cost after iteration 27800: 0.042951\n",
      "Cost after iteration 27900: 0.042946\n",
      "Cost after iteration 28000: 0.042941\n",
      "Cost after iteration 28100: 0.042937\n",
      "Cost after iteration 28200: 0.042932\n",
      "Cost after iteration 28300: 0.042927\n",
      "Cost after iteration 28400: 0.042923\n",
      "Cost after iteration 28500: 0.042918\n",
      "Cost after iteration 28600: 0.042914\n",
      "Cost after iteration 28700: 0.042910\n",
      "Cost after iteration 28800: 0.042905\n",
      "Cost after iteration 28900: 0.042901\n",
      "Cost after iteration 29000: 0.042897\n",
      "Cost after iteration 29100: 0.042893\n",
      "Cost after iteration 29200: 0.042888\n",
      "Cost after iteration 29300: 0.042884\n",
      "Cost after iteration 29400: 0.042880\n",
      "Cost after iteration 29500: 0.042876\n",
      "Cost after iteration 29600: 0.042872\n",
      "Cost after iteration 29700: 0.042868\n",
      "Cost after iteration 29800: 0.042864\n",
      "Cost after iteration 29900: 0.042860\n",
      "Cost after iteration 30000: 0.042856\n",
      "Cost after iteration 30100: 0.042852\n",
      "Cost after iteration 30200: 0.042849\n",
      "Cost after iteration 30300: 0.042845\n",
      "Cost after iteration 30400: 0.042841\n",
      "Cost after iteration 30500: 0.042837\n",
      "Cost after iteration 30600: 0.042834\n",
      "Cost after iteration 30700: 0.042830\n",
      "Cost after iteration 30800: 0.042826\n",
      "Cost after iteration 30900: 0.042823\n",
      "Cost after iteration 31000: 0.042819\n",
      "Cost after iteration 31100: 0.042816\n",
      "Cost after iteration 31200: 0.042812\n",
      "Cost after iteration 31300: 0.042809\n",
      "Cost after iteration 31400: 0.042805\n",
      "Cost after iteration 31500: 0.042802\n",
      "Cost after iteration 31600: 0.042799\n",
      "Cost after iteration 31700: 0.042795\n",
      "Cost after iteration 31800: 0.042792\n",
      "Cost after iteration 31900: 0.042789\n",
      "Cost after iteration 32000: 0.042786\n",
      "Cost after iteration 32100: 0.042782\n",
      "Cost after iteration 32200: 0.042779\n",
      "Cost after iteration 32300: 0.042776\n",
      "Cost after iteration 32400: 0.042773\n",
      "Cost after iteration 32500: 0.042770\n",
      "Cost after iteration 32600: 0.042767\n",
      "Cost after iteration 32700: 0.042764\n",
      "Cost after iteration 32800: 0.042761\n",
      "Cost after iteration 32900: 0.042758\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deZxddX3/8dd79n2yzGQhCWRpEIKCQgCtG1ar4AJasYaqldYWtdJa7YbVHyr92Z9rra2oQMWlVRFxixRF3BVFMhECJBAIMSETs0z2PZOZ+fz+OGeSm8udySSZM/feue/n43Efc+73fO+5n3sI933P9j2KCMzMrHJVFbsAMzMrLgeBmVmFcxCYmVU4B4GZWYVzEJiZVTgHgZlZhXMQ2Lgg6buS3ljsOszKkYPAToqkNZJeVOw6IuKSiPhCsesAkPQTSX8xBu9TL+lmSbskbZT0zmP0f0fab1f6uvqcebMl/VjSPkmPDPXfVNIPJYWkmpy2NZL2S9qTPr4/ep/SxoKDwEpe7pdOsZVSLcD7gPnAacALgH+UdHGhjpJeAlwDvDDtPxd4f06XrwD3AZOBdwO3SerMW8brgNohanlFRLSkjxef8CeyonAQWGYkvVzS/ZJ2SPqlpLNz5l0j6XFJuyWtkPSqnHlXSrpb0sclbQXel7b9QtJHJW2X9FtJl+S85vCv8BH0nSPpZ+l7/0DS9ZL+Z4jPcJGkbkn/JGkj8DlJEyXdLqknXf7tkmam/T8APBf4ZPrr+JNp+xmS7pK0TdJKSX88Cqv4jcC/RMT2iHgYuAm4cpi+n42I5RGxHfiXwb6STgfOBd4bEfsj4uvAg8Crc9ZDO/Be4B9HoW4rMQ4Cy4SkZwA3A28m+ZV5A7A4Z3fE4yRfmO0kv0z/R9L0nEVcCKwGpgIfyGlbCXQAHwY+K0lDlDBc3y8D96Z1vQ94wzE+zjRgEskv6atI/r/5XPr8VGA/8EmAiHg38HPg6vTX8dWSmoG70vedAiwCPiVpQaE3k/SpNDwLPR5I+0wEpgPLcl66DDhriM9wVoG+UyVNTuetjojdwyzrX4FPAxuHWP6X0mD8vqRzhuhjJcpBYFm5CrghIn4dEf3p/vuDwDMBIuJrEfG7iBiIiK8CjwEX5Lz+dxHxnxHRFxH707a1EXFTRPQDXyD5Ipw6xPsX7CvpVOB84NqI6I2IXwCLj/FZBkh+LR9MfzFvjYivR8S+9MvzA8Dzh3n9y4E1EfG59PPcB3wdeE2hzhHxVxExYYjH4FZVS/p3Z85LdwKtQ9TQUqAvaf/8eUctS9JC4NnAfw6x7NcBs0mC8cfAnZImDNHXSpCDwLJyGvB3ub9mgVnAKQCS/jRnt9EO4Kkkv94HrSuwzMO/RiNiXzrZUqDfcH1PAbbltA31Xrl6IuLA4BNJTZJukLRW0i7gZ8AESdVDvP404MK8dfE6ki2NE7Un/duW09YG7C7Qd7B/fl/S/vnzDi9LUhXwKeDtEdFXaMERcXcakPsi4v8BO0i29qxMOAgsK+uAD+T9mm2KiK9IOo1kf/bVwOSImAA8BOTu5slqWNwNwCRJTTlts47xmvxa/g54CnBhRLQBz0vbNUT/dcBP89ZFS0S8tdCbSfpMzhk4+Y/lAOl+/g1A7m6Yc4DlQ3yG5QX6boqIrem8uZJa8+YvJwmEhcBX02MkS9L53ZKG+rIPjv5vaSXOQWCjoVZSQ86jhuSL/i2SLlSiWdLL0i+bZpIvix4ASX9GskWQuYhYC3SRHICuk/Qs4BXHuZhWkuMCOyRNIjmImmsTyVk5g24HTpf0Bkm16eN8SWcOUeNbcs7AyX/k7rf/IvCe9OD1GcBfAp8fouYvAm+StCDdbfOewb4R8ShwP/De9L/fq4CzSXZf7STZinp6+nhpurzzgF9LOlXSs9N12SDpH0i27O4ebgVaaXEQ2Gi4g+SLcfDxvojoIvli+iSwHVhFepZKRKwAPgb8iuRL82mM7RfH64BnAVuB/wt8leT4xUj9O9AIbAHuAb6XN/8TwOXpGUX/kR5HeDHJQeLfkey2+hBQz8l5L8lB97XAT4GPRMT3ANIv6D3pMRHS9g+T7MN/In1NboAtIvnlvx34IHB5RPREYuPggzS8SbYmeklC8dPp69YDFwOXpFsaVibkG9NYpZP0VeCRiMj/ZW9WEbxFYBUn3S0zT1KVkguwLgO+Vey6zIqllK6SNBsr04BvkFxH0A28NT2l06wiedeQmVmF864hM7MKV3a7hjo6OmL27NnFLsPMrKwsXbp0S0R0FppXdkEwe/Zsurq6il2GmVlZkbR2qHneNWRmVuEcBGZmFS7TIJB0cTr2+ipJ1xSY//F04LH7JT2aDsZlZmZjKLNjBOlIjNcDf0hyrvYSSYvT4QUAiIh35PT/a+AZWdVjZmaFZblFcAGwKiJWp2OS3EJyBedQriC5XZ6ZmY2hLINgBkeP896dtj1JOizxHOBHQ8y/SlKXpK6enp5CXczM7ASVysHiRcBt6d2kniQiboyIhRGxsLOz4GmwZmZ2grIMgvUcfcOPmWlbIYvIeLfQmi17+dD3HqF/wENqmJnlyjIIlgDzJc2RVEfyZf+ke8OmN9SYSDI2fWbuXL6RT//kcf7yi12s27bv2C8wM6sQmQVBen/Tq4E7gYeBWyNiuaTrJF2a03URcEtkPPrdm58/j/dfehZ3r9rCCz/2Uz743Uc42FdwT5SZWUUpu9FHFy5cGCczxMTGnQf46PdXctvSbp46o41PXnEuszuaR7FCM7PSI2lpRCwsNK9UDhaPmWntDXz0NedwwxvOY922/bzqU3ezbJ2vYzOzylVxQTDoJWdN49tvezYtDTVccdM93LPat1g1s8pUsUEAMLujma+/5fc5ZUIjb/7vpazdurfYJZmZjbmKDgKAKW0N3PzG84kI/v5ryxjw6aVmVmEqPggATp3cxP95+QKWrNnOl+59otjlmJmNKQdB6vLzZvLc+R186LuPsGHn/mKXY2Y2ZhwEKUn866ueRm/fAB+/69Fil2NmNmYcBDlmTWriDc86jduWdvN4z55il2NmNiYcBHneetE8aquruOGnjxe7FDOzMeEgyNPRUs9rz5/FN+9bz6ZdB4pdjplZ5hwEBfz5s+dwqD+45d51x+5sZlbmHAQFzO5o5nmnd/KVe5+gr3+g2OWYmWXKQTCEP7lgFht3HeDuxz30hJmNbw6CIbzgjCm0NdTwrfuGupeOmdn44CAYQn1NNS87ezp3Lt/IgUO+b4GZjV8OgmFc8tTp7Ovt5xePbSl2KWZmmXEQDOOZcyfTWl/DXSs2FbsUM7PMOAiGUVdTxUVnTOGHj2zyqKRmNm45CI7hotM72bKnl4c37ip2KWZmmXAQHMNz53cA8HMfJzCzccpBcAxT2hp4ytRWfv5YT7FLMTPLhINgBJ41bzJL127nkK8yNrNxKNMgkHSxpJWSVkm6Zog+fyxphaTlkr6cZT0nauHsiRw4NMCK3/k4gZmNP5kFgaRq4HrgEmABcIWkBXl95gPvAp4dEWcBf5tVPSfjvNMmArB07fYiV2JmNvqy3CK4AFgVEasjohe4Bbgsr89fAtdHxHaAiNicYT0nbHp7IzMmNDoIzGxcyjIIZgC54zh3p225TgdOl3S3pHskXVxoQZKuktQlqaunpzgHbc87bSJda7cR4esJzGx8KfbB4hpgPnARcAVwk6QJ+Z0i4saIWBgRCzs7O8e4xMR5p01k066DrN/hG9ub2fiSZRCsB2blPJ+ZtuXqBhZHxKGI+C3wKEkwlBwfJzCz8SrLIFgCzJc0R1IdsAhYnNfnWyRbA0jqINlVtDrDmk7YGdNaaaqrdhCY2biTWRBERB9wNXAn8DBwa0Qsl3SdpEvTbncCWyWtAH4M/ENElOSdYGqqqzh7ZjsPdO8sdilmZqOqJsuFR8QdwB15bdfmTAfwzvRR8s6c3sYt966jfyCorlKxyzEzGxXFPlhcVhZMb2P/oX7WbN1b7FLMzEaNg+A4nDm9DYCHN/gKYzMbPxwEx2H+1BZqquShJsxsXHEQHIf6mmp+b0qLtwjMbFxxEBynM6e3scJBYGbjiIPgOC2Y3samXQfZuudgsUsxMxsVDoLjdOSA8e4iV2JmNjocBMfpzOmtAKzY4AvLzGx8cBAcp8kt9XS21vPYpj3FLsXMbFQ4CE7A3I5mHu9xEJjZ+OAgOAHzprTweM9e35vAzMYFB8EJmNvRzM79h9i2t7fYpZiZnTQHwQmYN6UFgNVbPOaQmZU/B8EJmNeRBMHjm32cwMzKn4PgBMyY2EhdTZW3CMxsXHAQnIDqKjFncrO3CMxsXHAQnKB5U5q9RWBm44KD4ATN7WjhiW376O0bKHYpZmYnxUFwguZNaaZ/IHhim7cKzKy8OQhO0LzO5MyhVZsdBGZW3hwEJ2h2RzMAa33/YjMrc5kGgaSLJa2UtErSNQXmXympR9L96eMvsqxnNLU11NLeWMu67fuKXYqZ2UmpyWrBkqqB64E/BLqBJZIWR8SKvK5fjYirs6ojS7MmNbJu2/5il2FmdlKy3CK4AFgVEasjohe4Bbgsw/cbc7MmNnmLwMzKXpZBMANYl/O8O23L92pJD0i6TdKsQguSdJWkLkldPT09WdR6QmZNaqJ7234GBjwKqZmVr2IfLP4OMDsizgbuAr5QqFNE3BgRCyNiYWdn55gWOJxZk5ro7R9g827fv9jMyleWQbAeyP2FPzNtOywitkbE4LfofwHnZVjPqJs1sRHAu4fMrKxlGQRLgPmS5kiqAxYBi3M7SJqe8/RS4OEM6xl1syY1AbBum4PAzMpXZmcNRUSfpKuBO4Fq4OaIWC7pOqArIhYDfyPpUqAP2AZcmVU9WZgxIdkieMJBYGZlLLMgAIiIO4A78tquzZl+F/CuLGvIUkNtNVPb6n0KqZmVtWIfLC57PoXUzMqdg+AknTqpiW7vGjKzMuYgOEkzJzWxYdcBD0dtZmXLQXCSZk1sJALW7/BxAjMrTw6CkzRzYnIKabePE5hZmXIQnKTp7Q0AbNx5oMiVmJmdGAfBSZqWBsGmXQ4CMytPDoKT1FBbzYSmWjZ4i8DMypSDYBRMa2vwFoGZlS0HwSiY1t7ARgeBmZUpB8EomN7e4IPFZla2HASjYGpbA1v29PqiMjMrSw6CUTCtLTlzaPNubxWYWflxEIyCab6WwMzKmINgFBwOAh8wNrMy5CAYBdPbkhvUeIvAzMqRg2AUtDXW0FBb5SAws7LkIBgFkpje3uhdQ2ZWlhwEo2RqW723CMysLDkIRsm0Nl9dbGblyUEwSqa1N7J510EGBqLYpZiZHRcHwSiZ1lZPb/8A2/b1FrsUM7PjkmkQSLpY0kpJqyRdM0y/V0sKSQuzrCdLna3JtQRb9hwsciVmZscnsyCQVA1cD1wCLACukLSgQL9W4O3Ar7OqZSx0tNQBsGW3twjMrLxkuUVwAbAqIlZHRC9wC3BZgX7/AnwIKOsjrR2t9YC3CMys/GQZBDOAdTnPu9O2wySdC8yKiP8dbkGSrpLUJamrp6dn9CsdBR0tSRD07HYQmFl5KdrBYklVwL8Bf3esvhFxY0QsjIiFnZ2d2Rd3AtoaaqirrvIWgZmVnREFgaTXjKQtz3pgVs7zmWnboFbgqcBPJK0BngksLtcDxpLoaKmjx0FgZmVmpFsE7xphW64lwHxJcyTVAYuAxYMzI2JnRHRExOyImA3cA1waEV0jrKnkdLTWs2WPDxabWXmpGW6mpEuAlwIzJP1Hzqw2oG+410ZEn6SrgTuBauDmiFgu6TqgKyIWD/f6ctTR4mEmzKz8DBsEwO+ALuBSYGlO+27gHcdaeETcAdyR13btEH0vOtbySl1HSx0Prd9Z7DLMzI7LsEEQEcuAZZK+HBGHACRNJDnTZ/tYFFhOOlrq2bq3l4GBoKpKxS7HzGxERnqM4C5JbZImAb8BbpL08QzrKksdLfX0DwQ79h8qdilmZiM20iBoj4hdwB8BX4yIC4EXZldWefJFZWZWjkYaBDWSpgN/DNyeYT1lrTO9qGyLLyozszIy0iC4juTsn8cjYomkucBj2ZVVnjpbk/GGfC2BmZWTY501BEBEfA34Ws7z1cCrsyqqXA0OM+FrCcysnIz0yuKZkr4paXP6+LqkmVkXV27aG2uprZaPEZhZWRnprqHPkVwVfEr6+E7aZjkkMbm53gPPmVlZGWkQdEbE5yKiL318HijN0d+KrKO1zlsEZlZWRhoEWyW9XlJ1+ng9sDXLwspVR0u9g8DMyspIg+DPSU4d3QhsAC4HrsyoprLW0VLvu5SZWVkZ0VlDJKePvnFwWIn0CuOPkgSE5UiGmThIRCB5mAkzK30j3SI4O3dsoYjYBjwjm5LKW0dLHYf6g137hx2c1cysZIw0CKrSweaAw1sEI92aqCiHb1np4wRmViZG+mX+MeBXkgYvKnsN8IFsSipvRy4qO8jvTWkpcjVmZsc20iuLvyipC/iDtOmPImJFdmWVr450mImtvrrYzMrEiHfvpF/8/vI/hsnNHoHUzMrLSI8R2AhNaq6jSrDVQWBmZcJBMMqqq8Sk5jp6vGvIzMqEgyADk5t9dbGZlQ8HQQY6Wuu8a8jMykamQSDpYkkrJa2SdE2B+W+R9KCk+yX9QtKCLOsZK8l4Q941ZGblIbMgkFQNXA9cAiwArijwRf/liHhaRDwd+DDwb1nVM5a8a8jMykmWWwQXAKsiYnVE9AK3AJfldoiIXTlPm4HIsJ4x09Fax77efvb1epgJMyt9WQ4TMQNYl/O8G7gwv5OktwHvBOo4csFafp+rgKsATj311FEvdLQNXl28dU8vTZM8EoeZlbaiHyyOiOsjYh7wT8B7huhzY0QsjIiFnZ2lfz+cjhbfxN7MykeWQbAemJXzfGbaNpRbgFdmWM+Yyd0iMDMrdVkGwRJgvqQ5kuqARST3PT5M0vycpy8DHsuwnjGTO/CcmVmpy2wHdkT0SboauBOoBm6OiOWSrgO6ImIxcLWkFwGHgO3AG7OqZyxNah4ceM5BYGalL9MjmRFxB3BHXtu1OdNvz/L9i6WhtprWhhpfS2BmZaHoB4vHq86Weh8sNrOy4CDISEdLvXcNmVlZcBBkpLO1ns27HQRmVvocBBmZ0lbPpp0Hil2GmdkxOQgyMq2tgb29/ew+cKjYpZiZDctBkJFp7Q0AbNrl3UNmVtocBBmZ0joYBN49ZGalzUGQkcEtgo0+TmBmJc5BkJFpbekWwW4HgZmVNgdBRhrrqmlrqPGZQ2ZW8hwEGZra1sBGHyMwsxLnIMjQtPYGnzVkZiXPQZChqW0NPmvIzEqegyBDU9uSYSb6B8bFrZjNbJxyEGRoWlsD/QPhwefMrKQ5CDI0tc1XF5tZ6XMQZOjwRWU+TmBmJcxBkKHBLQIHgZmVMgdBhjpa6qmuEpsdBGZWwhwEGaquEp0t9Wzw1cVmVsIcBBk7ZUID67fvL3YZZmZDchBkbNakJrp37Ct2GWZmQ8o0CCRdLGmlpFWSrikw/52SVkh6QNIPJZ2WZT3FMHNiI7/bcYC+/oFil2JmVlBmQSCpGrgeuARYAFwhaUFet/uAhRFxNnAb8OGs6imWWROb6B8IHycws5KV5RbBBcCqiFgdEb3ALcBluR0i4scRMbjf5B5gZob1FMXMiU0AdPs4gZmVqCyDYAawLud5d9o2lDcB3y00Q9JVkrokdfX09IxiidmbNakRgHXbfZzAzEpTSRwslvR6YCHwkULzI+LGiFgYEQs7OzvHtriTNL29kSpB9zYHgZmVppoMl70emJXzfGbadhRJLwLeDTw/IsbdoDx1NVVMb29kzVYHgZmVpiy3CJYA8yXNkVQHLAIW53aQ9AzgBuDSiNicYS1FNbezmd9u2VvsMszMCsosCCKiD7gauBN4GLg1IpZLuk7SpWm3jwAtwNck3S9p8RCLK2tzO5IgiPB9Ccys9GS5a4iIuAO4I6/t2pzpF2X5/qVibmcLew720bP7IFPSgejMzEpFSRwsHu/mdDQDsNq7h8ysBDkIxsDcziQIfJzAzEqRg2AMnNLeSENtFY9t2lPsUszMnsRBMAaqqsRTprayctOuYpdiZvYkDoIx8pRprazcuLvYZZiZPYmDYIw8ZVobW/b00rN73F0zZ2ZlzkEwRs6c1grgrQIzKzkOgjFyxvQ2AFZs2FnkSszMjuYgGCOTmuuYMaGRB7odBGZWWhwEY+icWe0s695R7DLMzI7iIBhD58ycwLpt+9m2t7fYpZiZHeYgGENnz5wA4K0CMyspDoIxdPbMdqqrxNI124tdipnZYQ6CMdRcX8NTZ7Rz72+3FbsUM7PDHARj7MI5k7h/3Q4OHOovdilmZoCDYMydP3sSvf0D3PeEjxOYWWlwEIyxZ86dRG21+Mmj4/bOnGZWZhwEY6y1oZYL5kziRw87CMysNDgIiuAPzpjKY5v3sHarb1RjZsXnICiCl5w1FYDvLPtdkSsxM3MQFMXMiU1cMGcS37hvPRFR7HLMrMJlGgSSLpa0UtIqSdcUmP88Sb+R1Cfp8ixrKTWvPncGq3v2snStLy4zs+LKLAgkVQPXA5cAC4ArJC3I6/YEcCXw5azqKFWvOOcU2hpq+Pwv1xS7FDOrcFluEVwArIqI1RHRC9wCXJbbISLWRMQDwECGdZSkproaXnv+LL770EbWbdtX7HLMrIJlGQQzgHU5z7vTtuMm6SpJXZK6enp6RqW4UvCm58ylukp88keril2KmVWwsjhYHBE3RsTCiFjY2dlZ7HJGzbT2Bl534anc9ptuHlrvG9aYWXFkGQTrgVk5z2embZbjb194OhOb6njXNx6kr7/i9pCZWQnIMgiWAPMlzZFUBywCFmf4fmWpvamW9196Fg+u38nn7l5T7HLMrAJlFgQR0QdcDdwJPAzcGhHLJV0n6VIASedL6gZeA9wgaXlW9ZSylz5tGi86cyofvvMRD1FtZmNO5XZB08KFC6Orq6vYZYy6nfsO8apP3c32fb18+23P4dTJTcUuyczGEUlLI2JhoXllcbC4ErQ31fLZK89nIOCKm+5hdc+eYpdkZhXCQVBC5nQ08z9vupADh/q5/DO/omuNdxOZWfYcBCXmaTPbue2tv09LfQ2vvfEePvGDx3w2kZllykFQguZ0NHP73zyHV5w9nY//4FEu+cTP+cnKzR6gzswy4SAoUW0Ntfz7omdw4xvOo7d/gCs/t4RXfuqX/O8DGzjY5/sdm9no8VlDZaC3b4Bbu9Zx089Xs3brPtoba3n52dN5xTmncN5pE6mtdp6b2fCGO2vIQVBG+geCnz3Wwzd/s57vr9jIgUMDtNTX8PvzJnPh3Mk8fdYEzjqljYba6mKXamYlZrggqBnrYuzEVVeJFzxlCi94yhT2HOzjF4/18NNHt/CzR3v4/opNANRUiTOmt3L6lFbmTWlhbkcz86a0cOqkJgeEmRXkLYJxYtOuAyxbt4P71+3gwfU7WbV5Dxt2Hjiqz+TmOqa1NzC9vSH920hHSx0TmuqY1FzHxKZaJjbV0d5YS413N5mNK94iqABT2xp48VnTePFZ0w637T3Yx2+37OXxnj2s3bqPDTsPsHHnfrq376dr7XZ27Ds05PLaG2tpb6ylub6GlvpqmupqaKmvobm+Om2rSduSeQ211dTXVFFfW0V9TTUN6d/ctvqaKhpqq6mu0lisEjMbIQfBONZcX8NTZ7Tz1BntBecfONTP1r29bN/by/Z9vWzfd+jI9N5edu4/xJ6D/ew92Mf2fb10b9/H3vT5nt4+TnRjsqZK1NVUUVMlaqurqKkWNVWDf49uq82ZV1udvCa3f236t7pKVGnwL1RViWolbUem0/bBPmn/6iohJX2qq8iZFlKyS65aaZ+qI32qJAQoXZYAcqYlpfOSGTpqXjJN7usFIqlN6WuS6eQ1g30g+RyDyxl8DUe9/shrEE9677Siw+8/aKh5OjxfOdNH2qy8OQgqWENtNTMmNDJjQuNxvzYi2H+onz0H+9h3sJ/e/gEOHhrgYF8/B9K/B/vSv4cGCk4f6g/6Bgbo648j0wNBX3/aljN9oK8/7TdA/0DQN5BM96Wv6x8I+geCCOiPZHoggoFIDrLb2EmzJ51OAyVnXvL8SMIMNS83hPKXQ6GAKtD25PceKuCe/N75yyG/r3I+R968vDILL2fIJ0O/7u0vnM8rzjmF0eYgsBMiiaa6ZPcQrcWu5tgGBoL+SMNhgJzpJDT6Iw2RIQMlnR4gmU77RwQBRCTtERAMzkumB3L6cVS/I9Mc7nf0a8jpc/g9cpfD0a8hbRsYOLou8qfT9TK4VTdYc67Ied3ga3L7588bbCi07EJ9B+vObTwyL3L6DL+8o/oP0zd/Hvl1FVgXQy2PnOXl13D00vPfs9C8oV+X/ybtjbVkwUFgFaGqSlTl/+wyM8BXFpuZVTwHgZlZhXMQmJlVOAeBmVmFcxCYmVU4B4GZWYVzEJiZVTgHgZlZhSu70Ucl9QBrT/DlHcCWUSxnLJVr7a57bJVr3VC+tZdL3adFRGehGWUXBCdDUtdQw7CWunKt3XWPrXKtG8q39nKtO5d3DZmZVTgHgZlZhau0ILix2AWchHKt3XWPrXKtG8q39nKt+7CKOkZgZmZPVmlbBGZmlsdBYGZW4SomCCRdLGmlpFWSril2PcORtEbSg5Lul9SVtk2SdJekx9K/E0ugzpslbZb0UE5bwTqV+I90/T8g6dziVT5k7e+TtD5d7/dLemnOvHelta+U9JLiVA2SZkn6saQVkpZLenvaXtLrfZi6S3qdS2qQdK+kZWnd70/b50j6dVrfVyXVpe316fNV6fzZxaj7uEXEuH8A1cDjwFygDlgGLCh2XcPUuwboyGv7MHBNOn0N8KESqPN5wLnAQ8eqE3gp8F2S27E+E/h1Cdb+PuDvC/RdkP6bqQfmpP+WqotU93Tg3HS6FXg0ra+k1/swdZf0Ok/XW0s6XQv8Ol2PtwKL0vbPAG9Np/8K+Ew6vQj4ajHW9/E+KmWL4AJgVUSsjohe4BbgsiLXdLwuA76QTn8BeGURawEgIn4GbMtrHqrOy4AvRuIeYIKk6WNT6ZMNUftQLgNuiYiDEfFbYBXJv6kxF45KDKQAAAZ5SURBVBEbIuI36fRu4GFgBiW+3oepeyglsc7T9bYnfVqbPgL4A+C2tD1/fQ/+d7gNeKHy71pfgiolCGYA63KedzP8P8JiC+D7kpZKuiptmxoRG9LpjcDU4pR2TEPVWS7/Da5Od6HcnLP7rSRrT3c7PIPkV2rZrPe8uqHE17mkakn3A5uBu0i2TnZERF+B2g7Xnc7fCUwe24qPX6UEQbl5TkScC1wCvE3S83JnRrLdWfLn/ZZLnTk+DcwDng5sAD5W3HKGJqkF+DrwtxGxK3deKa/3AnWX/DqPiP6IeDowk2Sr5IwilzTqKiUI1gOzcp7PTNtKUkSsT/9uBr5J8o9v0+Amffp3c/EqHNZQdZb8f4OI2JT+Tz8A3MSRXRElVbukWpIv0y9FxDfS5pJf74XqLpd1DhARO4AfA88i2cVWk87Kre1w3en8dmDrGJd63ColCJYA89Mj/XUkB3EWF7mmgiQ1S2odnAZeDDxEUu8b025vBL5dnAqPaag6FwN/mp7F8kxgZ86ujJKQt+/8VSTrHZLaF6VnhMwB5gP3jnV9kJwFBHwWeDgi/i1nVkmv96HqLvV1LqlT0oR0uhH4Q5LjGz8GLk+75a/vwf8OlwM/SrfQSluxj1aP1YPk7IlHSfbvvbvY9QxT51ySsyWWAcsHayXZz/hD4DHgB8CkEqj1KySb84dI9pO+aag6Sc6+uD5d/w8CC0uw9v9Oa3uA5H/o6Tn9353WvhK4pIh1P4dkt88DwP3p46Wlvt6Hqbuk1zlwNnBfWt9DwLVp+1ySYFoFfA2oT9sb0uer0vlzi/nvfKQPDzFhZlbhKmXXkJmZDcFBYGZW4RwEZmYVzkFgZlbhHARmZhXOQWAlQ9Iv07+zJf3JKC/7nwu9V1YkvVLStRkt+5+P3eu4l/k0SZ8f7eVaefDpo1ZyJF1EMiLly4/jNTVxZOyXQvP3RETLaNQ3wnp+CVwaEVtOcjlP+lxZfRZJPwD+PCKeGO1lW2nzFoGVDEmDozx+EHhuOj79O9JBvz4iaUk6ONmb0/4XSfq5pMXAirTtW+lgfcsHB+yT9EGgMV3el3LfK73i9iOSHlJyD4jX5iz7J5Juk/SIpC8NjiIp6YNKxtV/QNJHC3yO04GDgyEg6fOSPiOpS9Kjkl6eto/4c+Usu9Bneb2SMfPvl3SDpOrBzyjpA0rG0r9H0tS0/TXp510m6Wc5i/8OyVX3VmmKfUWbH34MPoA96d+LgNtz2q8C3pNO1wNdJGPUXwTsBebk9B28oraR5ErQybnLLvBeryYZUbKaZMTOJ0jGzr+IZOTImSQ/mH5FcnXsZJIrXQe3picU+Bx/Bnws5/nnge+ly5lPciVzw/F8rkK1p9NnknyB16bPPwX8aTodwCvS6Q/nvNeDwIz8+oFnA98p9r8DP8b+MThoklkpezFwtqTBsV3aSb5Qe4F7IxmvftDfSHpVOj0r7TfcoF/PAb4SEf0kA7f9FDgf2JUuuxtAyTDEs4F7gAPAZyXdDtxeYJnTgZ68tlsjGVjtMUmrSUawPJ7PNZQXAucBS9INlkaODDjXm1PfUpJxcgDuBj4v6VbgG0cWxWbglBG8p40zDgIrBwL+OiLuPKoxOZawN+/5i4BnRcQ+ST8h+eV9og7mTPcDNRHRJ+kCki/gy4GrSW5Skms/yZd6rvyDccEIP9cxCPhCRLyrwLxDETH4vv2k/79HxFskXQi8DFgq6byI2EqyrvaP8H1tHPExAitFu0luZzjoTuCtSoYxRtLp6cis+dqB7WkInEFyS8FBhwZfn+fnwGvT/fWdJLewHHKUSyXj6bdHxB3AO4BzCnR7GPi9vLbXSKqSNI9kwLKVx/G58uV+lh8Cl0uaki5jkqTThnuxpHkR8euIuJZky2VwuOfTOTL6p1UQbxFYKXoA6Je0jGT/+idIdsv8Jj1g20PhW3V+D3iLpIdJvmjvyZl3I/CApN9ExOty2r9JMr78MpJf6f8YERvTICmkFfi2pAaSX+PvLNDnZ8DHJCnnF/kTJAHTBrwlIg5I+q8Rfq58R30WSe8huaNdFcloqm8D1g7z+o9Imp/W/8P0swO8APjfEby/jTM+fdQsA5I+QXLg9Qfp+fm3R8Rtx3hZ0UiqB35Kcne8IU/DtfHJu4bMsvGvQFOxizgOpwLXOAQqk7cIzMwqnLcIzMwqnIPAzKzCOQjMzCqcg8DMrMI5CMzMKtz/BxSsmCxEte/zAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(trainX, trainY, layers_dims, num_iterations=33000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(X, parameters):\n",
    "\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "#     # convert probas to 0/1 predictions\n",
    "#     for i in range(0, probas.shape[1]):\n",
    "#         if probas[0,i] > 0.5:\n",
    "#             p[0,i] = 1\n",
    "#         else:\n",
    "#             p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "#     print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000,)\n",
      "User-id:2\n",
      "程序开始运行时间为：2019-12-22 15:56:39.037931\n",
      "程序结束运行时间为：2019-12-22 15:56:39.105265\n",
      "程序运行时间（去除打分耗时）为：0.067334\n",
      "程序的准确率为：95.30750884\n",
      "最终得分为：95.77675796\n",
      "                              zjnsrsbh   Probability\n",
      "0     6a6c2ca338a3a48d33d701c7ef82986d  1.000000e+00\n",
      "1     e2ce1429b399560a9ffd5d6edee21379  3.811751e-01\n",
      "2     10978ccba846b58fc76742ab32b8c197  9.968454e-01\n",
      "3     c862b7f4ad6a531f3552bed41c94138c  2.452838e-05\n",
      "4     4628e5c776c5bede7d0208495e9d4ab7  1.000000e+00\n",
      "5     9e99997593f69c9e7dfa3a564ec4edd1  8.973358e-01\n",
      "6     4b35dacea2a952e3502b97ba68dc311c  2.078854e-04\n",
      "7     9467ca19422d87a4391850f3a4cad9da  9.999443e-01\n",
      "8     892b6fe24260b3659d432c9f5bbb68bd  3.640557e-07\n",
      "9     ee8b945f4b892c75e2aaf68844c2837d  4.447734e-06\n",
      "10    9087488f04c589f6e904d832b1bc2832  9.994452e-01\n",
      "11    11ffcd0b49ca5dc0aa353d175aac3680  9.999997e-01\n",
      "12    5f656da04f8d9c25beba2364c82194f5  4.407094e-05\n",
      "13    5aaaf45be591a4929ec066b508ca253f  9.065348e-05\n",
      "14    5967f1d5df5a6c0a9d186fd0b0f82fd2  9.999974e-01\n",
      "15    febf8bdafabf922de7f4c29a82852423  1.653196e-03\n",
      "16    0a964801ff126984aa97a5aaa11da140  1.806116e-05\n",
      "17    9bcc6a5b3e0b93497a543e5038b8e4fd  4.369920e-04\n",
      "18    80ee3e8ebb62a54496c9c46a1e58ef11  6.121134e-06\n",
      "19    c4b486536d6ec6b748d9134f726ad4e9  6.867632e-07\n",
      "20    9ed475c1792b5c28f2e12b6d0c5b8a3b  1.000000e+00\n",
      "21    6b539dcef270ebd548beb2f613a149e2  5.633856e-02\n",
      "22    d858bcd094b37698869d277de6d9ef40  9.999994e-01\n",
      "23    a18212e593c2132a34eadff4cb867879  1.244421e-04\n",
      "24    34cef22b86c03a26cd712ab2d6beb65b  1.161894e-06\n",
      "25    6852a0f04ef1739033fc73fafa9a64ab  1.956062e-06\n",
      "26    d4426c1904da21d3b56e47f4058a700b  6.104819e-05\n",
      "27    fa2b38f3278eda9c3902468e842ef4f2  9.222347e-01\n",
      "28    9ec0a8181d5f30c8b2e6d58beb7578b2  9.998565e-01\n",
      "29    bc740a9d964bb2df30271acb4fe9371b  3.811751e-01\n",
      "...                                ...           ...\n",
      "5970  325ddb397966b04a8470cfc496565059  8.804235e-06\n",
      "5971  23b3f3e98aefa5a25546d90a6d107cf5  1.000000e+00\n",
      "5972  01cdd6ef4e53a5c23e919680f8b7f773  5.523998e-06\n",
      "5973  34e3200d870521b4e828b6049f6709ce  1.096665e-06\n",
      "5974  3703762e23d072d1932bb5e87fead380  1.396465e-04\n",
      "5975  6b0f9fa4dfed434608888fcdd4f0ab0c  2.693710e-04\n",
      "5976  cc89148f31d5cbd267d741051cc913e9  1.000000e+00\n",
      "5977  c269b651f4e97ce4d3489e963e378ef1  9.998771e-01\n",
      "5978  790a8f6ddcd041cf4b50a36809ff6482  1.401843e-06\n",
      "5979  62a3d3bcc46954cb408d1bc3cc9fa18d  1.118369e-04\n",
      "5980  cc4e8ea9c951314656aff71107563878  4.635540e-06\n",
      "5981  b0b93b212880897ef24db12bacc3a050  9.999974e-01\n",
      "5982  8af822a39348aad05d0cd527088faa68  4.458660e-06\n",
      "5983  8de64d346bbccd609412be072bb3f190  4.925431e-06\n",
      "5984  1f078a2b3673cba745eddc7818d59a11  8.316651e-03\n",
      "5985  98114d971f5cfd761bc0fa20d41c5388  6.218913e-04\n",
      "5986  e76709274d5d2fbaae0724c2e2326e11  6.458961e-06\n",
      "5987  72697a0de934bd0b96d915fa739320ea  1.000000e+00\n",
      "5988  21ef94c4c070e15dcaeb13212fd5a451  3.811751e-01\n",
      "5989  f28059832017d5e8bcfb560d9d537ef3  1.631643e-06\n",
      "5990  4e86933c3023b360115a6dc5177d99e1  1.000000e+00\n",
      "5991  1e69f49c1b65336c7bffd1711d415a7a  7.233368e-08\n",
      "5992  a0ea43b33af7904b9f4b07dbe1345dfa  9.967510e-01\n",
      "5993  63e776eddf5ddc7e44d3ed8581385413  9.999977e-01\n",
      "5994  ddf6729245a569b6c77a9d24ca17e1f4  1.000000e+00\n",
      "5995  d81c6a4965e4c516ae6002f72bb97dc6  4.804762e-05\n",
      "5996  e0cf9faba0b6992f2489dad9b2f887ea  5.059009e-05\n",
      "5997  39a11caa50348bd561548f1865fe3e3d  2.025491e-04\n",
      "5998  c5467956a26bc280fe0cda00f9f74449  1.908197e-05\n",
      "5999  cf6a1485986332627d0b3397c6d88254  8.328004e-05\n",
      "\n",
      "[6000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "get_score.post_user_id('2')\n",
    "test_label = predict_label(testX, parameters)\n",
    "print(test_label[0].shape)\n",
    "prob = {\n",
    "    \"zjnsrsbh\": test_data['zjnsrsbh'],\n",
    "    'Probability':test_label[0],\n",
    "}\n",
    "# print(test_data)\n",
    "result = pd.DataFrame(prob)\n",
    "\n",
    "get_score.post_verify_data(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
